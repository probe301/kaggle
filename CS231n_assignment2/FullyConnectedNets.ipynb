{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully-Connected Neural Nets\n",
    "In the previous homework you implemented a fully-connected two-layer neural network on CIFAR-10. The implementation was simple but not very modular since the loss and gradient were computed in a single monolithic function. This is manageable for a simple two-layer network, but would become impractical as we move to bigger models. Ideally we want to build networks using a more modular design so that we can implement different layer types in isolation and then snap them together into models with different architectures.\n",
    "\n",
    "In this exercise we will implement fully-connected networks using a more modular approach. For each layer we will implement a `forward` and a `backward` function. The `forward` function will receive inputs, weights, and other parameters and will return both an output and a `cache` object storing data needed for the backward pass, like this:\n",
    "\n",
    "```python\n",
    "def layer_forward(x, w):\n",
    "  \"\"\" Receive inputs x and weights w \"\"\"\n",
    "  # Do some computations ...\n",
    "  z = # ... some intermediate value\n",
    "  # Do some more computations ...\n",
    "  out = # the output\n",
    "   \n",
    "  cache = (x, w, z, out) # Values we need to compute gradients\n",
    "   \n",
    "  return out, cache\n",
    "```\n",
    "\n",
    "The backward pass will receive upstream derivatives and the `cache` object, and will return gradients with respect to the inputs and weights, like this:\n",
    "\n",
    "```python\n",
    "def layer_backward(dout, cache):\n",
    "  \"\"\"\n",
    "  Receive derivative of loss with respect to outputs and cache,\n",
    "  and compute derivative with respect to inputs.\n",
    "  \"\"\"\n",
    "  # Unpack cache values\n",
    "  x, w, z, out = cache\n",
    "  \n",
    "  # Use values in cache to compute derivatives\n",
    "  dx = # Derivative of loss with respect to x\n",
    "  dw = # Derivative of loss with respect to w\n",
    "  \n",
    "  return dx, dw\n",
    "```\n",
    "\n",
    "After implementing a bunch of layers this way, we will be able to easily combine them to build classifiers with different architectures.\n",
    "\n",
    "In addition to implementing fully-connected networks of arbitrary depth, we will also explore different update rules for optimization, and introduce Dropout as a regularizer and Batch Normalization as a tool to more efficiently optimize deep networks.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.keyboard_manager.command_shortcuts.add_shortcut('f5', {\n",
    "    help : 'run cell',\n",
    "    handler : function (event) {\n",
    "        IPython.notebook.execute_cell();\n",
    "        return false;\n",
    "    }}                                               \n",
    ");\n",
    "Jupyter.keyboard_manager.command_shortcuts.add_shortcut('ctrl-.', {\n",
    "    help : 'run cell',\n",
    "    handler : function (event) {\n",
    "        IPython.notebook.execute_cell();\n",
    "        return false;\n",
    "    }}                                               \n",
    ");\n",
    "Jupyter.keyboard_manager.edit_shortcuts.add_shortcut('f5', {\n",
    "    help : 'run cell',\n",
    "    handler : function (event) {\n",
    "        IPython.notebook.execute_cell();\n",
    "        return false;\n",
    "    }}                                               \n",
    ");\n",
    "Jupyter.keyboard_manager.edit_shortcuts.add_shortcut('ctrl-.', {\n",
    "    help : 'run cell',\n",
    "    handler : function (event) {\n",
    "        IPython.notebook.execute_cell();\n",
    "        return false;\n",
    "        \n",
    "    }}                                               \n",
    ");\n",
    "\n",
    "\n",
    "Jupyter.keyboard_manager.edit_shortcuts.add_shortcut('ctrl-enter', {\n",
    "    help : 'none',\n",
    "    // 防止与 Sublime hotkey Ctrl+Enter 冲突\n",
    "    handler : function (event) {\n",
    "        return false;\n",
    "    }}\n",
    ");\n",
    "\n",
    "\n",
    "var cell = Jupyter.notebook.get_selected_cell();\n",
    "var config = cell.config;\n",
    "var patch = {\n",
    "      CodeCell: {\n",
    "        cm_config:{indentUnit: 2}\n",
    "      }\n",
    "    }\n",
    "config.update(patch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run the following from the cs231n directory and try again:\n",
      "python setup.py build_ext --inplace\n",
      "You may also need to restart your iPython kernel\n"
     ]
    }
   ],
   "source": [
    "# As usual, a bit of setup\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cs231n.classifiers.fc_net import *\n",
    "from cs231n.data_utils import get_CIFAR10_data\n",
    "from cs231n.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\n",
    "from cs231n.solver import Solver\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cs231n/datasets\\data_batch_1\n",
      "cs231n/datasets\\data_batch_2\n",
      "X_train (17000, 32, 32, 3)\n",
      "X_val (1000, 32, 32, 3)\n",
      "X_test (2000, 32, 32, 3)\n",
      "X_train:  (17000, 3, 32, 32)\n",
      "X_val:  (1000, 3, 32, 32)\n",
      "y_val:  (1000,)\n",
      "y_train:  (17000,)\n",
      "y_test:  (2000,)\n",
      "X_test:  (2000, 3, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "# Load the (preprocessed) CIFAR10 data.\n",
    "\n",
    "data = get_CIFAR10_data(num_training=17000, \n",
    "                        num_validation=1000,\n",
    "                        num_test=2000)\n",
    "for k, v in data.items():\n",
    "  print ('%s: ' % k, v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affine layer: foward\n",
    "Open the file `cs231n/layers.py` and implement the `affine_forward` function.\n",
    "\n",
    "Once you are done you can test your implementaion by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test the affine_forward function\n",
    "\n",
    "num_inputs = 2\n",
    "input_shape = (4, 5, 6)\n",
    "output_dim = 3\n",
    "\n",
    "input_size = num_inputs * np.prod(input_shape)\n",
    "weight_size = output_dim * np.prod(input_shape)\n",
    "\n",
    "x = np.linspace(-0.1, 0.5, num=input_size).reshape(num_inputs, *input_shape)\n",
    "w = np.linspace(-0.2, 0.3, num=weight_size).reshape(np.prod(input_shape), output_dim)\n",
    "b = np.linspace(-0.3, 0.1, num=output_dim)\n",
    "\n",
    "out, _ = affine_forward(x, w, b)\n",
    "correct_out = np.array([[ 1.49834967,  1.70660132,  1.91485297],\n",
    "                        [ 3.25553199,  3.5141327,   3.77273342]])\n",
    "\n",
    "# Compare your output with ours. The error should be around 1e-9.\n",
    "print ('Testing affine_forward function:')\n",
    "print ('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affine layer: backward\n",
    "Now implement the `affine_backward` function and test your implementation using numeric gradient checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test the affine_backward function\n",
    "\n",
    "x = np.random.randn(10, 2, 3)\n",
    "w = np.random.randn(6, 5)\n",
    "b = np.random.randn(5)\n",
    "dout = np.random.randn(10, 5)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: affine_forward(x, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: affine_forward(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: affine_forward(x, w, b)[0], b, dout)\n",
    "\n",
    "_, cache = affine_forward(x, w, b)\n",
    "dx, dw, db = affine_backward(dout, cache)\n",
    "\n",
    "# The error should be around 1e-10\n",
    "print ('Testing affine_backward function:')\n",
    "print ('dx error: ', rel_error(dx_num, dx))\n",
    "print ('dw error: ', rel_error(dw_num, dw))\n",
    "print ('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLU layer: forward\n",
    "Implement the forward pass for the ReLU activation function in the `relu_forward` function and test your implementation using the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test the relu_forward function\n",
    "\n",
    "x = np.linspace(-0.5, 0.5, num=12).reshape(3, 4)\n",
    "\n",
    "out, _ = relu_forward(x)\n",
    "correct_out = np.array([[ 0.,          0.,          0.,          0.,        ],\n",
    "                        [ 0.,          0.,          0.04545455,  0.13636364,],\n",
    "                        [ 0.22727273,  0.31818182,  0.40909091,  0.5,       ]])\n",
    "\n",
    "# Compare your output with ours. The error should be around 1e-8\n",
    "print ('Testing relu_forward function:')\n",
    "print ('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLU layer: backward\n",
    "Now implement the backward pass for the ReLU activation function in the `relu_backward` function and test your implementation using numeric gradient checking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.random.randn(10, 10)\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: relu_forward(x)[0], x, dout)\n",
    "\n",
    "_, cache = relu_forward(x)\n",
    "dx = relu_backward(dout, cache)\n",
    "\n",
    "# The error should be around 1e-12\n",
    "print ('Testing relu_backward function:')\n",
    "print ('dx error: ', rel_error(dx_num, dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Sandwich\" layers\n",
    "There are some common patterns of layers that are frequently used in neural nets. For example, affine layers are frequently followed by a ReLU nonlinearity. To make these common patterns easy, we define several convenience layers in the file `cs231n/layer_utils.py`.\n",
    "\n",
    "For now take a look at the `affine_relu_forward` and `affine_relu_backward` functions, and run the following to numerically gradient check the backward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from cs231n.layer_utils import affine_relu_forward, affine_relu_backward\n",
    "\n",
    "x = np.random.randn(2, 3, 4)\n",
    "w = np.random.randn(12, 10)\n",
    "b = np.random.randn(10)\n",
    "dout = np.random.randn(2, 10)\n",
    "\n",
    "out, cache = affine_relu_forward(x, w, b)\n",
    "dx, dw, db = affine_relu_backward(dout, cache)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: affine_relu_forward(x, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: affine_relu_forward(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: affine_relu_forward(x, w, b)[0], b, dout)\n",
    "\n",
    "print ('Testing affine_relu_forward:')\n",
    "print ('dx error: ', rel_error(dx_num, dx))\n",
    "print ('dw error: ', rel_error(dw_num, dw))\n",
    "print ('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss layers: Softmax and SVM\n",
    "You implemented these loss functions in the last assignment, so we'll give them to you for free here. You should still make sure you understand how they work by looking at the implementations in `cs231n/layers.py`.\n",
    "\n",
    "You can make sure that the implementations are correct by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_classes, num_inputs = 10, 50\n",
    "x = 0.001 * np.random.randn(num_inputs, num_classes)\n",
    "y = np.random.randint(num_classes, size=num_inputs)\n",
    "\n",
    "dx_num = eval_numerical_gradient(lambda x: svm_loss(x, y)[0], x, verbose=False)\n",
    "loss, dx = svm_loss(x, y)\n",
    "\n",
    "# Test svm_loss function. Loss should be around 9 and dx error should be 1e-9\n",
    "print ('Testing svm_loss:')\n",
    "print ('loss: ', loss)\n",
    "print ('dx error: ', rel_error(dx_num, dx))\n",
    "\n",
    "dx_num = eval_numerical_gradient(lambda x: softmax_loss(x, y)[0], x, verbose=False)\n",
    "loss, dx = softmax_loss(x, y)\n",
    "\n",
    "# Test softmax_loss function. Loss should be 2.3 and dx error should be 1e-8\n",
    "print ('\\nTesting softmax_loss:')\n",
    "print ('loss: ', loss)\n",
    "print ('dx error: ', rel_error(dx_num, dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-layer network\n",
    "In the previous assignment you implemented a two-layer neural network in a single monolithic class. Now that you have implemented modular versions of the necessary layers, you will reimplement the two layer network using these modular implementations.\n",
    "\n",
    "Open the file `cs231n/classifiers/fc_net.py` and complete the implementation of the `TwoLayerNet` class. This class will serve as a model for the other networks you will implement in this assignment, so read through it to make sure you understand the API. You can run the cell below to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N, D, H, C = 3, 5, 50, 7\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=N)\n",
    "\n",
    "std = 1e-2\n",
    "model = TwoLayerNet(input_dim=D, \n",
    "                    hidden_dim=H, \n",
    "                    num_classes=C, \n",
    "                    weight_scale=std)\n",
    "\n",
    "print ('Testing initialization ... ')\n",
    "W1_std = abs(model.params['W1'].std() - std)\n",
    "b1 = model.params['b1']\n",
    "W2_std = abs(model.params['W2'].std() - std)\n",
    "b2 = model.params['b2']\n",
    "assert W1_std < std / 10, 'First layer weights do not seem right'\n",
    "assert np.all(b1 == 0), 'First layer biases do not seem right'\n",
    "assert W2_std < std / 10, 'Second layer weights do not seem right'\n",
    "assert np.all(b2 == 0), 'Second layer biases do not seem right'\n",
    "\n",
    "print ('Testing test-time forward pass ... ')\n",
    "model.params['W1'] = np.linspace(-0.7, 0.3, num=D*H).reshape(D, H)\n",
    "model.params['b1'] = np.linspace(-0.1, 0.9, num=H)\n",
    "model.params['W2'] = np.linspace(-0.3, 0.4, num=H*C).reshape(H, C)\n",
    "model.params['b2'] = np.linspace(-0.9, 0.1, num=C)\n",
    "X = np.linspace(-5.5, 4.5, num=N*D).reshape(D, N).T\n",
    "scores = model.loss(X)\n",
    "correct_scores = np.asarray(\n",
    "  [[11.53165108,  12.2917344,   13.05181771,  13.81190102,  14.57198434, 15.33206765,  16.09215096],\n",
    "   [12.05769098,  12.74614105,  13.43459113,  14.1230412,   14.81149128, 15.49994135,  16.18839143],\n",
    "   [12.58373087,  13.20054771,  13.81736455,  14.43418138,  15.05099822, 15.66781506,  16.2846319 ]])\n",
    "scores_diff = np.abs(scores - correct_scores).sum()\n",
    "assert scores_diff < 1e-6, 'Problem with test-time forward pass'\n",
    "\n",
    "print ('Testing training loss (no regularization)')\n",
    "y = np.asarray([0, 5, 1])\n",
    "loss, grads = model.loss(X, y)\n",
    "correct_loss = 3.4702243556\n",
    "assert abs(loss - correct_loss) < 1e-10, 'Problem with training-time loss'\n",
    "\n",
    "model.reg = 1.0\n",
    "loss, grads = model.loss(X, y)\n",
    "correct_loss = 26.5948426952\n",
    "assert abs(loss - correct_loss) < 1e-10, 'Problem with regularization loss'\n",
    "\n",
    "for reg in [0.0, 0.7]:\n",
    "  print ('Running numeric gradient check with reg = ', reg)\n",
    "  model.reg = reg\n",
    "  loss, grads = model.loss(X, y)\n",
    "\n",
    "  for name in sorted(grads):\n",
    "    f = lambda _: model.loss(X, y)[0]\n",
    "    grad_num = eval_numerical_gradient(f, model.params[name], verbose=False)\n",
    "    print ('%s relative error: %.2e' % (name, rel_error(grad_num, grads[name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solver\n",
    "In the previous assignment, the logic for training models was coupled to the models themselves. Following a more modular design, for this assignment we have split the logic for training models into a separate class.\n",
    "\n",
    "Open the file `cs231n/solver.py` and read through it to familiarize yourself with the API. After doing so, use a `Solver` instance to train a `TwoLayerNet` that achieves at least `50%` accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = TwoLayerNet(hidden_dim=200, reg=0.5)\n",
    "solver = None\n",
    "\n",
    "##############################################################################\n",
    "# TODO: Use a Solver instance to train a TwoLayerNet that achieves at least  #\n",
    "# 50% accuracy on the validation set.                                        #\n",
    "##############################################################################\n",
    "optim_config = {'learning_rate': 0.0001}\n",
    "solver = Solver(model, data, \n",
    "                batch_size=100, \n",
    "                optim_config=optim_config, \n",
    "                lr_decay=0.99,\n",
    "                num_epochs=80,\n",
    "                verbose=False,)\n",
    "solver.train()\n",
    "\n",
    "# 无论怎么调整都是 val_acc = 0.47\n",
    "# 目前是 17000 训练数据, \n",
    "# 需要加大, 但内存不够\n",
    "##############################################################################\n",
    "#                             END OF YOUR CODE                               #\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run this cell to visualize training loss and train / val accuracy\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title('Accuracy')\n",
    "plt.plot(solver.train_acc_history, '-o', label='train')\n",
    "plt.plot(solver.val_acc_history, '-o', label='val')\n",
    "plt.plot([0.5] * len(solver.val_acc_history), 'k--')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='lower right')\n",
    "plt.gcf().set_size_inches(15, 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer network\n",
    "Next you will implement a fully-connected network with an arbitrary number of hidden layers.\n",
    "\n",
    "Read through the `FullyConnectedNet` class in the file `cs231n/classifiers/fc_net.py`.\n",
    "\n",
    "Implement the initialization, the forward pass, and the backward pass. For the moment don't worry about implementing dropout or batch normalization; we will add those features soon.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial loss and gradient check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, run the following to check the initial loss and to gradient check the network both with and without regularization. Do the initial losses seem reasonable?\n",
    "\n",
    "For gradient checking, you should expect to see errors around 1e-6 or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N, D, H1, H2, C = 2, 15, 20, 30, 10\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=(N,))\n",
    "\n",
    "for reg in [0, 3.14]:\n",
    "  print ('Running check with reg = ', reg)\n",
    "  model = FullyConnectedNet([H1, H2], input_dim=D, num_classes=C,\n",
    "                            reg=reg, weight_scale=5e-2, dtype=np.float64)\n",
    "\n",
    "  loss, grads = model.loss(X, y)\n",
    "  print ('Initial loss: ', loss)\n",
    "\n",
    "  for name in sorted(grads):\n",
    "    f = lambda _: model.loss(X, y)[0]\n",
    "    grad_num = eval_numerical_gradient(f, model.params[name], verbose=False, h=1e-5)\n",
    "    print ('%s relative error: %.2e' % (name, rel_error(grad_num, grads[name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As another sanity check, make sure you can overfit a small dataset of 50 images. First we will try a three-layer network with 100 units in each hidden layer. You will need to tweak the learning rate and initialization scale, but you should be able to overfit and achieve 100% training accuracy within 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO: Use a three-layer Net to overfit 50 training examples.\n",
    "\n",
    "num_train = 50\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "weight_scale = 1e-2\n",
    "learning_rate = 1e-2\n",
    "\n",
    "model = FullyConnectedNet([100, 100],\n",
    "              weight_scale=weight_scale, dtype=np.float64)\n",
    "solver = Solver(model, small_data,\n",
    "                print_every=10, num_epochs=20, batch_size=25,\n",
    "                update_rule='sgd',\n",
    "                optim_config={\n",
    "                  'learning_rate': learning_rate,\n",
    "                }\n",
    "         )\n",
    "solver.train()\n",
    "\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.title('Training loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Training loss')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(solver.train_acc_history, 'o-', color='red')\n",
    "plt.title('Training acc history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Training acc')\n",
    "plt.show()\n",
    "\n",
    "# val_acc_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try to use a five-layer network with 100 units on each layer to overfit 50 training examples. Again you will have to adjust the learning rate and weight initialization, but you should be able to achieve 100% training accuracy within 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Use a five-layer Net to overfit 50 training examples.\n",
    "\n",
    "num_train = 50\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "learning_rate = 1e-2\n",
    "weight_scale = 6e-2  # 5层之后, 这个参数特别敏感, 为什么?\n",
    "weight_scale = 1e-2  # 5层之后, 这个参数特别敏感, 为什么?\n",
    "# weight_scale = 8e-2  # 5层之后, 这个参数特别敏感, 为什么?\n",
    "model = FullyConnectedNet([100, 100, 100, 100],\n",
    "                weight_scale=weight_scale, dtype=np.float64)\n",
    "solver = Solver(model, small_data,\n",
    "                print_every=10, num_epochs=20, batch_size=25,\n",
    "                update_rule='sgd',\n",
    "                optim_config={\n",
    "                  'learning_rate': learning_rate,\n",
    "                }\n",
    "         )\n",
    "print_weight(model.params['W1'])\n",
    "solver.train()\n",
    "print_weight(model.params['W1'])\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.title('Training loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Training loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W1 = model.params['W1']\n",
    "def print_weight(W):\n",
    "  print(W1.shape)\n",
    "  print(model.num_laoyers)\n",
    "  plt.imshow(W.T[:, :150], interpolation='none')\n",
    "  plt.axis('off')\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "\n",
    "print_weight(W1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inline question: \n",
    "Did you notice anything about the comparative difficulty of training the three-layer net vs training the five layer net?\n",
    "\n",
    "# Answer:\n",
    "\n",
    "learning_rate 没看出区别, weight_scale 在 layer5 时非常敏感, 选择 0.04/0.06/0.08都造成很大变化. \n",
    "\n",
    "如果选择了不好的参数, 则loss一直不下降. 可能因为梯度太小了, 反向传播回来没有实质的权重更新? 为什么weight_scale选错了, dW就会特别小呢?\n",
    "\n",
    "如果 weight_scale 比较大, 训练后的 W1 会很稀疏, W1.shape=D,H1 可能是仅使用了通向 H1=100 的极少数数据, 大部分都是0? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update rules\n",
    "So far we have used vanilla stochastic gradient descent (SGD) as our update rule. More sophisticated update rules can make it easier to train deep networks. We will implement a few of the most commonly used update rules and compare them to vanilla SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD+Momentum\n",
    "Stochastic gradient descent with momentum is a widely used update rule that tends to make deep networks converge faster than vanilla stochstic gradient descent.\n",
    "\n",
    "Open the file `cs231n/optim.py` and read the documentation at the top of the file to make sure you understand the API. Implement the SGD+momentum update rule in the function `sgd_momentum` and run the following to check your implementation. You should see errors less than 1e-8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from cs231n.optim import sgd_momentum\n",
    "\n",
    "N, D = 4, 5\n",
    "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
    "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
    "v = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n",
    "\n",
    "config = {'learning_rate': 1e-3, 'velocity': v}\n",
    "next_w, _ = sgd_momentum(w, dw, config=config)\n",
    "\n",
    "expected_next_w = np.asarray([\n",
    "  [ 0.1406,      0.20738947,  0.27417895,  0.34096842,  0.40775789],\n",
    "  [ 0.47454737,  0.54133684,  0.60812632,  0.67491579,  0.74170526],\n",
    "  [ 0.80849474,  0.87528421,  0.94207368,  1.00886316,  1.07565263],\n",
    "  [ 1.14244211,  1.20923158,  1.27602105,  1.34281053,  1.4096    ]])\n",
    "expected_velocity = np.asarray([\n",
    "  [ 0.5406,      0.55475789,  0.56891579, 0.58307368,  0.59723158],\n",
    "  [ 0.61138947,  0.62554737,  0.63970526,  0.65386316,  0.66802105],\n",
    "  [ 0.68217895,  0.69633684,  0.71049474,  0.72465263,  0.73881053],\n",
    "  [ 0.75296842,  0.76712632,  0.78128421,  0.79544211,  0.8096    ]])\n",
    "\n",
    "print ('next_w error: ', rel_error(next_w, expected_next_w))\n",
    "print ('velocity error: ', rel_error(expected_velocity, config['velocity']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have done so, run the following to train a six-layer network with both SGD and SGD+momentum. You should see the SGD+momentum update rule converge faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_train = 4000\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "solvers = {}\n",
    "\n",
    "for update_rule in ['sgd', 'sgd_momentum']:\n",
    "  print ('running with ', update_rule)\n",
    "  model = FullyConnectedNet([100, 100, 100, 100, 100], weight_scale=5e-2)\n",
    "\n",
    "  solver = Solver(model, small_data,\n",
    "                  num_epochs=5, batch_size=100,\n",
    "                  update_rule=update_rule,\n",
    "                  optim_config={\n",
    "                    'learning_rate': 1e-2,\n",
    "                  },\n",
    "                  verbose=True)\n",
    "  solvers[update_rule] = solver\n",
    "  solver.train()\n",
    "  print ('-----')\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Training accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "for update_rule, solver in solvers.items():\n",
    "  plt.subplot(3, 1, 1)\n",
    "  plt.plot(solver.loss_history, 'o', label=update_rule)\n",
    "  \n",
    "  plt.subplot(3, 1, 2)\n",
    "  plt.plot(solver.train_acc_history, '-o', label=update_rule)\n",
    "\n",
    "  plt.subplot(3, 1, 3)\n",
    "  plt.plot(solver.val_acc_history, '-o', label=update_rule)\n",
    "  \n",
    "for i in [1, 2, 3]:\n",
    "  plt.subplot(3, 1, i)\n",
    "  plt.legend(loc='upper center', ncol=4)\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSProp and Adam\n",
    "RMSProp [1] and Adam [2] are update rules that set per-parameter learning rates by using a running average of the second moments of gradients.\n",
    "\n",
    "In the file `cs231n/optim.py`, implement the RMSProp update rule in the `rmsprop` function and implement the Adam update rule in the `adam` function, and check your implementations using the tests below.\n",
    "\n",
    "[1] Tijmen Tieleman and Geoffrey Hinton. \"Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude.\" COURSERA: Neural Networks for Machine Learning 4 (2012).\n",
    "\n",
    "[2] Diederik Kingma and Jimmy Ba, \"Adam: A Method for Stochastic Optimization\", ICLR 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test RMSProp implementation; you should see errors less than 1e-7\n",
    "from cs231n.optim import rmsprop\n",
    "\n",
    "N, D = 4, 5\n",
    "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
    "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
    "cache = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n",
    "\n",
    "config = {'learning_rate': 1e-2, 'cache': cache}\n",
    "next_w, _ = rmsprop(w, dw, config=config)\n",
    "\n",
    "expected_next_w = np.asarray([\n",
    "  [-0.39223849, -0.34037513, -0.28849239, -0.23659121, -0.18467247],\n",
    "  [-0.132737,   -0.08078555, -0.02881884,  0.02316247,  0.07515774],\n",
    "  [ 0.12716641,  0.17918792,  0.23122175,  0.28326742,  0.33532447],\n",
    "  [ 0.38739248,  0.43947102,  0.49155973,  0.54365823,  0.59576619]])\n",
    "expected_cache = np.asarray([\n",
    "  [ 0.5976,      0.6126277,   0.6277108,   0.64284931,  0.65804321],\n",
    "  [ 0.67329252,  0.68859723,  0.70395734,  0.71937285,  0.73484377],\n",
    "  [ 0.75037008,  0.7659518,   0.78158892,  0.79728144,  0.81302936],\n",
    "  [ 0.82883269,  0.84469141,  0.86060554,  0.87657507,  0.8926    ]])\n",
    "\n",
    "print ('next_w error: ', rel_error(expected_next_w, next_w))\n",
    "print ('cache error: ', rel_error(expected_cache, config['cache']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test Adam implementation; you should see errors around 1e-7 or less\n",
    "from cs231n.optim import adam\n",
    "\n",
    "N, D = 4, 5\n",
    "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
    "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
    "m = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n",
    "v = np.linspace(0.7, 0.5, num=N*D).reshape(N, D)\n",
    "\n",
    "config = {'learning_rate': 1e-2, 'm': m, 'v': v, 't': 5}\n",
    "next_w, _ = adam(w, dw, config=config)\n",
    "\n",
    "expected_next_w = np.asarray([\n",
    "  [-0.40094747, -0.34836187, -0.29577703, -0.24319299, -0.19060977],\n",
    "  [-0.1380274,  -0.08544591, -0.03286534,  0.01971428,  0.0722929],\n",
    "  [ 0.1248705,   0.17744702,  0.23002243,  0.28259667,  0.33516969],\n",
    "  [ 0.38774145,  0.44031188,  0.49288093,  0.54544852,  0.59801459]])\n",
    "expected_v = np.asarray([\n",
    "  [ 0.69966,     0.68908382,  0.67851319,  0.66794809,  0.65738853,],\n",
    "  [ 0.64683452,  0.63628604,  0.6257431,   0.61520571,  0.60467385,],\n",
    "  [ 0.59414753,  0.58362676,  0.57311152,  0.56260183,  0.55209767,],\n",
    "  [ 0.54159906,  0.53110598,  0.52061845,  0.51013645,  0.49966,   ]])\n",
    "expected_m = np.asarray([\n",
    "  [ 0.48,        0.49947368,  0.51894737,  0.53842105,  0.55789474],\n",
    "  [ 0.57736842,  0.59684211,  0.61631579,  0.63578947,  0.65526316],\n",
    "  [ 0.67473684,  0.69421053,  0.71368421,  0.73315789,  0.75263158],\n",
    "  [ 0.77210526,  0.79157895,  0.81105263,  0.83052632,  0.85      ]])\n",
    "\n",
    "print ('next_w error: ', rel_error(expected_next_w, next_w))\n",
    "print ('v error: ', rel_error(expected_v, config['v']))\n",
    "print ('m error: ', rel_error(expected_m, config['m']))\n",
    "\n",
    "print(expected_next_w)\n",
    "print('   ')\n",
    "print(next_w)\n",
    "print(expected_next_w - next_w)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have debugged your RMSProp and Adam implementations, run the following to train a pair of deep networks using these new update rules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learning_rates = {'rmsprop': 1e-4, 'adam': 1e-3}\n",
    "for update_rule in ['adam', 'rmsprop']:\n",
    "  print ('running with ', update_rule)\n",
    "  model = FullyConnectedNet([100, 100, 100, 100, 100], weight_scale=5e-2)\n",
    "\n",
    "  solver = Solver(model, small_data,\n",
    "                  num_epochs=5, batch_size=100,\n",
    "                  update_rule=update_rule,\n",
    "                  optim_config={\n",
    "                    'learning_rate': learning_rates[update_rule]\n",
    "                  },\n",
    "                  verbose=True)\n",
    "  solvers[update_rule] = solver\n",
    "  solver.train()\n",
    "  print('-----')\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Training accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "for update_rule, solver in solvers.items():\n",
    "  plt.subplot(3, 1, 1)\n",
    "  plt.plot(solver.loss_history, 'o', label=update_rule)\n",
    "  \n",
    "  plt.subplot(3, 1, 2)\n",
    "  plt.plot(solver.train_acc_history, '-o', label=update_rule)\n",
    "\n",
    "  plt.subplot(3, 1, 3)\n",
    "  plt.plot(solver.val_acc_history, '-o', label=update_rule)\n",
    "  \n",
    "for i in [1, 2, 3]:\n",
    "  plt.subplot(3, 1, i)\n",
    "  plt.legend(loc='upper center', ncol=4)\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a good model!\n",
    "Train the best fully-connected model that you can on CIFAR-10, storing your best model in the `best_model` variable. We require you to get at least 50% accuracy on the validation set using a fully-connected net.\n",
    "\n",
    "If you are careful it should be possible to get accuracies above 55%, but we don't require it for this part and won't assign extra credit for doing so. Later in the assignment we will ask you to train the best convolutional network that you can on CIFAR-10, and we would prefer that you spend your effort working on convolutional nets rather than fully-connected nets.\n",
    "\n",
    "You might find it useful to complete the `BatchNormalization.ipynb` and `Dropout.ipynb` notebooks before completing this part, since those techniques can help you train powerful models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "import random\n",
    "\n",
    "\n",
    "def clip_small_data(data, num_train=500, num_val=500):\n",
    "  small_data = {\n",
    "    'X_train': data['X_train'][:num_train],\n",
    "    'y_train': data['y_train'][:num_train],\n",
    "    'X_val': data['X_val'][:num_val],\n",
    "    'y_val': data['y_val'][:num_val],\n",
    "  }\n",
    "  return small_data\n",
    "\n",
    "def random_select(a, b=None, kind='exp'):\n",
    "  if kind == 'exp':\n",
    "    p = 10 ** random.uniform(math.log10(a), math.log10(b))\n",
    "  elif kind == 'linear':\n",
    "    p = random.uniform(a, b)\n",
    "#   print('selecting', p)\n",
    "  return p\n",
    "\n",
    "\n",
    "def random_search(max_search_times=20,\n",
    "                  report_val_acc=0.2,\n",
    "                  num_epochs=5,\n",
    "                  lr_decays=(0.5, 0.99),   # 0.99\n",
    "                  weight_scales=(0.001, 0.1),  # 5e-2\n",
    "                  regs=(0.1, 1, 'linear'),  # 0.01\n",
    "                  learning_rates=(0.001, 0.1),  # 1e-2\n",
    "                  ):\n",
    "  \n",
    "  small_data = clip_small_data(data)\n",
    "  print('small_data X_train shape', small_data['X_train'].shape)\n",
    "  results = []\n",
    "  for i in range(max_search_times):\n",
    "    model = FullyConnectedNet([100, 100, 100, 100, 100], \n",
    "                              weight_scale=random_select(*weight_scales),\n",
    "                              reg=random_select(*regs))\n",
    "    if i % 10 == 0:\n",
    "      print('try params ', i+1, max_search_times)\n",
    "    solver = Solver(model, \n",
    "                    small_data,\n",
    "                    num_epochs=num_epochs, \n",
    "                    batch_size=100,\n",
    "                    update_rule='adam',\n",
    "                    lr_decay=random_select(*lr_decays),\n",
    "                    optim_config={\n",
    "                      'learning_rate': random_select(*learning_rates),\n",
    "                    },\n",
    "                    verbose=False)\n",
    "    solver.train()\n",
    "    val_acc = max(solver.val_acc_history)\n",
    "#     if val_acc >= stop_val_acc:\n",
    "#       break\n",
    "    if val_acc >= report_val_acc:\n",
    "      results.append(solver.report_hyper_params(verbose=True))\n",
    "    else:\n",
    "      results.append(solver.report_hyper_params(verbose=False))\n",
    "\n",
    "  print('done', max_search_times)\n",
    "  return results\n",
    "\n",
    "\n",
    "def print_results(results):\n",
    "  from pprint import pprint\n",
    "#   results = sorted(results, key=lambda x: x['last_val_acc'], reverse=True)\n",
    "#   for r in results:\n",
    "#     pprint(r)\n",
    "  ys = [r['last_val_acc'] for r in results]\n",
    "    \n",
    "  plt.subplot(4, 1, 1)\n",
    "  xs = [r['init_learning_rate'] for r in results]\n",
    "  plt.plot(xs, ys, 'o')\n",
    "  plt.xlabel('init_learning_rate')\n",
    "  plt.ylabel('val acc')\n",
    "  \n",
    "  plt.subplot(4, 1, 2)\n",
    "  xs = [r['lr_decay'] for r in results]\n",
    "  plt.plot(xs, ys, 'o')\n",
    "#   plt.title('lr_decay')\n",
    "  plt.xlabel('lr_decay')\n",
    "  plt.ylabel('val acc')\n",
    "  \n",
    "  plt.subplot(4, 1, 3)\n",
    "  xs = [r['reg'] for r in results]\n",
    "  plt.plot(xs, ys, 'o')\n",
    "#   plt.title('reg')\n",
    "  plt.xlabel('reg')\n",
    "  plt.ylabel('val acc')\n",
    "  \n",
    "  plt.subplot(4, 1, 4)\n",
    "  xs = [r['weight_scale'] for r in results]\n",
    "  plt.plot(xs, ys, 'o')\n",
    "#   plt.title('weight_scale')\n",
    "  plt.xlabel('weight_scale')\n",
    "  plt.ylabel('val acc')\n",
    "  \n",
    "  plt.gcf().set_size_inches(12, 18)\n",
    "  plt.show()\n",
    "  \n",
    "  \n",
    "  \n",
    "    \n",
    "def print_solver(solver):\n",
    "  \n",
    "  plt.subplot(2, 1, 1)\n",
    "  plt.title('Training loss')\n",
    "  plt.plot(solver.loss_history, 'o')\n",
    "  plt.xlabel('Iteration')\n",
    "\n",
    "  plt.subplot(2, 1, 2)\n",
    "  plt.title('Accuracy')\n",
    "  plt.plot(solver.train_acc_history, '-o', label='train')\n",
    "  plt.plot(solver.val_acc_history, '-o', label='val')\n",
    "  plt.plot([0.5] * len(solver.val_acc_history), 'k--')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.legend(loc='lower right')\n",
    "  plt.gcf().set_size_inches(15, 12)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small_data X_train shape (500, 3, 32, 32)\n",
      "try params  1 200\n",
      "try params  11 200\n",
      "try params  21 200\n",
      "try params  31 200\n",
      "try params  41 200\n",
      "try params  51 200\n",
      "try params  61 200\n",
      "try params  71 200\n",
      "try params  81 200\n",
      "try params  91 200\n",
      "try params  101 200\n",
      "try params  111 200\n",
      "try params  121 200\n",
      "try params  131 200\n",
      "try params  141 200\n",
      "try params  151 200\n",
      "try params  161 200\n",
      "try params  171 200\n",
      "try params  181 200\n",
      "try params  191 200\n",
      "done 200\n",
      "200\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuYAAAQYCAYAAABC0RwAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzs3XuUHFd57/3fIwQxlmzLhMQsyboM4ySYO06CRewXRtgC\nJywksHNYwAhsCDZrJchgLDDJYaKZIwKB5cMl5uR944REXBzuwSY54SLHjBw5OBiwjW3ZXEYzAiQw\nh4WvMidc9Lx/dI/V6umeru6uXbV31fezVi9N36qefamarZpn1zZ3FwAAAIByLSk7AAAAAAAMzAEA\nAIAoMDAHAAAAIsDAHAAAAIgAA3MAAAAgAkvLDiAPZsatZQAAABCcu1uobVdiYC5J3PYRrSYnJzU5\nOVl2GIgM/QKd0C/QCf0CnZgFG5NLIpUFAAAAiAIDcwAAACACDMxRSWNjY2WHgAjRL9AJ/QKd0C9Q\nBqtCbraZeRXKAQAAgHiZWdDJn1wxBwAAACLAwBwAAACIAANzAAAAIAIMzAEAAIAIMDAHAAAAIsDA\nHAAAAIgAA3MAAAAgAgzMAQAAgAgwMAcAAAAiwMAcAAAAiAADcwAAACACDMwBAACACAQfmJvZOWZ2\nl5l9y8wu6/D+JjO71cxuNrOvmNkZbe8vMbOvm9lnQ8cKAAAAlMXcPdzGzZZI+paksyQdlHSTpJe6\n+10tnznW3R9q/vwUSZ9w91Nb3r9E0m9LOt7dN3XZj4csBwAAVTQ7u18TEzt14MBhrVq1RDt2XKCR\nkbVlhwVEy8zk7hZq+0tDbbjpmZK+7e77JcnMPiZps6SHB+bzg/Km5ZIOzz8xs5Ml/YGkv5D0xsCx\nAgBQG7Oz+7Vx4xWamZmStEzSId1443bt2rWVwTlQktCpLKskfa/l+febrx3FzF5kZndK+mdJr255\n6z2S3iSJy+EAAORoYmJny6BckpZpZmZKExM7S4wKqLfQV8wzcferJV1tZmdKepukjWb2Akl3u/st\nZjYmadE/G0xOTj7889jYmMbGxoLFCwBA6g4cOKwjg/J5y3Tw4OFOHwdqaXp6WtPT04XtL/TA/ICk\nNS3PT26+1pG77zGzx5vZYySdIWmTmf2BpEdLOs7MPuTur+z03daBOQAAWNyqVUskHdLRg/NDWrmS\nG7YB89ov9k5NTQXdX+jJn4+Q9E01Jn/+QNJXJL3M3e9s+cyou880fz5N0jXuvrptO8+RdCmTPwEA\nyEenHPPRUXLMgcUkPfnT3X9pZq+T9EU18tk/4O53mtlrG2/7lZLOM7NXSvqZpJ9KeknImAAAgDQy\nsla7dm3VxMTlOnjwsFauXKIdOxiUA2UKesW8KFwxBwAAQGihr5iTSAYAAABEgIE5AAAAEAEG5gAA\nAEAEGJgDAAAAEWBgDgAAAESAgTkAAAAQAQbmAAAAQAQYmAMAAAARYGAOAAAARICBOQAAABCBpWUH\nAGA4s7P7NTGxUwcOHNaqVUu0Y8cFGhlZW3ZYAACgT+buZccwNDPzKpQD6Nfs7H5t3HiFZmamJC2T\ndEijo9u1a9dWBucAAOTMzOTuFmr7pLIACZuY2NkyKJekZZqZmdLExM4SowIAAINgYA4k7MCBwzoy\nKJ+3TAcPHi4jHAAAMAQG5kDCVq1aIulQ26uHtHIlhzYAAKnhtzeQsB07LtDo6HYdGZw3csx37Lig\ntJgAAMBgmPwJJG7+riwHDx7WypXclQUAgFBCT/5kYA4AAABkwF1ZAAAAgBpggSEAQ2GBIwAA8kEq\nC4CBscARAKBOSGUBEC0WOAIAID8MzAEMjAWOAADIT6VzzMl9RUpS7K9HFjhqHZyzwBEAAIOobI45\nua9ISar9NdW4AQAYBPcxz6DTwHzLlildddU2tV/JGx+/XB/5yPZC4wN6Sbm/ssARAKAuQg/MK5vK\nQu4rUpJyfx0ZWRv9fx4AAEhBZQfm5L4iJfRXoDgpzucAUA+VTWUh9xUpob8CxeBYAzAMcswz6LbA\nELmvSAn9FQgv5fkcAMpHjvkQyH1FSuivQHgpz+cAUH0ksAIAauPIfI5WzOcAEIdKp7Kgeuo4aauO\nZQZCuf76G/SCF7xTDz74VEmPlPQSjY5+gBxzAJmQygI0dZq0deON1Z60VccyA6HMzu7Xq1/9GT34\n4Ec1fzwtX75Vf//3f8TxBCAK/O0OyZiY2NkyQJWkZZqZmdLExM4SowqrjmUGQul0PD344BW68spr\nywwLAB4WfGBuZueY2V1m9i0zu6zD+5vM7FYzu9nMvmJmZzRfP9nMrjOzO8zsNjO7OHSsiFsdJ23V\nscxAKBxPAGIXNJXFzJZIer+ksyQdlHSTmV3j7ne1fOxad/9s8/NPkfQJSadK+oWkN7r7LWa2XNLX\nzOyLbd9NShG5wlXOR67jIjx1LDMQSpnHU5XPzQBy5O7BHpLWS/pcy/O3SLpskc8/S9IdXd67WtJZ\nXd7z2O3bN+ejo5e69KBL7tKDPjp6qe/bN5fUPspU9fJ1UscyA6GUdTxxHAPV0RxzBhs7B70ri5md\nJ+n57n5R8/kWSc9094vbPvciSe+Q9GuSXuDu/9n2/jpJ05Ke7O4PdtiPhyxHHopY1KIOC2fUcRGe\nOpYZCKWM46kO52agLmpxVxZ3v1rS1WZ2pqS3Sdo4/14zjeVTkl7faVA+b3Jy8uGfx8bGNDY2Firc\ngRSR21iH/Mk6LsJTxzIDoZRxPNXh3AxU1fT0tKanpwvbX+iB+QFJa1qen9x8rSN332Nmjzezx7j7\nT8xsqRqD8g+7+zWL7ah1YB6jInIbyUeuhzrlqtaprKguzs1Autov9k5NTYXdYcg8GUmPkPQdSWsl\nPUrSLZJObfvMaMvPp0n6XsvzD0l6d4b9DJgpVBxyzJGHOrVxncqKaqMvA9WhlHPMpcbtEiW9T41b\nM37A3f/SzF7bLNiVZvZmSa+U9DNJP5W0zd2/3Lxt4vWSbpPkzcefufvnO+zDQ5cjD0XkNpKPXG11\nylWtU1lRfZybgWpIPse8OZD+rbbX/qbl53dJeleH792gxhX3yigit5F85GqrU65qncqK6uPcDCAL\nEtyAhBzJVW1VzVzVOpUVAABJ4VNZipBKKksZek2eS2VyXVXKMazZ2f3auPGKlmXFD2l0dLt27do6\ncHljrbujy/pjSX+nY47Zp+c9b6Xe+97XRREjjhZrXwKAvIROZQk6+bOohxKY/FmGXhOOUpmQVJVy\n5GXfvjkfH5/0DRv+3MfHJ4cqZ+x1t2/fnG/e/AZ/9KNfFW2MaIi9LwFAHhR48mfpg+pcCsHAvKPx\n8cmWX5L+8C/L8fHJTO/HoirliFEKdZdCjKCdANRD6IE5yZoV1mvyXCqT66pSjhilUHcpxAjaCQDy\nwMC8D7Oz+7Vly5Q2bNiuLVumNDu7v+yQFtVr8lwqk+uqUo5eiuxf8/vau3dW0oSk1n3FVXdVad+q\no50AIAchL8cX9VABqSwp5k9WJTe7KuVYTJFl6LQv6RKX5qKsuyq0bx3QTgDqQKkvMFSEIu7Kkupi\nJ70WtUhl0YuqlKObIvtXt32ddNIrdPbZT4uy7lJv37qgnQBUXei7sjAwz2jDhu2anp7q+Pp11y18\nHehHkf2LvgwAwGCSX/mzKo7kTx59lbHo/EnuE1xNRfavWPoygHjxuwYoB1fMMwqxsEuKMSCMItuW\nfgRgMZwjgO5IZcmgqJU/y86fTDXPHdkU2b/K7ssA4sXvGqA7UlkiMjKyttSTEvcJrrYi+1fZfRlA\nvPhdA5SHpNKEcJ9gAEBo/K4BykMqS0HymEhD3h+QTRkT1/LaJ5PuUDZ+1yAvVTyfkWOeQewD8zxP\ncuQGA4srY1CR1z4ZECEW/K7BsKp6Pgs9MC991c48Hipg5c9hjI9PtqyGN/940MfHJ8sODaicMo63\nvPbJuQJAVVT1fKbAK3+SMFYAJtIAxSnjeMtrn5wrAFQF57PBcFeWArCgy/CqmKe2mJTLGyL2LNuc\n/8zevbOSJiS9RtL8Z8Ieb3kd47GdK1Luh8A8+nE4i9VtbOezZIS8HF/UQ5GnsuzbN+ejo5e2/Enn\nQR8dvdT37ZsrO7Qk1K3+Ui5viNizbLPTZ6RLXJorpP7yKndMbR9TLMCg6Mfh9Krbqta9AqeylD6o\nzqUQkQ/M3RsddHx80jds+HMfH59MvmMWqap5at2kXN4QsWfZZrfPnHTSiws73vI6xmM5V6TcD4F5\n9ONwstRtLOezPIUemJPKUhAWdBlc3fLUUi5viNizbLPbZ574xKckt2hTLOeKlPshMI9+HE6Wuo3l\nfJYSEn0QvbotdpFyeUPEnmWbKddZrKhTVAH9OBzqNpBel9QlvVjSCS3PV0h6UcjL+P0+lEAqy6Dm\n/ww0Nlb+n4HKiqWqeWrdpFzemHLMU6mzWFGn9RHT75m8devHu3fvqWyZ+zFM29f1HKHAqSw9Fxgy\ns1vc/eltr93s7s8I8R+FQcS+wNCgYro5f9mx1G2xi5TLGyL2LNtMuc5iRZ1WX9nn9iK09+OLLjpb\nr371Zypd5izyaPs6niNKX2BI0jc6vHZbyP8t9PtQRa+YxzRpJaZYAAD5qOO5vY5l7oR6GIwiWGDo\nq2b2bjMbbT7eLelrwf6ngIfFNGklplgAAPmo47m9jmXuhHqIU5a7smxVY7WOj0tySbsk/UnIoNCQ\n183581hcgYUCUCUsOIKqy9rHUzq3t5bp+OMfktkvdPfdj9IPf/gdnXTSap1yyomZjuVYy1z0eSnW\neqi9kJfji3qooqkseUysqOLCJ8Aw6Muoun76eCrHQ69FxKRLXdqbKfYYy1xGTDHWQwpU9gJDalwh\nX9Hy/ERJXwgZVN+FqOjA3H34m/PnmUNWxYUCUD/kVaLq+u3jKZzbu5VJmmz7OduxHFuZyzovxVYP\nKQg9MM+SyvJYd7+35Qr7PWb26zlfuEcXw96cP88cMhYKQBWQV4mq67ePp3Bu71Ym6XDbz9mO5djK\nXNZ5KbZ6QLYc88NmtsbdvytJZrZWjVxzJIAcMqSiqPzKYY+JWPLTy46j7P3XxSD1XMXzfrcyHVkn\ncf7nNMtZxTbDgHpdUpd0jqTvSvqwpI9I2i/p+SEv4/f7UIVTWYZFDhlSUGQ/HWZfsRxPZcdR9v7r\nYtB6rmL75JljHqMqtllVqewFhiTJzB4raX3z6Y3u/uMA/0cYWFUXGMpLHRcAQFq2bJnSVVdtU/vV\novHxy4P8mXXQY6LoOGONo+z918Uw9VzF835rmY47bv6uLI/UD384o8c9brVGR7PdlSVWVWyzKgq9\nwFCWVBZJ+qWkH0k6RtITm0Fdn+WLZnaOpPeq8TemD7j7O9ve3yRphxrJYT+XdIm735Dlu8iGHDLE\nruj8ykGPiVjy08uOo+z918Uw9VzF834Vy9Sq6uVDNj0H5mb2Gkmvl3SypFvUuHL+ZUnPzfDdJZLe\nL+ksSQcl3WRm17j7XS0fu9bdP9v8/FMkfULSqRm/C6ACUsmvjCXOsuMoe/91QT0D9dMzlcXMbpP0\nu2qksDzdzJ4g6e3ufm7PjZutl7Td3X+/+fwtauTmdLzybWbPkvR37v6kfr47n8oy/2eg73znHt19\n9/f0uMedotHRY4/6c1CICUu9ttnvPmOaVFVkLP3sa7HPxlR/ZRim/GXV3ezsfm3ceIVmZqbUGIQc\n0ujodu3atTWqtoslzrLjyGv/dT9Weym7nYtEX0hPzG0WMrbQqSxZJlbe1Pz3Fkm/0vz5jiwJ7JLO\nk3Rly/Mtkv6qw+deJOlOST+WdHo/322+1zJxYm9zEsjCCRQhJlf02ma/+4xpAkisE/IW+2xM9VeG\nlCc2pnI/3VjiLDuOYfdfdn9LRdntXAT6QnpibrPQsSmCBYY+I2mFpElJ10u6RtK/Ztp4H4Pr5vtn\nStrV73cltdycv/tN+kPcwL/XNvvdZ0yLnxQZSz/7WuyzMdVfGYYpf93rDsWiv2EefSE9MbdZ6NhC\nD8x75pi7+4ubP06a2ZcknSDp8xkvyB+QtKbl+cnN17rta4+ZPd7MHtPvd/fsuU6N26t/SdJzJI21\nvNuYLNMYw+c7YanX5Jx+J+/ENKmqyFj62ddinw3RxikZps1i6nuoPvob5tEX0hNzm+Ud2/T0tKan\np4cNK7Osd2WRJLn77j63f5OkU5qLEv1A0kslvaz1A2Y26u4zzZ9Pk/Qod/+JmfX8bqszz3yu9u/f\nJsnUSImXGrdc3ynp55qdvVNPe9oaLTaRJsRCDt3en529XRs2bF+wnyIn+/Qqb5Gx9LOv3p+Nd7JU\n6Jy8fuqxPZYTTrg/83eLEnMOI4bDxMYjypj7FJOi+kJKdRK7mI/fbrEdd9xD2rJlqu/2Hxsb09jY\n2MPPp6amco64TcjL8Y0r/jpH0jclfVvSW5qvvVbSRc2f3yzpdklfl3SDpGct9t0u++iQY74w13zN\nmot99eoLO+YdhVrIodP7S5ee34wv2+dD5G1l2Q855vkqIras++j0ucWOjzLE3JYYHu3bUMbcp9jE\ndG5ENjHXZ+jfbyo7xzyFR7OSHp4ks379633Zsud2zDHatGlbx4k0w+Qk9Zqc0/r+unXntgzKO++n\niMk+Wctb5MSjfva12GdjnSxVVE5elvJ3i6Xb8VGGmHMYkY9Yj9UilTH3KUah+0KKdRK7mI/f9tg2\nb35Dbu0femDeVypL7Fpvzr9hw3ZNTy/MMXrggWN1zTXbF3w35EIO7XHNzZ266H6KWGQga3mLXPCg\nn30t9tlYF2koKicvS/m7xdLt+ChDzDmMyEesx2qRQvTzFI+d0H0hxTqJXczHb3tsGzZsVyrt33Vg\nbmYPqDGbcsFbavxv4fhgUQ2oNX9sbu529ZP/dHRO0tG56bOz+3PLQxsmBzjPfLh+88PIzcteB90+\nl2dO3rDtEXN+4LwUYkSxqngeCtHPOXYWKqJO8uqfqfbzmONO6pgIeTm+qIeOyjGf/1PF3mYud7/3\nD+9+H/Q8DJMDXEYcRcSSgjzaLa96zGM7KbRpCjGiOFXtD+SYFyOm36llxhlK7HHnGZ9iyTGX9Otq\n3L5wjaQ1IYPquxBSl/yxvb5u3bmZ85/27Ztr5oCHzUMbJge46DiKiiV2Weug1+fyyMnLqz1izg+c\nl0KMKEaVz0Mh+jnHzkIh6ySv/plqP08h7rzaP/TAvGeOuZltkvQ/Ja2U9CNJa9VYpfNJAS7gD6xz\n/tipGhl5sq67LtutbUZG1mrduidrbi5sHtIwOcBFx1FULLHLWge9PpdHTl5e7RFzfuC8FGJEMap8\nHgrRzzl2FgpZJ3n1z1T7eQpxp3JMZEmu2SFpvaRvufuIpLMk3Rg0qgEcyR9q1X/+UF7bGVYsccQW\nS1my1kERdUV7oI7o94hZ1cYg/Uo17ij1uqQu6avNf2+VtGT+55CX8ft9qGOO+WB5t5s2bfNjjnm5\nS291ac6lB3358lf56ae/vtA/Bw5bnvk/2YyNDf8nu15503nsJ894Q8hrbkBrOTdvfoNv2rSt7zL3\n2zf6qdvY2wFhxNzu87GdfvrrffnyV0WbwxqbQdq0rH4QQ/8bNgZyzMuJu992y6Ovqewcc0nXSlou\n6QpJH5X0Pkn/ETKovgvRdh/zQfKHOnWqX/mV8/3Rjz7buy0GFNqg5Qk1mag9lrqdiLK2R7fPHV3O\nOZcuGfo/XlliYZIvFhNzuy+Mba8vX/5CX7/+TdH9ByImg7RpmQOrsvtfnr/L8shhTnV+QNFxD3KR\nKo92jmFgvkyNlJelks6XdLGkXw0ZVN+FaA7Mh9Ft4kLjyvnRr8U0maGToiZh1H2yS7+OLmd8bVSX\ndsDRYm73mGOL2SD1VlZdx9DGMcSA/vXbbnm1c+iBeZYFhl4r6ePufkDSB4fOnYlUt4kLC9Pw45rM\n0ElRkzDqPtmlX0eXM742qks74Ggxt3vMscVskHorq65jaOMYYkD/+m23VNo5y8D8OElfNLOfSPq4\npE+6+91hwypet5vPNwZQOuq1GCYzLHYj/6JupJ/XfoqKd9DFD/JaNOHocsbXRkktwJCjmBfFKEKR\n7d5vXde1Tw5rkHorq65jaOMYYkD/+m23ZNo566V1SU+V9BeS7pJ0bcjL+P0+lEMqS6fcozVrLvbV\nqy+MLvcyywTDInL2UsoxH3QfecaWZ475YPtcfD8x5HoWrY5lbhfz+YL2GUxKdR1DG8cQA/pX1Rxz\na+yjNzN7nKT/Jumlko5z96eG+I/CIMzMs5ZjMfNXcw4ePKyVKxtXcyQteK3sq2lbtkzpqqu2qf1/\nfePjlz98j85OZQkRd177CR1vljrL83vdtJbz+OPvl/tSPfDAsdG0UVH9JhZ5t2+qimj3Qeu6bn0y\nL4PUW1l1HUMbxxAD+tdvu+XRzmYmd7fhIl9k+70GtGb2x5JeIunXJH1S0ifcfW+ogAaR18A8FRs2\nbNf09MJFkzZs2J55MaW6GbTOqOtqo32LQ10DqILQA/MsOearJb3B3W8JFQT6k0yeVEQGrTPqutpo\n3+JQ1wDQW+ZUlphlvWJelUles7P7tXHjFZqZmVLjl9whrVnzZ3rGM5bovvuOT7psoXSqs9HR7dq1\na2vPP3tl+V6WvlWV/peHWOpi0H5RJUW1Rae6Xr58q570pON1yikn9pVqUXa/Qf+yth1tXG9ltX8/\n+w19xbz0iZt5PJRh8mfVJne03sh/06ZtUU5Sjc0wCzYt9r0sfatq/W8YsdVFqot55KHotpiv6/Xr\n3+TLl7/Q+1m8LbZ+g+yyth1tXG+pTEBW2QsMpfDIMjCv8gICVS5bCrLUP210BHURj5QWlaHfpCtr\n29HG9ZbK+Sj0wLw2yX2p3Fh+EFUuWwqy1D9tdAR1EY+UFpWh36Qra9vRxvWW0vkopCyTP5M3O7tf\nc3O3S3qrpEdKukDSWpU18ag9l+mii87WlVdeO3BO1aCTqsjly0eW+u+njXq1y7Dvl41JgPFIaVGZ\nELEOc6yUeZzFfoy3y9p2sZ8bUqv3IuRZJ0W0f6d4o+t3IS/HF/XQIqksnXKHpEtd2ltK7trCePb6\n0qXnD5VTldJiElWUZ475sItHpdCuKcRYF6nkdIaIdZjtldmHUzx+qpBjHnNsZYnpmBxm+7t37yHH\nPPdCLDIw75Y7tG7duaUcUAvjySenqt8JbOTy5StL/Wf5TK92Gfb9WNR5wmVsymqLQfabZ6zDHCtl\nHmepHOPtsrZdrOeGVOs9pBB1ErL9F4u3n/2GHphXPpWlW+7QyMiTS/kT1MJ48sltGhlZ29dKhbHl\nVKUuS/1n+Uyvdhn2/Vj0218RTlltMch+84x1mGOlzOMslWO8Xda2i/XckGq9hxSiTkK2/2LxxtTv\nKj8wD5k7NEhu1cJ4+osvr3yu6HKqEldUuzTev1PSJ9T4T90SSS9pez/+diVXc3DUXXaL1dUwx0qZ\nx1kqx3jZOrW9pIGPnbzrvQrHcR51UmQ9JHPshLwcX9RDfeaY55GzNOh29+2b8zVrLm753l6Xzs+0\nnTzLQr5cfopsl9279yyYk7B06fm+e/ee3GMJJYUYY0XdZRdyPgY55nHrVEerV1/Y9ru3v3rj9+9C\nw5ajjHUU8tifyDEfbmDuHiZnaZjcqs2b3+DSW136c2/kmO9x6a1+0kmvWDS+vPO5Ys3lS02R7ZJl\nX7G3K7mag6Pusgt9rJR5nMV+jJetc9u/dehjJ696r9JxPEydlFEPebRh6IF55VNZpDA5S8PkVt13\n3/GSptpePUNPfOL2RePMO58rppyqlBXZLln2FXu7kqs5OOouu9DHSpnHWezHeNk6t/2SDq/1d+zk\nVe9VOo6HqZMy6iGFYyeyxJp0HMlVatVvfmJ/3x1mnwinyHapQh+oQhnKQt1lR13VV+e2P9zhtXL6\nA32zgXroIuTl+KIeknxsLNstmHp9Lutny8hP7PS9NWsu9s2b35CpXMjXfD85/fTX+/LlrzqqXZYv\nf9XDed+9vt9P2+WVIzfIvvNSlfzKbkLWbdXrLk/UVX2FyDEPHd8g65eUdQ7PS6rHqMgxzzYwz2vR\ngn4/W3R+Yuv3Nm3a5qtXX5hcp66Chf1kr5s9zxuLV016rwWshv2P3TA5cjGcDKuaI1tE3Va17kKg\nruqrU9vH1B+GHT+UfQ7PS0xtkhUD874G5t5x4kA/EwxSmpSRUqxV063uG4Py3m3BAiXVRN0CCI3z\nTLlCD8wrmMizcOJAPxMMUpqUkVKsVdOt7ht5jEeed2sLFiipJuoWQGicZ6qtgndlWThxoJ+byvfz\n2bIXCEjmZvkV1K3uj55P3b0tymi7+f66d+9tQ+277H4fM47JaqlrX+9V7rrWSzdF11ev8wztk7iQ\nl+OLeqiEHPMYcrxiiKGuOtV9Y+GfvZnaotyFFeZcumTg/Hb6XHfUT3XUtS17lbuu9dJNGfW12DZp\nn/BEjnm2gXmviQP9TDDI8tlYcrxSnDhRFe11v3v3nr7aosi2W9hf5zzLola9t1NOv48Zx2Q11LWv\n9yp3Xeulm7Lqq9t5hvYJL/TAPHgqi5mdI+m9avyN/wPu/s62918u6bLm0wck/bG7f6P53iWS/kiN\nxN3bJL3K3X/WaT/XXde+YM/R+rmpfJbPxpLjlcLN8quqU90/+9lnDPX9UBb217WSdvRc1Kr3diRy\nG4/GMVkNde3rvcpd13rppqz66naeoX3SF3RgbmZLJL1f0lmSDkq6ycyucfe7Wj62T9Kz3f2+5iD+\nSknrzWylpK2SnuDuPzOzj0t6qaQPhYw5q2FySUPlf5FXFo9h2iJEO+aV+0wOdfX10/+qfM6pa1/v\nVe661ks3sdUX7VMBIS/HS1ov6XMtz98i6bJFPr9C0veaP6+UtF/SiWr8B+KfJZ3d5XtD/mGif3ku\nEpRH/hd5ZfEoY/GporZLP6u2UGs+pKjq5euGHPP+xFZftE94SjnHXNJ5kq5seb5F0l8t8vltbZ+/\nWI30lrslfXiR7+VQ1f0bJJc0VP4XeWXxGKYtQrZjXrnP5FBXV1XXfBhUXft6r3LXtV66ia2+aJ+w\nQg/Mo7ldopltkPQqSWc2n6+QtFmNZNj7JH3KzF7u7v/Y6fuTk5MP/zw2NqaxsbHAEQ+WSxoq/4u8\nsngM0xbntgwaAAAgAElEQVQh2zGv3GdyqKurqms+DKqufb1XuetaL93EVl+0T76mp6c1PT1d2P5C\nD8wPSFrT8vzk5mtHMbOnqpFbfo6739N8+WxJ+9z9J83P/JOk35PUc2Aes1D5X+SVxWOYtqAdUaZQ\naz4AQKraL/ZOTS1+s5GhhbwcL+kRkr6jxlXvR0m6RdKpbZ9ZI+nbkta3vf5MNe7Ecowkk7RT0p90\n2U8uf54oQuw5xN22PT4+6WNj/FksixhzzOukqv119+49vm7duX7CCa/wdevO9d279+S+D3LMAWBx\nCpzKYo19hNO808r7dOR2iX9pZq9tFuxKM/tbSeeqMdHTJP3c3Z/Z/O52Ne7E8nNJN0t6jbv/vMM+\nPHQ58jR/J4ODBw9r5cr878qS53ZnZ/dr48YrNDMzpcaVsUMaHd2uXbu2VubuCyEM0xah+kcdVLW/\nXn/9DTrrrL/VL37xvzRfrqVL/0T/9m8X9nWLziz66X/0VQB1Y2Zydwu2/ZQGtN2kNjBPyZYtU7rq\nqm1q/3P1+Pjl5LAhOlXtryMj52lu7kNqL9e6da/U7OynywoLAGon9MCcZEAsqg4TvFAdVe2v99yz\nTJ3Kde+97a8BAFIWzV1ZECcmeB2tyguqVEFV++uJJx7SffctLNeKFYfKCgk1x7kwHOo2m8XqKek6\nDJnAXtRDCU3+TA0TvI6gLuJX1TbavXuPL116/lHlWrr0/CATQIFeqnqcxYC6zWaxegpdh0p98mcR\nyDEPiwleDVXNX66aqvbX66+/Qeef/27de+8yrVhxSB/84Btzn/gJZMG5MBzqNpvF6klS0DoMnWNO\nKgt6YrGChqrmL1dNVfvrs599hmZnGYijfJwLw6Fus1msnhrXadOtQwbmA0o6fwkDqWr+MgD0g3Nh\nONRtNr3rKeE6DJknU9RDBeeYkwNWT7Q7AHAuDIm6zYYc88gVnWNODlh9VTV/GQD6wbkwHOo2m8Xq\nKWQdssBQBkUPzDds2K7p6amOr1933cLXAQAAkD4WGIrQkdymVgnlLwEAACA6XDEfwOzsfm3ceIVm\nZqbUSGc5pNHR7dq1ayt/bkoQE3kBpILzFVAuUlkyKOM+5uSAVQP/yQKQCs5XQPkYmGfAAkMYFBN5\nAaSC8xVQPnLMgYBYzAFAKjhfAdXHAkOoNRZzSAv5tfmjTtPB+Qpl4BxRLFJZUGvkbKaDtsofdZoW\n2gtFo88tRI55BgzMMQwm8qaB/Nr8Uafp4XyFInGOWCj0wJxUFtTeyMja2p5gUkJ+bf6o0/RwvkKR\nOEcUj8Q0AElgYa/8UacAFsM5oniksgyAiRBA8ch1zB91CmAxnCMWIsc8gyIH5nRSoDzk1+aPOgWw\nGM4RR2NgnkGRA3MmQgAAANQTCwxFhokQAAAACKFWd2XJIzecBR4AAAAQQm1SWfLKDSfHHAAAoJ7I\nMc8gy8A8z9xwJkIAAADUDwsM5STP3HAWeAAAAEDeajMwHzY3nHuXowrox0B8OC4BzKtNKsswueHk\nlaMK6MdAfDgugbSQY55B1vuYD5obzr3LUQX0YyA+HJdAWsgxz9GgueHcuxxVQD8G4sNxCaAVN9/O\n4Eh+eivuXY600I+B+HBcAmjFkZ/Bjh0XaHR0u46cPBs5gDt2XFBaTEC/6MdAfDguAbSqVY75MLh3\nOaqAfgzEh+MSSAeTPzMoYmAOAACAegs9MA+eymJm55jZXWb2LTO7rMP7LzezW5uPPWb21Jb3TjCz\nT5rZnWZ2h5mdHjpeAAAAoAxBr5ib2RJJ35J0lqSDkm6S9FJ3v6vlM+sl3enu95nZOZIm3X19872d\nkna7+z+Y2VJJx7r7/R32wxVzAAAABJX6FfNnSvq2u+93959L+pikza0fcPcb3f2+5tMbJa2SJDM7\nXtL/4+7/0PzcLzoNygEAAIAqCD0wXyXpey3Pv998rZvXSPpc8+cRST82s38ws6+b2ZVm9uhAcQIA\nAAClimaBITPbIOlVks5svrRU0mmS/sTdv2pm75X0FkkdVwianJx8+OexsTGNjY2FDBcAAAAVNz09\nrenp6cL2FzrHfL0aOePnNJ+/RZK7+zvbPvdUSZ+WdI67zzRfO0nSl9398c3nZ0q6zN1f2GE/5JgD\nXczfiu3AgcNatYpbsQEAMKjQOeahr5jfJOkUM1sr6QeSXirpZa0fMLM1agzKXzE/KJckd7/bzL5n\nZr/p7vMTSPcGjheolNnZ/dq48QrNzEypsez3Id1443bt2rWVwTkAAJEJmmPu7r+U9DpJX5R0h6SP\nufudZvZaM7uo+bEJSY+R9NdmdrOZfaVlExdLusrMbpH0NElvDxkvUDUTEztbBuWStEwzM1OamNhZ\nYlQAAKCT4Dnm7v55Sb/V9trftPx8oaQLu3z3Vkm/GzRAoMIOHDisI4Pyect08ODhMsIBAACLCL7A\nEIDyrFq1RNKhtlcPaeVKDn0AAGLDb2egwnbsuECjo9t1ZHB+SKOj27VjxwWlxQQAADoLeleWonBX\nFqC7+buyHDx4WCtXclcWAAAGFfquLAzMAQAAgAxCD8xJZQEAAAAiwMAcAAAAiAADcwAAACACDMwB\nAACACDAwBwAAACIQfOVPAABQTfO3Yz1w4LBWreJ2rMCwuF0iAADo2+zsfm3ceIVmZqYkLdP8Ama7\ndm1lcI7K4naJAAAgOhMTO1sG5ZK0TDMzU5qY2FliVEDaGJgDAIC+HThwWEcG5fOW6eDBw2WEA1QC\nA3NU0vT0dNkhIEL0C3RCvxjMqlVLJB1qe/WQVq6sxtCCfoEyVOPoAdpwQkUn9At0Qr8YzI4dF2h0\ndLuODM4bOeY7dlxQWkx5ol+gDNyVBQAA9G1kZK127dqqiYnLdfDgYa1cuUQ7djDxExgGA3MAADCQ\nkZG1+shHtpcdBlAZlbldYtkxAAAAoPpC3i6xEgNzAAAAIHVM/gQAAAAiwMAcAAAAiAADcwAAACAC\nUQzMzewcM7vLzL5lZpd1+cxfmdm3zewWM3t6r++a2R+a2e1m9kszO61tW3/a3NadZva8cCXDMIrs\nF2Z2tpl91cxuNbObzGxD2NJhEEWfK5rvrzGzB8zsjWFKhWGV8DvkqWb2H833bzWzR4UrHQZV8O+Q\npWa208y+YWZ3mNlbwpYOgwrUL97VHFPeYmafNrPjW97rb8zp7qU+1PjPwXckrZX0SEm3SHpC22d+\nX9L/bv58uqQbe31X0m9J+g1J10k6rWVbp0q6WY1bRa5rft/KrgcepfeLp0l6XPPnJ0n6ftl1wKPc\nPtGyzU9K+rikN5ZdBzzK7xeSHiHpVklPbj4/kd8h8T1K6Bcvk/SPzZ8fLWlW0pqy64FHYf3ibElL\nmj//paR3NH9+ovocc8ZwxfyZkr7t7vvd/eeSPiZpc9tnNkv6kCS5+39KOsHMTlrsu+7+TXf/tqT2\nW9pslvQxd/+Fu89J+nZzO4hLof3C3W919x82f75D0jFm9shwxcMAij5XyMw2S9on6Y5AZcLwiu4X\nz5N0q7vf3vzcPd78DYyoFN0vXNIyM3uEpGMl/Zek+8MUDUMI1S+udffDze/fKOnk5s+b1OeYM4aB\n+SpJ32t5/v3ma1k+k+W7vfZ3IMN3ULyi+8XDzOwPJX29eeAhHoX2CTNbJunNkqbUYdCOaBR9rvhN\nSTKzzzfT3940SNAIruh+8SlJD0n6gaQ5SZe7+719R43QiugXr5b0r1221XPMmerKn/ySRCdD9wsz\ne5Kkd0jaOHw4iMAwfWJS0nvc/SEzG3ZbiMswbblU0hmSfkfS/5X0b2b2VXf/Ui6RoUzD9ItnSvqF\npMdJ+lVJ/25m1zavkiJtmfuFmf13ST93948OurMYBuYHJK1peX5y87X2z6zu8JlHZfhup/112hbi\nUnS/kJmdLOmfJL2Ck2mUiu4Tp0s6z8zepUYe8S/N7Kfu/tcDxI5wiu4X35d0vbvfI0lm9q+STpPE\nwDwuRfeLl0v6fDOd4f+Y2Q1q/Odtru/IEVKwfmFmF0j6A0nPzbCtrmJIZblJ0ilmtrY5s/2lkj7b\n9pnPSnqlJJnZekn3uvvdGb8rHf2/nc9KeqmZPcrMRiSdIukruZYIeSi0X5jZCZL+RdJl7n5j7qVB\nHgrtE+7+bHd/vLs/XtJ7Jb2dQXmUiv4d8gVJTzGzY8xsqaTnSNqba4mQh6L7xXfVHJA10+DWS7or\nx/IgH0H6hZmdI+lNkja5+3+1bau/MWfZM2Sbc2bOkfRNNZLi39J87bWSLmr5zPvVmM16q46eCb3g\nu83XX6RGXs9P1cj5+lzLe3/a3Nadkp5Xdvl5lN8vJP13SQ9I+roaM6i/LumxZdcBj/L6RNt+t4u7\nskT7KOF3yMsl3S7pG2refYFHfI+Cf4csk/SJZr+4nfNFvI9A/eLbkvY3xw5fl/TXLe/1Nea05pcA\nAAAAlCiGVBYAAACg9hiYAwAAABFgYA4AAABEgIE5AAAAEAEG5gAAAEAEGJgDAAAAEWBgDgA5MrM9\nGT5zpZk9ofnzn2b4/AN5xNZjHy80szeH3k+XfW+erw8AqDPuYw4AJTKzB9z9uB6fud/dj89hX0u8\nsWR44Rbbt5n9g6R/cfdPFxwWAESFK+YAkKP5q9tm9hwz+5KZfdLM7jSzD7d85ktmdpqZvUPSo83s\n663v99j+NjP7ipndYmbbW17/jJndZGa3mdlrWuMxs8vN7GZJzzKzWTObNLOvmdmtZvabzc+db2ZX\nNH/+BzN7n5ndYGbfMbNzm6+bmf21me01sy+Y2f+ef69LrLNm9pdm9lVJf2hmr2nGfnOzXo4xs2dJ\n2iTpXc16GDGzx5vZ55rl2T0fIwBUHQNzAMhX658hny7pYklPlDRqZr931Afd/1TSQ+5+mru/oteG\nzWyjpN9w92dKeoak3zGzM5tvv8rdf1fS70p6vZmd2Hx9maQvu/sz3P2G5ms/cvfflvT/SdrWJfbH\nufsZkl4o6Z3N186TtMbdnyjplZKe1StmST92999x909I+rS7P9PdnyHpLkl/5O5flvRZSW9q1sOs\npCslva5ZnjdJ+n8z7AcAkre07AAAoMK+4u4/kCQzu0XSOkn/McT2nidpo5l9XZKpMej+DUl7JL3B\nzF7U/NzJzde/IukXkv6pbTufaf77NUkv7rKvqyXJ3e80s19vvnaGpE82X7/bzL6UIeaPt/z8FDN7\nm6QVzdi/0P5hM1sm6fckfdLMrPnyIzPsBwCSx8AcAML5r5aff6nO51zr8Fo3Jukd7v63R71o9hxJ\nz5V0urv/V3PAfEzz7f/rCycTzcfVLab22PuJsd2hlp93Strk7reb2fmSntPh80sk3ePupw2xTwBI\nEqksAJCvfgexPzOzR2Tc5hckvbp5VVlmttLMfk3SCWoMZv+reXeT9UPEs9j+b5B0XjPX/CRJY31u\nZ7mkH5rZIyWNt7z+gKTjJcndH5A0a2Z/+PDOzZ46aOAAkBIG5gCQr263uvIuP18p6bYekz9dktx9\nl6R/lPRlM/uGGmklyyV9XtIjzewOSW+X9OVF4slyK65u3/m0pO9LukPSh9RIhbmvj+1MqJFe8++S\n7mx5/WOS3tSckDqixqD9j5oTXG9XY3IoAFQet0sEAGRmZsvc/ZCZPUbSf0o6w91/VHZcAFAF5JgD\nAPrxL2a2Qo0Jmf+DQTkA5Icr5gAQgeYV6H/TkfQPa/58lrvfU1pgGZjZP6lxxxnpSNyXNVNvAAAZ\nMTAHAAAAIsDkTwAAACACDMwBAACACDAwBwAAACLAwBwAAACIAANzAAAAIAIMzAEAAIAIMDAHAAAA\nIsDAHAAAAIgAA3MAAAAgAgzMAQAAgAgwMAcAAAAiwMAcAAAAiMDSsgPIg5l52TEAAACg+tzdQm27\nEgNzSXJnbI4jJicnNTk5WXYYiAz9Ap3QL9AJ/QKdmAUbk0silQUAAACIAgNzAAAAIAIMzFFJY2Nj\nZYeACNEv0An9Ap3QL1AGq0Jutpl5FcoBAACAeJlZ0MmfXDEHAAAAIsDAHAAAAIgAA3MAAAAgAgzM\nAQAAgAgwMAcAAAAiwMAcAAAAiAADcwAAACACDMwBAACACDAwBwAAACLAwBwAAACIAANzAAAAIALB\nB+Zmdo6Z3WVm3zKzyzq8v8nMbjWzm83sK2Z2Rtv7S8zs62b22dCxAgAAAGUxdw+3cbMlkr4l6SxJ\nByXdJOml7n5Xy2eOdfeHmj8/RdIn3P3UlvcvkfTbko53901d9uMhywEAAJDF7Ox+TUzs1IEDh7Vq\n1RLt2HGBRkbWlh0WcmJmcncLtf2loTbc9ExJ33b3/ZJkZh+TtFnSwwPz+UF503JJh+efmNnJkv5A\n0l9IemPgWAEAAAY2O7tfGzdeoZmZKUnLJB3SjTdu165dWxmcI5PQqSyrJH2v5fn3m68dxcxeZGZ3\nSvpnSa9uees9kt4kicvhAAAgahMTO1sG5ZK0TDMzU5qY2FliVEhJ6Cvmmbj71ZKuNrMzJb1N0kYz\ne4Gku939FjMbk7Tonw0mJycf/nlsbExjY2PB4gUAAGh34MBhHRmUz1umgwcPd/o4EjA9Pa3p6enC\n9hd6YH5A0pqW5yc3X+vI3feY2ePN7DGSzpC0ycz+QNKjJR1nZh9y91d2+m7rwBwAAKBoq1YtkXRI\nRw/OD2nlSm6Cl6r2i71TU1NB9xd68ucjJH1TjcmfP5D0FUkvc/c7Wz4z6u4zzZ9Pk3SNu69u285z\nJF3K5E8AABCrTjnmo6PkmFdJ0pM/3f2XZvY6SV9UI5/9A+5+p5m9tvG2XynpPDN7paSfSfqppJeE\njAkAACCEkZG12rVrqyYmLtfBg4e1cuUS7djBoBzZBb1iXhSumAMAACC00FfMSXoCAAAAIsDAHAAA\nAIgAA3MAAAAgAgzMAQAAgAgwMAcAAAAiwMAcAAAAiAADcwAAACACDMwBAACACDAwBwAAACLAwBwA\nAACIwNKyA0A9zM7u18TETh04cFirVi3Rjh0XaGRkbdlhAQAARMPcvewYhmZmXoVyVNXs7H5t3HiF\nZmamJC2TdEijo9u1a9dWBucAACAZZiZ3t1DbJ5UFwU1M7GwZlEvSMs3MTGliYmeJUQEAAMSFgTmC\nO3DgsI4Myuct08GDh8sIBwAAIEoMzBHcqlVLJB1qe/WQVq6k+wEAAMxjZITgduy4QKOj23VkcN7I\nMd+x44LSYgIAAIgNkz9RiPm7shw8eFgrV3JXFgAAkJ7Qkz8ZmAMAAAAZcFcWAAAAoAZYYAgAAADJ\nqtIihqSyAAAAIElFL2JIKgsAAADQQdUWMWRgDgAAgCRVbRFDcsxLVKWcKKAT+jgAIKQjixi2Ds7T\nXcSQHPOSFJ0TBRSNPg4ACK1qOeYMzEuyZcuUrrpqm9r/hzc+frk+8pHtZYUF5IY+DgAoQpGLGIYe\nmJPKUpKq5UQB7ejjAIAijIysrcwFHwbmJalaThTQjj4OAKiC1vlSoZHKUhLyb1F19HEAQOoW/i4j\nx7ynFAfmUrE5UUAZ6OMAgJQtnC/FwLynVAfmAAAAiNeGDds1PT3V8gorfwIAAACFOzJfqhhcMQdQ\neyyEFBb1C9RTFY59cswHwMAcwKCYpBoW9QvUU5WO/db5Ul/60v9gYN4LA3MAg2IhpLCoX6Ceqnrs\nh15gKHiOuZmdY2Z3mdm3zOyyDu9vMrNbzexmM/uKmZ3RfP1kM7vOzO4ws9vM7OLQsQKoHxZCCov6\nBeqJY38wQRcYMrMlkt4v6SxJByXdZGbXuPtdLR+71t0/2/z8UyR9QtKpkn4h6Y3ufouZLZf0NTP7\nYtt3AWSUQq5fGTGyEFJY1C9ikcI5sEo49gfk7sEektZL+lzL87dIumyRzz9L0h1d3rta0lld3nMA\n3e3bN+ejo5e69KBL7tKDPjp6qe/bN1d2aA8rK8YU6iZl1C9iQD8sXlXrvDnmDDZ2DppjbmbnSXq+\nu1/UfL5F0jPd/eK2z71I0jsk/ZqkF7j7f7a9v07StKQnu/uDHfbjIcsBpC6FXL8yY2QhpLCoX5Qt\nhXNgFVXx2A+dYx40lSUrd79a0tVmdqakt0naOP9eM43lU5Je32lQPm9ycvLhn8fGxjQ2NhYqXCA5\nKeT6lRnjyMhafjkHRP2ibCmcA6uoCsf+9PS0pqenC9tf6IH5AUlrWp6f3HytI3ffY2aPN7PHuPtP\nzGypGoPyD7v7NYvtqHVgHhp5akhNCrl+KcSI4XDuRFk4v2BQ7Rd7p6amun84DyHzZCQ9QtJ3JK2V\n9ChJt0g6te0zoy0/nybpey3PPyTp3Rn2M2CmUP+qmjOFakuh36YQIwZH+6JM9D/kRSnnmEuN2yVK\nep8at2b8gLv/pZm9tlmwK83szZJeKelnkn4qaZu7f7l528TrJd0myZuPP3P3z3fYh4cuxzzy1JCq\nFHL9UogRg+HcibJxfkEeks8xbw6kf6vttb9p+fldkt7V4Xs3qHHFPSrkqSFVKeT6pRAjBsO5E2Xj\n/IIUkFzVpyN5aq3IUwOAxXDuBIDegqeyFKHIVJbZ2f3auPEKzcxMqXH155BGR7dr166t/EkMGAIT\nA6st5nMnfQ+9DNpHqtq3Zmf365JL3qsvf3m/pOVav/4kvfe9r+tatirVQ+hUlqCTP4t6qOAFhvbt\nm/Px8UnfsOHPfXx8kskjwJCYmFUPMZ476XvoZdA+UtW+tW/fnK9efaFLlxxVtjVrLu5YtqrVgwJP\n/ix9UJ1LIVj5E0ja+Phky0nbHz55j49Plh0aKo6+h14G7SNV7VuNcr01c9mqVg+hB+Yk9wEoHRMD\nURb6HnoZtI9UtW81yrVEWctW1XoIJYqVP8tWpdwnIEUs/pGu1M+f9D30MmgfKbJvFXkcNsr1C2Ut\nG8dYn0Jeji/qoSFSWaqW+wSkiOMwTVVotyqUAWHFnmNedB8mxzxsKkvt78rCohdAHFj8Iz1VOX/S\n99DLoH2kiL5VxnE4f1eWG29s3JXl9NOz3ZWlCsdY8gsMxY7cJyAOLP6RnqqcP+l76GXQPlJE3yrj\nOBwZWaurr35PX5/nGMum9gNzcp8AYDCcPweXem5+FaXaJhyH1VL7VJaYF70AgJhx/hwM9RaflNsk\n5dhTFDqVpfYDc6lauU8AUCTOn/2rSm5+laTeJhyHxSHHvADkPgHAYDh/9q8quflVknqbcBxWBwlI\nAAAU6EhOcCtygstEmyAWpLJELNWJKEBZOGaQgqJzgjkueuvUJqtXX6JnPOME3X//sdRbwvLu/+SY\nZ1DFgTmTOYD+cMwgJUXlBHNcZNfaJscff79uvvmwvvvdt4t6S1eI/h96YF76qp15PDTEyp+xGh+f\nbFkla/7xoI+PT5YdGhAljhlgIY6LwVBv1RCiHRV45U+SpyKV+kQUoGgcM8BCHBeDod6qIcV25K4s\nOcozjymVBQPIXexf3essVPlTOWaqbr59Z2Ye0g9/+B2ddNJqnXLKibXr57Eo+riI+fzWT2ycT6oh\nyXYMeTm+qIciSGXZt2/OR0cvbfmTyYM+Onqp79s3F8X2QkghxtjUvc5Clr/udRuDTm0gXerSXtqi\nJEUeFzEfg/3GFnNZkF2IdlTgVJbSB9W5FCKCgXmIPKZ9++Z8fHzSN2z4cx8fn4zuhEAOXv/qXmeh\nyx/7MVN13dpXmqxVP49NUcdFzOe3QWLjfFINebdj6IE5qSw5CZHHFPuCASnmbpWt7nUWuvyxHzNV\n1619pcbrdennsSnquIj5/DZIbJxPqiG1dow4ySYtdVycoI5lHlbd66zu5a+6bu3b+FVDO1ddzMd3\nzLEBR+l1SV3SiyWd0PJ8haQXhbyM3+9DEaSy5JHHNP/nlrGxhX9u6fbeYt8JjRy8/tW9zupe/ljk\nfd6Y39769W/25ctf6NJecsxrKObjO+bYkBYFTmXpucCQmd3i7k9ve+1md39GiP8oDCKWBYaGWTBi\nsZvgS+r43t///Yv16ld/ptSFI4paJKNK6l5ndS9/2fJecKPT9pYv36pTTnmU7r33/+hxj1ut0VHu\nylIXMR/fMceGdJS+8qeZfcPdn9r22m3u/pRQQfUrloH5MLZsmdJVV21T+y19xscvl6SO761b90rN\nzX2o43dSyqcCUJzFzjWDnDfy3h4AxCz0wDzL5M+vmtm7Jf2v5vM/kfS1UAHV1WITUxr/51j43r33\nLuv6HQDoJO8JejFP+AOA1GQZmG+VNCHp45Jc0i41BufoYNDFFXrfBH/heytWHNK99+Zz4/yYF4VA\nZym1WUqxVl3eC250295xxz2kLVumaHNUBuexdCTdViET2It6KILJn+7DTS5Z7Lvd3tu9e08uk1mY\nFJOelNospVjroIjF0NasudhXr76QNkdlcB5LR+i2UtkLDKlxhXxFy/MTJX0hZFB9FyKSgfmwiyss\ndhP8bu/lceP8mBeFQGcptVlKsdZF3gtutG9v8+Y30OaoFM5j6QjdVqEH5llSWR7r7ve2XGG/x8x+\nPecL95UwbK7lYjfB7/ZeHjfOJ0c0PSm1WUqx1kXeC260b2/Dhu2izVElnMfSkXpbZRmYHzazNe7+\nXUkys7Vq5JqjTd65m/0YJp+qzLgHlXT+WA5SarNhYx20rUP2kbr3v15S6p9AFvTpdCTfVr0uqUs6\nR9J3JX1Y0kck7Zf0/JCX8ft9KJJUlrJy0Ibdb2q5c6nFG0JKdRBq7kWofYYsT11QR6ga+nQ6Us8x\n73kfc0kys8dKWt98eqO7/zjA/xEGFtN9zMtYwCCP+wintPAC901uSKnNBo110LYO2Ufof9mk1D+B\nLOjT6QjZVjHcx1ySfinpR5KOkfTEZlDXZ/mimZ0j6b2Slkj6gLu/s+39TZJ2SDos6eeSLnH3G7J8\nN0Z5525mkUc+VRlxDyr1/LG8pNRmg8Y6aFuH7CP0v2xS6p9AFvTpdKTcVj0H5mb2Gkmvl3SypFvU\nuHL+ZUnPzfDdJZLeL+ksSQcl3WRm17j7XS0fu9bdP9v8/FMkfULSqRm/C1Ugn6pPdStvnQ3a1iH7\nCBPFdWgAACAASURBVP0PABBKz1QWM7tN0u+qkcLydDN7gqS3u/u5PTdutl7Sdnf//ebzt6iRm9Px\nyreZPUvS37n7k/r5bkypLGWYnd2vjRuv0MzMlBqDhUM65pitev7zT9B73vOG5P/U1j7R7qKLztar\nX/2Zo8o7Orpdu3ZtTb6sZch7ImOe2+vUt7O09aDfCxlT6/djm8wau1TLXmbc3fbdT0yp1HuoOHtt\nd/79mZmH9MMffkcnnbRap5xyYrT1VKRU+s4gQqeyZJlYeVPz31sk/Urz5zuyJLBLOk/SlS3Pt0j6\nqw6fe5GkOyX9WNLp/XzXI5r8WaZ9++Z806ZtfswxL3fprS7NVWJyymKLK+V5H+a6KmKxmWH74KD3\n3M77Xt15xRTbZNbYpVr2MuPOY1G6VOo9VJy9ttvpfelSl/ZGWU9FSqXvDEoRLDD0GUkrJE1Kul7S\nNZL+NdPG+xhcN98/U9Kufr/LwLyhigsgVLFMMcm7fmmvxQ1aP3Wu11TLXmbc3fa9bt25mWNKpd5D\nxdlru93elyajrKcipdJ3BhV6YN4zx9zdX9z8cdLMviTpBEmfz3hB/oCkNS3PT26+1m1fe8zs8Wb2\nmH6/Ozk5+fDPY2NjGhsbyxhidVRxUloVyxSTvOuX9lpcjJNZY5dq2cuMu9u+7713WeaYUqn3UHH2\n2m639xv3sYivnoqUSt/Janp6WtPT04XtL+tdWSRJ7r67z+3fJOmU5qJEP5D0Ukkva/2AmY26+0zz\n59MkPcrdf2JmPb/b6jvfsYdzmc4/f6TPMKuRD1XFSWknnHC/pAk1bsyzRNIFkh6bdJlikkefaT12\n5uZuH3p7ZQp9HohxMmvsFpZ9v6S/0x137NeWLVPRnquPjnu/pJ2Sfq7Z2Ts1O7s/aMzd+suKFYd0\n773Z+lFMfW6x4zLvc9j89nttt9v7jd9T4RdQi1lMfScP7Rd7p6amwu4w5OX4xhV/nSPpm5K+Lekt\nzddeK+mi5s9vlnS7pK9LukHSsxb7bpd9DJXLVJV8qKqUY96+fXO+Zs3FbTl8l/jq1RcmW6bY5L84\n1V5fuvT8JPtgEccPOeb9O7rscy5dkkQ9HIl7rzdyj4uLuVt/+ehHP7Xg+Fy69HzfvXtP5m0UXdeD\n5HrnscBer3z8PHLMY6njvFW1XPNUdo55Co/GwNwHzmWqUj5UyAlvRevWLps2bSs7tEoZps90bqO9\nvm7ducn1waLOAzFOZo3dfNlPOunFSZ2r9+2b6yuvO+99t/eXRh/f64086D9v/ru3aywx9Lksx2X+\n57AHj6qzbtudf3/9+jf5unXn+vr1r+9r/1Uae7SLoe+EEnpg3lcqSzr6y2WqUj5UyjfVb9etXR54\n4NgywqmsYfpM5zY6VSMjT9Z11wX+c1/OijoPDFrfVTq2+zVf9g0btuvuu9M5V4+MrNW6dU/W3Fzx\nMXfqL40+fqqko1/vFksMfS7LcZn/Oayx/V7bHbZ+qjT2aBdD30lV14G5mT0gyTu9pcb/Fo4PFtXQ\n+stlqlo+VBYp5LXVsV1SMd9/9u69TSm1Uehc1TziqLJhy53iOSGmmGOKJateMcfcp3rdS37Y82en\nNT6uvPLa2p1XhhHluTjk5fiiHiLHvC+plDeVOOsm/ZzfMLmqecVRVXmUO8W6iynmmGLJarGYY+5T\n2XLXBz9/Vml+T1kGbXvFkmMu6dfVuH3hGklrQgbVdyGkoXOZqpwP1S6lvLY6tUsqFvafOZfe6ied\n9Iqo2yh0rmqecVRRXuVO8ZwQU8wxxZJVt5hj7lPZ7yU/2Plz4fbreV4ZxqD9J/TAvGeOuZltkvQ/\nJa2U9CNJa9VYpfNJAS7gD2zYXKY65UOllNdWp3ZJxcL+s1bSDj3xidujbqvQuap5xlFFeZU7xXNC\nTDHHFEtW3WKOuU9lv5f8YOfPhduv53llGLGei7MkMu2QtF7St9x9RNJZkm4MGhWCOpJT1yruPEPE\nI9X+E0vcscRRtLqWG+HE3Ke6xbZixaGOr/cb88Ltx1sXsYq2//S6pC7pq81/b5W0ZP7nkJfx+300\nitEw/yepsbF0/kxXlCO3dnqzL1/+Qm/cNstzyUXLq97zbL9u22p/fffuPZXvM3nXa7fcySz7KOsY\nzZJPWERsKeb55iHlcvN7JR6tbbFp0zZfvfrCvvtUmcd5r/ujD779cDnmWeorxWOk1/yFbuVR2Tnm\nkq6VtFzSFZI+Kul9kv4jZFB9F6I5ME/5xB9ap7pZvvxVfd93Neu28znRDN5+/ZwUGyez/P6TEpsQ\nx0V7TmbWXzZlH6OL5ZIWGVuKeb55SLHcZfdZHNGpLdasudg3b35D5j4Vw3Ge13HQ6Tyc9/GV9YJG\nqsdIp7boVZ4YBubL1PgbyVJJ50u6WNKvhgyq70I0B+Z1nVSVRci6yWvbecaYfeKNN59PVrbPFHFc\nZN1HzMdozLGhPPSLeOTRFrRnf7LUV9XqtFd5Qg/Msyww9FpJH3f3A5I+OHTuTECxJvLHIGTd5LXt\nPGPMPvFGzeeHj3pepT5TxHGRdR8xH6Mxx4by0C/ikUdb0J79yVJfVavTssuTJcP9OElfNLN/N7PX\nmdlJoYMaVLSJ/BHoVjfHHfeQtmyZ0oYN27Vly5RmZ/fntu3hJ7MMtp3FttVt4s3Rh0I1+szs7H5t\n2TKlvXtnJU1Iam3bfMuYte1iPkZjjm0Y8/1gmGO8zvLsF0W2RRXbPY+2qOpxHkqW+qpanZZenqyX\n1iU9VdJfSLpL0rUhL+P3+xA55j11y80bZOJMlm2TY16uTuVvLGQxF6SMWdsu5mM05tgGVcUyFS3G\n81tM+ypSHuWqat2EUvUc807KzjG3xj56M7PHSfpvkl4q6Th3f2qI/ygMwsx8vhzzy6sePHhYK1dG\nsrxqJNrr5sEH79U117xN7csBj49f3vc9XfOq9zzbr9u22l+fX8a4Sn1my5YpXXXVNrW37UknvUJn\nn/20IGXM2nYxH6MxxzaIbv1gkGO8zvLoF0W2RZXbPY+2qNpxHlqW+qpanS5WHjOTu1uoffccmJvZ\nH0t6iaRfk/RJSZ9w972hAhpE68Ac2W3YsF3T01MdX7/uuoWvIx20LST6QUyKbAvaHQgn9MA8y+TP\n1ZLe4O63hAoC5TiSR3X0VZVU88JwBG0LiX4QkyLbgnYH0pU5lSVmqV4xn/9TyYEDh7VqVfF/+pmd\n3a+NG6/QzMyUGifwQxod3a5du7bmEkcR5eu2j7Lrtmyh23ax/aZY77HEnXccZfWDfsVS/1njGSTe\nItsilXaPWWx9Ep2V0U6hr5iXPnEzj4daVv5MRSyTJUIt+FFE+UKvrJa6ohdziaVP9yuWuEPFEfui\nPrHUf9Z4hom3yLaIvd1jFlufRGdltZPKXmAohUeKA/Oq3ZC/XZmL2nRbRKgqdRurVPt0LHHHEkfR\nYit3r3hiixf5o43TUFY7hR6Yk3BWkrJvYB9amYvadFtEqCp1G6tU+3QscccSR9FiK3eveGKLF/mj\njdNQ1XbKMvkTAXSbnDM7e7tmZ/cvepu5PHKpQudl5Tn5qFus3faxYsUh3Xtv/SY+lZ0TmeqEsxNO\nuF+NBZiWNB8XSHps4XFnqb8Quc9li63f9IontnixUJbjYLHP1LWNr7/+Bp1//rt1zz3LdOKJh/TB\nD75Rz372GT2/V9Z5p7LtFPJyfFEPJZjK0nkBmEtd2ht8YZYy87/zXJSDHPMjYsiJjCGGfu3bN+dr\n1lzcdhxe4qtXXxhdjnnI3OcyxRZ3Veu5LvJYEKeObbx7957mYntHL763e/eeRb9XZl2RYx7xI8WB\nuXujUzXyof+7S5PeWJXRO+ZI5ZlLVVReVh6Tj3rF2m0fdZv4FEtOZGr13q3eNm3aVko8i9VflXOf\nY+s3veKJLV4ckeU4yPKZurVxt7lZ69adu+j3yj7vlNFOoQfmpLKUaGRkrdate7Lm5toXfFiYI5Vn\nLlVReVkjI2uHXmWuV6zd9pHHvlMSS65davXerd4eeODYMsJZtP6qnPscW7/pFU9s8eKILMdBls/U\nrY3vuafz3KzGnK3uyj7vVLGdGJgH0E++VdYcqTxzqQbZFjlkcefvxlRPKTlSbz+WtFPSYUmHdfzx\nD/a9rbLnbaTaB0LV27B5xshm0DoMVfdZjoNYj5Uy++OJJx7Sffd1nrO1mJTGE8kIeTm+qIciSmXp\nN+cp6+fLzDGvYw5ZrHF0E3t8sdq3b85Xr77QpUuOqrs1ay7uq+5imLeRYh8Iee/2YfOM0dugdRiy\n7lNt+7JjKirHvOxy5kHkmKc1MB8k3yprjlSeuVT9bKuOOWTtyq6DLGKopxRt2rRt6LaNZd5Gan0g\nVL3llWeMxQ1ah6HrPstxENuxEkN/3L17j69bd66vWPEKX7fu3J6D8nkpjSfyEHpgTipLzgbJt8qa\nI5VnLlU/2yKHrPw6yCKGekrR/fcfq2HbNpZ5G6n1gVD1lleeMRY3aB2Grvssx0Fsx0oM/fHZzz5D\ns7O9b4/YLqXxRAriTj5M0JF8q1bl564No4pl6hd1UF15tC39YzCh6i3Ldmmz4Q1ah9T9QnWpk7qU\ncyghL8cX9VAfqSzzf3IZGwvz56ui8qcGLccg36tCTtiw6loHoY+XGOTRtmX0jyq0Taw55jHWba+Y\nyog5xhzzVJVVJ0X3m1TafrF6ETnm+Q3Mix40h8pdK+NkGFs+XhnqVgepnEDzkEfbFtk/qtQ2oept\n0DzjGOs25om/g7Zf3c6nWRRdJ2X/ZyDWtu9VLwzMcxyYV2HSgXu8E25QLfSXeNE24cRYt1VeXArl\nod901qteQg/Ma5XUU5VJB7FOuEG10F/iRduEE2PdVnlxKZSHftNZ2fVSubuyLHbj+lgXFejXoOXI\no/wsDFAfVTleqoi2GUyW81eMdZvH4lIhzt0s4pSW9rY4/viH1E+/OeGE++W+VPfff2yl27L0c0DI\ny/FFPdRMZYk5Dy9PZU24qUr9IRvaO160Tf+y1lmMdTvs77YQZRp2gi2K1aktVq++0NesuThjv5nz\n9oXYqtqW5JjnODDPki8V+6SDrMqYcEM+Wv1U5XipItqmP/2cv2Ks214xLfZ+iHM3izilpVtbbN78\nhoz9pl5tudjxFHpgHjyVxczOkfReNe6Z/gF3f2fb+y+XdFnz6QOS/tjdv9F87xJJfyTpsKTbJL3K\n3X/WbV9Z8oJiW1RgUIOWY5jyl513heJV5XipItqmP/2cv2Ks22EWlwpx7mYRp7R0a4v77z9eV1+d\npd/Uqy3LPAcEHZib2RJJ75d0lqSDkm4ys2vc/a6Wj+2T9Gx3v685iL9S0nozWylpq6QnuPvPzOzj\nkl4q6UPd9ld6XlDF9apfcgmrjzZeHPUTrzr/fghR9izbLLvOOR6PWKwtutXT0d+p5vETZR8JeTle\n0npJn2t5/hZJly3y+RWSvtf8eaWk/ZJOVOM/EP8s6ewu33v4Tw/ks4XTa0EO6r7aaOPFUT9xq3P7\n1DHHvM7t3Um3+ti9e0/G3+vVyzEftI8ocCqLNfYRhpmdJ+n57n5R8/kWSc9094u7fH6bpN9s+fzF\nkv5C0kOSvujur+jyPZ8vx/z/fg4ePKyVKyP530+FdKvfLVumdNVV29T+v+nx8cuj+5MwBkMbL476\niV+dfz+EKHuWbZZV5xyPC3Vqi4mJnYvWU+t3jj++cVeWBx44thLHz6B9xMzk7hYqrmhul2hmGyS9\nStKZzecrJG2WtFbSfZI+ZWYvd/d/7PT9ycnJh39+zWvGNDY2FjjieuqWd0UuYfXRxoujfuIXY+54\nUUKUPcs2y6pzjseFOrVFr3qq8jGTtY9MT09renq6qLCCD8wPSFrT8vzk5mtHMbOnqpFb/v+zd/9h\nllT1ve8/36EhhBl+eaN4ephftCeKicRwbnASiPYE53GOHgEx14v2AIMRea46GJEIl0Pb3U+rMT4E\nYzA5JyTGQeWgSBIgz7kampAeLkQCRkCE4Yc9TTN2x8k18rPhRKC/949dPb2nZ+/u2nvXqlpV+/16\nnv3M7tq1q9Zatar6O9XfVWuLuz+ZLH6rpN3u/tNknb+W9BuSlg3Mkb+icwkRHsd4abQPEA/Ox3S6\nuZ3S1r2/f/+bvSMjI2ELFjJPRtJBkn6o2l3vQyTdJ+n4ReuslfSYpI2Llp+k2pNYDpVkknZI+nCT\n/SyfTISgYsznm3/cUX9/PI88K7MYj3ForfShbmwfIFYxn48x/W6KuZ2y0qy9uzLHXNr3uMQvaOFx\niZ81swuSil1tZn8u6UzVBnqapBfd/aTku0OqPYnlRUn3SvqAu7/YYB8euh5YXkz5m5OTU9q8+SpN\nTIyo9r/hWfX1DWlsbHupc+KKFtMxDq2dPtRN7QPELsbzMcbfTTG2U1aWa+926h46xzx4YJ4HAnMs\nxsAfdIo+BCBrXFfyFaK9Qwfm1U8iQldi4A86RR8CkDWuK/kqY3tH81QWLC3UQ/CjfLh+Brp5QAuy\nQR9CvapeKztFu7SG60q+Om3vRv07uJAJ7Hm9VPHBn6EGZ1R50EeV64Z80Icwj77QGO3SOtosX520\nd7PvquyDP/NQ9RzzUDlpVc91q/KAFuSDPgSp+tfKdtEu7eG6kq9227tZ/5ZWBc0xJ5WlBELlSJUx\n96oVVZ4YAfmgD0Gq/rWyXbRLe7iu5Kvd9m7Wv0MjMC+BUDlp5LohLfJI0c24VjZGu6DKmvXv4ELm\nyeT1EjnmUW0X1UI/QbfjHGiMdkGVkWPegarnmEvhctLIdcNyyCMFuFY2Q7ugyhr17+OOWx80x5zA\nHMCSNm0a0vj4SMPlt9124HIAAKqKCYYAFGohz64eeaQAAGSNO+YAljQ5OaXNm6/SxMSIaukss+rr\nG9LY2Hb+ZI2OMKg4HhwLIJ3Qd8wJzAEsizxSZI3/8MWDYwGkR2CeAoE5AJQLg4rjwbEA0iPHHABQ\nOUxOEw+OBRAPJhhCR8hLBJrj/GiOyWniMDk5pccf/4E4FkAcSGVB28hLBJrj/Fga7VO8hWPwO5K+\nJIljASyHHPMUCMyLQV4i0Bznx/IYVFys/fvolKQdkl7U+vW7dNttV3IsgAZCB+aksqBt5CUCzXF+\nLG/DhnX8J6VA+/fRdZJqx2LDhiGCcqAgJJChbUw8AzTH+YHY0UeB+JDK0gADttIhRxRojvMDsaOP\nAq0jxzyFLANzLlStIUcUaI7zA7GjjwKtITBPIcvAnAFbAAAAaIQJhnLGgC0AAAAUgaeyLMKkFwAA\nAPlifF8NqSyLkGMOAACQnzLFXuSYpxDqqSwMhgEAAAirTOP7mGCoAEx6AQAAkA/G9y0gMEepkZMG\nAOXDtbtaOj2ejO9bQCoLSqtMOWkAgBqu3dWSxfEsU58gxzwFAvPuVKacNABADdfuasnqeJZlfB85\n5kAT5KQBQPlw7a6WrI4n4/tqui95B5WxkJNWrztz0gCgLLh2VwvHM1u0GkprdHSb+vqGtHBBqOWk\njY5uK6xMAIClce2uFo5ntsgxR6mVJScNALCAa3e1dNPxZPBnCgTmAAAACC10YB48lcXMtpjZw2b2\nqJld0uDz95nZ/cnrDjM7oe6zI83sm2a2y8weNLM3hS4vAAAAUISgd8zNbIWkRyWdKmlG0j2SznL3\nh+vW2Shpl7s/bWZbJA27+8bksx2Sdrr7l82sR9Jh7v5Mg/1wxxwAAABBlf2O+UmSHnP3KXd/UdLX\nJZ1ev4K73+XuTyc/3iVptSSZ2RGSftPdv5ys91KjoBwAAACogtCB+WpJe+p+/lGyrJkPSPpW8n6D\npJ+Y2ZfN7HtmdrWZ/XygcgIAAACFimaCITPbJOk8Sacki3oknSjpw+7+XTP7I0mXSmr49Pnh4eF9\n7/v7+9Xf3x+yuAAAAKi48fFxjY+P57a/0DnmG1XLGd+S/HypJHf3P1i03gmS/krSFnefSJYdI+k7\n7n5c8vMpki5x93c22A855gAAACUw/3jF6ek5rV5drscrhs4xD33H/B5JrzGzdZL+RdJZkt5bv4KZ\nrVUtKD97PiiXJHffa2Z7zOwX3X1+AOlDgcsLAACAQCYnp7R581WamBiRtFLSrO66a0hjY9tLE5yH\nFDTH3N1flvQRSbdIelDS1919l5ldYGYfTFYblPQKSX9qZvea2d11m7hQ0rVmdp+kX5H0mZDlBQAA\nQDiDgzvqgnJJWqmJiRENDu4osFTxCJ5j7u7flvTaRcv+rO79+ZLOb/Ld+yX9WtACAgAAIBfT03Na\nCMrnrdTMzFwRxYlO8AmGAAAAAElavXqFpNlFS2fV20tIKhGYAwAAICejo9vU1zekheB8Vn19Qxod\n3VZYmWIS9KkseeGpLAAAAOUw/1SWmZk59fbyVJb9tl+FgJbAHAAAAKGFDsxJZQEAAAAiQGAOAAAA\nRIDAHAAAAIgAgTkAAAAQAQJzAAAAIALBZ/4su/lH+kxPz2n16nI90gcAAADlweMSlzA5OaXNm6/S\nxMSIatPH1h6CPza2neAcAACgy/C4xAINDu6oC8olaaUmJkY0OLijwFIBAACgigjMlzA9PaeFoHze\nSs3MzBVRHAAAAFQYgfkSVq9eIWl20dJZ9fbSbLEbHx8vugiIEP0CjdAv0Aj9AkUgwlzC6Og29fUN\naSE4r+WYj45uK6xMSIcLKhqhX6AR+gUaoV+gCDyVZQkbNqzT2Nh2DQ5eoZmZOfX2rtDoKAM/AQAA\nkD0C82Vs2LBOX/vaUNHFAAAAQMVV5nGJRZcBAAAA1RfycYmVCMwBAACAsmPwJwAAABABAnMAAAAg\nAgTmAAAAQASiDszNbIuZPWxmj5rZJQ0+P8LMbjaz+8zsATPblva7KK92+4WZHWtmt5nZg8nyC3Mv\nPILp5HqRfL7CzL5nZjfnVmgE1+HvkSPN7Jtmtiu5brwp18IjmA77xcfM7Adm9n0zu9bMDsm18Agm\nRb84ysz+2szuN7O7zOz1ab+bmrtH+VLtPw0/lLRO0sGS7pP0ukXr/N+Sfj95/wuS/k21R0Au+11e\n5Xx12C9eLemNyfJVkh6hX1Tj1Um/qPv8Y5K+JunmouvDK45+IWmHpPOS9z2Sjii6TryK7ReSeiXt\nlnRI8tk3JJ1TdJ145dYvPidpMHn/Wkm3pv1u2lfMd8xPkvSYu0+5+4uSvi7p9EXruKTDk/eHS/o3\nd38p5XdRTm33C3f/sbvfJ0nu/pykXZJW51RuhNXJ9UJmdqykt0v6i5zKi3y03S/M7AhJv+nuX5ak\n5BryTF4FR1AdXS8kHSRppZn1SDpM0kwOZUZ4afrF6yXdJknu/oik9Wb2ypTfTSXmwHy1pD11P/9I\nBwZRX5T0ejObkXS/pI+28F2UUyf9Yh8zWy/pjZL+KUgpkbdO+8XnJf2ear+MUR2d9IsNkn5iZl9O\nUpyuNrOfD15i5KHtfuHuM5L+UNITkqYlPeXutwYvMfKQpl/cL+lMSTKzkyStlXRsyu+mEnNgnsbb\nJN3r7r2SflXSn5jZqoLLhOIt2S+S9zdI+mhy5xzdoWG/MLN3SNqb/DXFkhe6R7PrRY+kEyX9ibuf\nKOl5SZcWV0zkrNn14ijV7oSuUy2tZZWZva/AciJfn5V0tJl9T9KHJd0r6eUsdxBzYD6t2v9E5h2b\nLKt3nqS/liR3n5A0Kel1Kb+LcuqkXyj50+MNkr7q7jcFLy3y0km/OFnSaWa2W9J1kjaZ2VeClxh5\n6KRf/EjSHnf/brLeDaoF6ii/TvrFWyXtdvefuvvLyTq/EbzEyMOy/cLdn3X397v7ie5+rqRXqTbm\nILO4M+bA/B5JrzGzdcmI57MkLX5awpRqJ4nM7BhJv6haA6X5Lsqpk34hSX8p6SF3/0JO5UU+2u4X\n7n6Zu6919+OS793m7ufkWHaE00m/2Ctpj5n9YrLeqZIeyqfYCKyT3yNPSNpoZoeamanWL3blVnKE\ntGy/SJ7UdHDy/nxJO5O/vGcWd/Z0UoOQ3P1lM/uIpFtU+w/El9x9l5ldUPvYr5b0KUk7zOz7ydc+\n4e4/laRG382/FshaJ/3CzE6WNCDpATO7V7V84svc/dsFVAUZ6vR6gWrKoF9cKOna5BfxbtXuoqLk\nOuwXd5vZDaqlMLyY/Ht1/rVA1lL2i+MlXWNmc5IelPQ7S323nXJY8pgXAAAAAAWKOZUFAAAA6BoE\n5gAAAEAECMwBAACACBCYAwAAABEgMAcAAAAiQGAOAAAARIDAHABKwMye7eC7Q2Z2UZblAQBkj8Ac\nAMrhgEknzOygIgoCAAiDwBwASsTM3mJmt5vZTarNPNdsvf9qZo+Y2e2SXlu3/Dgz+5aZ3WNmO+en\nnDezV5nZX5vZfWZ2r5ltTJb/TbLuA2b2gWTZeWb2+bptfsDM/jBUnQGgW/QUXQAAQMt+VdIvufsT\njT40sxMlvUfSCZIOkfQ9Sd9NPr5a0gXuPmFmJ0n6b5JOlfTHksbd/UwzM0mrkvXPc/enzOxQSfeY\n2V9Jul7SZWZ2sbu/rNpU9R8MUlMA6CIE5gBQPnc3C8oTvynpb9z93yX9u5ndLElmtlLSb0j6ZhJ8\nS9LByb+/JelsSXJ3lzSf0/67ZnZG8v5YSf/R3e82s9sk/Rcze1hSj7s3vXsPAEiHwBwAyme2ze+t\nkPSku5/Y4LNGOexvUS1gf5O7/7uZ/YOkQ5OPvyTpMkkPS/pym+UBANQhxxwAysGWX2Wf2yWdYWY/\nZ2aHS3qnJLn7s5Imzey3923U7ITk7d9L+lCybIWZHSHpSNUC+X83s9dJ2jj/PXe/W9IaSe+VdF37\n1QIAzCMwB4ByOOCOdtMV3e+V9A1J35f0PyXdXffxVkm/kwzy/IGk05Llvytpk5l9X7V89OMlfVvS\nwWb2oKTPSPrOol1dL+lOd3+6jfoAABaxWiohAACtMbO/lXSlu/9D0WUBgCrgjjkAoCVmdqSZPSJp\nlqAcALLDHXMAKCkze4VqueHzF3JL3p/q7k8WVjAAQFsIzAEAAIAIkMoCAAAARIDAHAAAAIgANkBB\n3QAAIABJREFUgTkAAAAQAQJzAAAAIAIE5gAAAEAECMwBAACACBCYAwAAABEgMAcAAAAiQGAOAAAA\nRIDAHAAAAIgAgTkAAAAQAQJzAAAAIAI9RRcgC2bmRZcBAAAA1efuFmrblQjMJcmd2BwLhoeHNTw8\nXHQxEBn6BRqhX6AR+gUaMQsWk0silQUAAACIAoE5AAAAEAECc1RSf39/0UVAhOgXaIR+gUboFyiC\nVSE328y8CvUAAABAvMws6OBP7pgDAAAAESAwBwAAACJAYA4AAABEgMAcAAAAiACBOQAAABABAnMA\nAAAgAgTmAAAAQAQIzAEAAIAIEJgDAAAAESAwBwAAACJAYA4AAABEIHhgbmZbzOxhM3vUzC5p8Plp\nZna/md1rZneb2cmLPl9hZt8zs5tDlxUAAAAoirl7uI2brZD0qKRTJc1IukfSWe7+cN06h7n788n7\nN0i63t2Pr/v8Y5L+k6Qj3P20JvvxkPUAQpicnNLg4A5NT89p9eoVGh3dpg0b1hVdLAAA0ISZyd0t\n1PZ7Qm04cZKkx9x9SpLM7OuSTpe0LzCfD8oTqyTNzf9gZsdKerukT0u6KHBZgdxMTk5p8+arNDEx\nImmlpFndddeQxsa2E5wDANClQqeyrJa0p+7nHyXL9mNmZ5jZLkl/K+n9dR99XtLvSeJ2OCplcHBH\nXVAuSSs1MTGiwcEdBZYKAAAUKfQd81Tc/UZJN5rZKZI+JWmzmb1D0l53v8/M+iUt+WeD4eHhfe/7\n+/vV398frLxAp6an57QQlM9bqZmZuUarAwCAAoyPj2t8fDy3/YUOzKclra37+dhkWUPufoeZHWdm\nr5B0sqTTzOztkn5e0uFm9hV3P6fRd+sDcyB2q1evkDSr/YPzWfX28qAkAABisfhm78jISND9hR78\neZCkR1Qb/Pkvku6W9F5331W3Tp+7TyTvT5R0k7uvWbSdt0j6OIM/URWNcsz7+sgxBwAgZqUe/Onu\nL5vZRyTdolo++5fcfZeZXVD72K+W9G4zO0fSzyS9IOk9IcsExGDDhnUaG9uuwcErNDMzp97eFRod\nJSgHAKCbBb1jnhfumAMAACC00HfMSWgFAAAAIkBgDgAAAESAwBwAAACIAIE5AAAAEAECcwAAACAC\nBOYAAABABAjMAQAAgAgQmAMAAAARIDAHAAAAIkBgDgAAAESgp+gCAEArJienNDi4Q9PTc1q9eoVG\nR7dpw4Z1RRcLAICOmbsXXYaOmZlXoR4AljY5OaXNm6/SxMSIpJWSZtXXN6Sxse0E5wCA4MxM7m6h\ntk8qC4DSGBzcUReUS9JKTUyMaHBwR4GlAgAgGwTmAEpjenpOC0H5vJWamZkrojgAAGSKwBxAaaxe\nvULS7KKls+rt5VIGACg/fpsBKI3R0W3q6xvSQnBeyzEfHd1WWJkAAMgKgz8BlMr8U1lmZubU28tT\nWQAA+Qk9+JPAHAAAAEiBp7IAAAAAXYAJhroYE7UAAADEg1SWLsVELQAAAK0hlQVBMFELAABAXAjM\nuxQTtQAAAMSFHPMOlDlHe2GilvrgfFaHH/68tm4dKWWdAAAAyowc8zaVPUe7UfnXrr1M7i9oz57P\nq4x1AgAACInnmKdQRGC+deuIrr32Yi2+4zwwcIW+9rWhXMvSrsUTtTz33FO66aZPqcx1AgAACCV0\nYE4qS5uqkKO9YcO6/QLuTZuGVPY6AQAAlBWBeZua5Wj39mY/njavXPY86wQAAID9kcrSprxyzPPM\nZS973jwAAEBI5JinUNQEQ4tztEPcyc47lz2POgEAAJQROeYRW5yjHULeuex51AkAAAAHInk4cgt5\n3/XI+wYAAKgaUlkyEHJwJnnfANCaMk/+ljXaAsgWOeYpFBmY5xE4k/cNAOlwM2MBbQFkj8A8hSID\n8ypMNAQAVcE1eQFtAWQvdGAePFHZzLaY2cNm9qiZXdLg89PM7H4zu9fM7jazk5Plx5rZbWb2oJk9\nYGYXhi5rO6ow0RAAVAXX5AW0BVA+QZ/KYmYrJH1R0qmSZiTdY2Y3ufvDdavd6u43J+u/QdL1ko6X\n9JKki9z9PjNbJemfzeyWRd8tXLdMykOeYvaybFOOD1DTLdfkNGgLoITcPdhL0kZJ36r7+VJJlyyx\n/q9LerDJZzdKOrXJZ16U3bsf976+j7v0nEvu0nPe1/dx37378cLKlLVuqGPesmxTjg+wgPNhAW0B\nZC+JOYPFzkFzzM3s3ZLe5u4fTH7eKukkd79w0XpnSPp9Sa+U9A53/6dFn6+XNC7pl939uQb78ZD1\nWE7VB2eSp5i9LNuU4wPsr+rX5FbQFkC2umKCIXe/UdKNZnaKpE9J2jz/WZLGcoOkjzYKyucNDw/v\ne9/f36/+/v5QxT1A1SflIU8xe1m2KccH2F/Vr8mtoC2AzoyPj2t8fDy3/YUOzKclra37+dhkWUPu\nfoeZHWdmr3D3n5pZj2pB+Vfd/aaldlQfmCNb5ClmL8s2zWJb5KgjRvRLAEVbfLN3ZGQk7A5D5slI\nOkjSDyWtk3SIpPskHb9onb669ydK2lP381ckXZliP21mCiEN8hSzF1OOOccXMaJfAoiRypxjLtUe\nlyjpC6o9mvFL7v5ZM7sgqdjVZvYJSedI+pmkFyRd7O7fSR6beLukByR58rrM3b/dYB8euh7djjzF\n7GXZpp1sixx1xIh+CSBGpc8xTwLp1y5a9md17z8n6XMNvnenanfcEQHyFLOXZZt2si1y1BEj+iWA\nbkSSMNDlFnLU6zGGAMWiXwLoRsFTWfJAKkt2GGy14Pbb79S5516pJ59cqaOPntU111ykN7/55Mq1\n0eTklDZvvkoTEyOq3aGcVV/fkMbGti9br6Lbouj9I5xO+mXs6LdAeYVOZSEwxz5V/kXYqttvv1On\nnvrneumlP9F8W/T0fFhf/eo7dfnl36lcG7WTo150fyl6/wivimNb6LdAuYUOzIM+lSWvl3gqSyYG\nBobrnoAw/3rOBwaGiy5a7tavP7NhW6xc+Vu0UaLo/lL0/oF20G+BclPgp7KQrId9GGy14MknV6pR\nW/yv//WKhsu7sY2K7i9F7x9oB/0WwFKimPmzTELnBna6/U6+z0RCC44+elZPP31gWxx66E81O0sb\nScX3l6L3D7SDfgtgSSFvx+f1Uk6pLKEnvCh6ohgm9Fiwc+cd3tNz7n5t0dNzrl933Q20UaLo/lL0\n/oF20G+BclPZJxjKQ16DP0NPeNHp9rMoXxUHW7Vr/qksTz21UkcddeBTWWij4vtL0fsH2kG/Bcqr\n9BMMVUno3MBOt59F+ZhIaEEtCD/5gOW00YKi26Lo/QPtoN8CaIakthaEnvCi0+0zIQcAAEB5kcrS\ngtDPn+10+zwftxqKmHyECU8AAFgeEwylkOcEQ6FzAzvdPrmL5VbEf674Dx0AAOkQmKfAzJ+oitAD\njGPZJwAAZRQ6MCf5GIhIEZOPMOEJAABx4KksGShyUiBUSx6Tjyzub0ce+UzwfaYtC30fKBfOYSBj\nIR+SntdLOU0w1AiT+iBLRUxitXbthb5mzfm590H6PlBunMPoRmKCoeUVmWMew6RAqJaQA3ib9bfT\nThvW4YevynXQMH0fKDfOYXQjJhiKXAyTAqFaQk4+0qy/PfvsYbrppnx/kdL3gXLjHAayR2DeoU5z\ngjv5fiu5fcutW/U8wdjrl1f58shhL2NZWhF7X1pOTOWPqSxoXVnPYSBqIfNk8nqpC3PMW/necutW\nPU8w9vrlWb6Y2iKmsqRVxjLXi6n8MZUF7eEYohspcI554UF1JpUoMDB3r12cBgaGfdOmT/rAwHDL\nF6V2vj8wMFx3MfR9F8WBgeGW121lW2UUe/3yLl+n/bWqZUkj9r60nJjKH1NZ0L6yncNAp0IH5qSy\nZKDTnOB2vt9Kbt9y61Y9TzD2+uVdvpA57K2KqSxpxN6XlhNT+WMqC9pXtnMYiB2JYCW1kNtXr3Fu\n33LrtrKtMoq9frGXDwvKfqxiKn9MZQGAaCx3S13SuyQdWffzUZLOCHkbv9WXIkll6e/P70955Jin\nF1v9FveXnTvvSJ4jfrlLn3Tpcl+z5vzKtH+VZNGXirhe1O87lnMhprJ0gyL7HVAlKvo55mZ2n7u/\ncdGye939V0P8R6EdRT7HfHJySps3X6WJiRHV/iw7q76+IY2NbQ/+dIFWnne93Lohn50dg1jq16i/\nrFnzMb388sGamfncvmVr116m8fGLKnUMqqKTvlTk9aK+DDGcC7GVpcpi6HdAVYR+jnmawPz77n7C\nomUPuPsbQhWqVWWeYAjdpXF/GZR0qehD1cf1AkWg3wHZCR2Yp0nm+66ZXWlmfcnrSkn/HKpAZcMA\nJrSicX9Z0WAZfaiKuF6gCPQ7oDzSPJVlu2q39L4hySWNSfpwyEKVSV4TBHW6rSpO5FHGOjXuL3MN\nls3q8MOf19atIy3Xr4zt0i0OPP5Tkv5CDz44pa1bRzhWCIKJgIASCZnAntdLJZxgKMuBT904uLOs\ndWpU7jVrzve1ay/cb9natRcmA0KL61fI3v7H53GXPsaxQnBcF4DsqOgJhlS7Q35U3c9HS/q7kIVq\nuRKRPJUl1ARBnW6rihN5lLlOjfrL4mWnn/67bdWvzO3SLeaP9THHvItjhdwwERCQjdCBeZpUll9w\n96fq7rA/aWavyvjGfamFniCo021VMb+wzHVq1l/ql23aNKR26lfmdukW88d/06Yh7d3LsUI+mAgI\nKIc0gfmcma119yckyczWqZZr3pWyyt/NMudvuW1VMb+winWq1279qt4u7Wp23obIx0+7zW4+VoyD\nAIAmlrulLmmLpCckfVXS11QbrfS2kLfxW30pp1SWPPPCs9xWFfMLq1inejGMXaiKZm2yc+cdmbdV\nlhN/VVW31htANajoCYYkycx+QdLG5Me73P0nAf6P0La8nmOe9bNgs5xcoxsnEKpineq1W7+qt0ur\nmp2369efo8cf/8oByzt5tnOr14huPFY8UxtAmYV+jnmaVBZJelnSv0o6VNLrk0LdnuaLZrZF0h+p\n9rDmL7n7Hyz6/DRJo6o9M+5FSR9z9zvTfDdvWefvZpnzt9y2qphfWMU61Wu3flVvl1Y1O2+fempl\nw+Wd5Hi3eo3oxmPFOAgAaG7ZwNzMPiDpo5KOlXSfanfOvyPpt1J8d4WkL0o6VdKMpHvM7CZ3f7hu\ntVvd/eZk/TdIul7S8Sm/m6tuzgkFyqrZeXvUUbN66qlsz2euEcujjQCguWVTWczsAUm/ploKyxvN\n7HWSPuPuZy67cbONkobc/T8nP1+qWm5OwzvfZvbrkv7C3X+ple/mlcoyOTmlt7zl09qz5xjVbuLP\nac2avdq587+29efnPCcYimWbWWpUPkltlbnouha9/6LM1/uHP3xSe/fu0atf/Rr19R2Waf0nJ6e0\nefNVmpgYUS0YnFVf35D+8i/fpfe//28OWD42tv2Afac9Ps321WibactetX4xOTml/v4r9cQTn9F8\nG61de5nGxy/K9JgPDu7QxMTz+vGPf6hjjlmj17zm6Mzbr4rHB8DSQqeypBlYeU/y732Sfi55/2Ca\nBHZJ75Z0dd3PWyX9cYP1zpC0S9JPJL2ple96zoM/G00EE/vgz3bEPkCrUfnKOilP0fsvykK9H3Ip\nbP2bPcM5zbOdWz0+WT0vuqr9Yvfux5Pz9HKXPunS5b5mzfmZ1atRu9X610OZtl9Vjw+ApSmCCYb+\nRtJRkoYl3S7pJkn/T6qNtxBcJ5+fImms1e/mFZjnOSlQUdsKuc0sNStf7Zd9a2Uuuq5F778oC/WO\nu/5FHZ+q9ovQ9Wp+bRjOZT9lPz4AlhY6MF82x9zd35W8HTazf5B0pKRvp7whPy1pbd3PxybLmu3r\nDjM7zsxe0ep3h4eH973v7+9Xf39/yiKml+ekQEVtK+Q2s9SsfLUUo/2XxT4pT9H7L8pCveOuf1HH\np6r9InS9ml8b5nLZT9mPD4D9jY+Pa3x8PLf9pX0qiyTJ3Xe2uP17JL0mmZToXySdJem99SuYWZ+7\nTyTvT5R0iLv/1MyW/W69+sA8lDSDlupzDg866Md65JEZPfvs0Tr66Fldc81FevObT069rSzLFcM2\nl9Jqrmaz8tV++Wq/ZTFMyrNU/bp1MNxCveOt/+TklB5//AeSLpd0sKRtktapnfK1kqc+OLhDDz00\nKWlQ0geSfUrSrI444hlt3TpS2rzm0P29+bVhxb79ZJEb3q3nbd7I40fRFt/sHRkZCbvDkLfja3f8\ntUXSI5Iek3RpsuwCSR9M3n9C0g8kfU/SnZJ+fanvNtlHBn+cWF5rE/k87tLH9lu3p+dc37nzjlTb\nyrJcIeqapXb2VaYc826cACqNPHPMOytf57nKaY9x431+LLmePOdr1px/wDiXGNqqFUWcb/XHLauJ\npbr1vM0TbYwYqegc8zK88grM3Zce2LV/zmHj/MP1689Mta0syxXTNhtpN1ezUfnaLXPIuqapX15t\nHZv5em/c+FFfv/5M37jx96Kpf7Pjtn79mS2XL20fb7beMce8ywcGhv200y5u61yJTej+vtCvfi/p\nVx/dt58sc8O79bzNC3n8iFHowLylVBYsPSHI/jmHS01qsvy2sixXTNtspN1czWbli21SnjT168aJ\nZqS4693suG3Y8Mst/yk9bR9vtt7rX/8Gfe1rQ9q0aSjVdmIX+rinv07Pa68NY+6/VUAeP7pR08Dc\nzJ6V5I0+Uu1/C0cEK1VJLH4G87PP/pyki1Rrtn9To/zQo46abXn7zXLrQufe5ZXbl2WuZoz5iFXI\nRY2xXUMrYhzIcutVoS/VK6JfVa0NYxHiWBZ1rLrxeoeIhLwdn9dLOaayzFsqP7Y+J7T+fX2Oefrt\nF5OXHHuOedFlrkK50ip7+dtVxDiQos/7PBVVlyq1YSxCtWkRx4r+geUolhxzSa9S7fGFayWtDVmo\nlitRQGC+3DOYa8tr7w8+eLOvX39m6qB8/+3vv9353LqingUcKrcvi1zNmPMRy5yLGnO7hlbEOJDl\n1itzX6pXZL+qShvGIuSxzPtYdfP1DumEDsyXzTE3s9Mk/aGkXkn/qlpexi5JvxTgBn5pLPcM5oXH\n9q3UKaf8um67rbXH6yyXW1fUs4BD5fZlkasZcz5imXNRY27X0IoYB7LcemXuS/WK7FdVacNYhDyW\neR+rbr7eIQ5pErVGJW2U9Ki7b5B0qqS7gpaqBA58BnO9+eW1953lpO6/3QNzTRt/3qnQ2w+hjGUu\nA9oVIdCvqqNKx7JKdUFJLXdLXdJ3k3/vl7Ri/n3I2/itvhRtjvlDfthh/8U3bvxEqj9d9/fv/9i/\nInJNFx4z9glfteqdSf2y3359XZdqg0brLrXtZm3S6rayqlcIWbRVq/uLNecyz3avoiLbL+Z+FcLO\nnXf4+vVn+pFHnt1yamPs8jiWefXVbuuXaJ2KzjGXdKukVZKuknSdpC9I+seQhWq5EgUE5u61E/i0\n0y72Qw453aUzXLrQpS0ubXfpoy5tW/bkThNM5pVr2qgsq1adt98zgLPefrMJPzqZLKjRs82rPFA2\nj0lnYszJ5RdoZ2Jovxj7VQg7d97hPT3n7tfWrTwMoAxCHsu8+2q39Eu0J4bAfKVqeRk9ks6VdKGk\n/y1koVquREGBuftSkwp1NqFIEQNNihpQun79mQ2XS5dnUpYqDZRtvK/Lo+lDeYrp3Ckj2i8/za5x\n9RPOoTn6KmISOjBPM8HQBZK+4e7Tkq7pOHemYppPKtTZhCJFDDQpakBpbdKlRgNoF+f0tVeWKg2U\nbbyvFbntPyYxnTtlRPvl58knG1/j6iecQ3P0VXSTNIH54ZJuMbOfSvqGpG+6+96wxSqHyckpPf74\nDyRdLulgSc+o9sCa6yVNqtEEQ61OKJKn0GVptv2jjprVU08duHzhyTadlaWoeoU4ho33NZfb/lsV\ncqKOmM6dMqL98nP00bN6+unG176yy2MyHvoqukraW+uSTpD0aUkPS7o15G38Vl8qdPBn/aDPD7n0\n24uWLUww1M6EIkXXKY9c7CxzzGOqV5VyzLMqa1nbvYq6Ie85FlVt67zOQc51xESBU1msto/lmdmr\nJf0fks6SdLi7nxDiPwrtMDNPW4+sbN06omuvvVgH3uX9rGpPmFxYdswxZ+utb/2VpncS5u84zMzM\nqbe32Ol/Q5el2fYbLZeUWVmKqlcIodsqK83OkYGBKzJ7LnFM507Z1I7Pe1T7C9+cailR79HAwPU8\n4zuA22+/U+eee6WeemqljjpqVtdcc5He/OaTiy5WR/I4x+dxriMWZiZ3t2DbXy6gNbMPSXqPpFdK\n+qak6939oVAFakcRgfmmTUMaH280adCQpJED1m11giGg7JqdI5wPceD4oFP0IXSj0IF5mhzzNZJ+\n193vC1WIMmqW85ZVXjRQduSFxo3jg07Rh4DspU5liVmed8zn/xz5b//2c3rhhZ/qpZc+L+l4SbNa\nu/Yyub+gPXs+r9qFalZ9fUMaG9ue+k9urQykCTHoJsttNtqWpOADhZbaP3/6zNZSbTw5OaXNm6/S\nxMSI5s+HQw/drre97Uh9/vO/y7EoWKPj0+r1qkq4XrQuzz7E8UEsQt8xL3zgZhYv5TT4s9EAHrP3\n+RvfeMEBMzC2MzFBKwNcQgyGyXKbRQ9SZLBQeGnaeH4SrkMPfZ/XnrfefCA08sdEKjVcL9qXRx/i\n+CAmKnqCoTK88grMQ08S0cokCiEmXMhym0VPhMOEFOGlbWOOBWJHH40bxwcxCR2YkwjWgtCTRLQy\niUKICRey3GbRE+EwIUV4aduYY4HY0UfjxvFBN0kz+BOJZpNEvPDCj7R160iqnLel8uRaGUiTxaCb\nxWU58shnmm4zbX7f/HoPPTSpAydYan8inFbzC0MOSiLXsSZtG2d9LBq1/549P9K5516pJ59cqaOP\nrsaj6FqV1ZiObuzfDGKMG8cHXSXk7fi8Xiowx1w616U7UuW8LZcnl2eOeaPvN5vUp9EEQGknS6qf\nYKndHPN26hoqJ5FcxwVp2yL02IX/8B/O84MOOnu/ZVWYvKUVWY3p6Nb+3a31LguOD2IicszjCczd\na8H5+vVn+sEH/7ZLZyRBuafKeUuTJ9fKQJpOBt00K8tpp118wDY7zSU+5ph3dTQ4tt38whCDksh1\n3F/aNs7qWLQydiGrsR9lkNWYjm7u3wyEjRvHB7EIHZiTytKiN7/5ZE1OntxkYoWlc97S5Mlt2LAu\n9YxpraybtizPPnuYbrppKNW6aXOJX//6N+xXzlbL3G5+YSftk3VZqiptG2d1LFoZu5DV2I8yyGpM\nRzf37xDXC2SH44NuQWCeQn3O5ZFHPiP3Hj300F4dmEM9qyOOeEZbt450nEOedbnzyGcPVb+Y8gtj\nKkunypBLnG4cROOxC0cdNZtnUQvVuF+2Pqaj1f4det6DPOdwAIAohLwdn9dLAVNZ9s9tezzJmW6c\nQ71ixW97b++HmubB5ZknV0Q+ezfkdcdUlk6UoR5px0G88pVnuzSw6Lzc6tddd0PRVchNo7Z69avf\n6z095+y3bLnc+6LmUih6DgcASEuBU1mY+XMZW7eO6NprL1btDtKIpPn382YlnS3pVyQ9LWn0gM8H\nBq7Y9ye4+Ts9MzNz6u0Nd6dn/3J3Xpa064aqX17tVraytCtN/yhaszKedtqwDj981b72f+65p3TT\nTe+Q9N80P/ug9H9pYODOaOqSh8X98tlnn9PNN79f0vWq3T1fIek9Ghi4fsl2Sdu/s+xDrWyrDH0X\nQHWFnvmTVJZl7J9z2Tj/UnqDpKHklV0OeSeKymcPVb+Y8gtjKku7ypBLnHYcxKZNQ5LemrwWzMz8\nv6GLGJXF/bLWLserdl1akNXYjPDzHuQ3hwMAxKJ8ibE5W8i5lGrNtThvdVYLzTjX8PNic6GLLwvi\nU4b+kbaMZahLEUK3S5bbb2VbHG8AlRYyTyavlwLnmC88C7hZjvkdLl3uhxxyuh922LlBcqwHBoa9\nvz/9Y6J27348ycW93KVPunS5r1lzPnmYEZh/5OaRR57t69ef2TTnt53jnlYr4wZClSHLMpJzfKA0\n7dLJ8c2i3ef3/6Y3fdRXrTqPHHMA0RM55ssLmWM+OTmlt7zl09qz5xjV7ow/qcMOe0wnnPAGvepV\nrueff1J33jmnF164SrU/r+7SqlWX6Jd/+XXq61vZcf7x5OSUNm++ShMTI5rPn+3rG9LY2PYltzs5\nOaX+/iv1xBOf2fe9tWsv0/j4RaXLh66S22+/U6ee+ud66aU/0fxx6en5sP7+78/fb6bKdo97K5bL\nJc6jDJ2WsdX1us1S7ZLF8e2k3Q/cf/prJ8cbQFFC55gXfrc7i5cC3jFfbsKN0BNytLv9bp4oJGbr\n15/Z8LgsngwnhuMXQxkQTtHHt+j9A0A7FPiOOUl5y1huoFHogUjtbp8BUnF68smVSjMZTgzHL4Yy\nIJyij2/R+weAGPFUlmU0m3DjwQcf0NatIzriiOcl7dLiR5LNfz46uk2S2p4Mo90JbdJ8L/ZJOmIv\nXzuOPnpWTz+9/GQ4eUxktFz7VmkyJRyo6OPbyf6reG0AAEmksiyn0UCj+kmFGk3iIZ2bDAh9ztes\nOb9u8Ki3PFCp3YFOWU4wVITYy9eu6667waWti/rLgZPhhK5/2oGBVTwGqCn6+Ia6tgFASAqcylJ4\nUJ1JJQIG5u4LTw445pizvfaUk8frciIvb5gnKQ0v+XkreZTz+9+0qbUnJyz1vdjzO2MvX7tq9Rpz\n6UyXzk7+HWtYr3aPe/pyLN++IcuA4hV9fNvZf1WvDQDKIXRgHjyVxcy2SPoj1XI8vuTuf7Do8/dJ\nuiT58VlJH3L37yeffUzS76iWI/KApPPc/Wehy7zY/IQbmzYNae/ekUWfrlDjSYfmlvy8lTzKdie0\nWep7sed3xl6+dtXqlW4ynJATGaVt3ypMpoTmij6+7ey/qtcGAJAC55ib2QpJX5R0qqRl7YEbAAAg\nAElEQVQZSfeY2U3u/nDdarslvdndn06C+KslbTSzXknbJb3O3X9mZt+QdJakr4Qs81Ia50TONVjW\naNKh/HO9l9pu0fmly2m3fLHnntbqdeCYhLzbPevjH3u7o5wa9avYr10A0JGQt+MlbZT0rbqfL5V0\nyRLrHyVpT/K+V9KUpKNV+w/E30p6a5PvdfiHiXQa5TY2yiGvz0FfLsc8VL5kN+aYx14n99rkQj09\n5+5Xxp6ec5tOMhRKlm1VhnZH+TTrVzt33kF/A1AYlXmCITN7t6S3ufsHk5+3SjrJ3S9ssv7Fkn6x\nbv0LJX1a0vOSbnH3s5t8z0PWo16jiS0k7Vt2+OHPy+wlPfPMEQ0/XzwZxtatI7r22ou1+O7PwMAV\nHf2JOc12Y5+ko9XyhWrLLMVUxqyOf0x1QnUs1a9GR7dFfe0CUF2hJxiK5nGJZrZJ0nmSTkl+PkrS\n6ZLWSXpa0g1m9j53/x+Nvj88PLzvfX9/v/r7+4OUs1lO5HIBSN653mm2W3R+6XJaLV8Zck9jKmNW\nxz+mOqE6lupXsV+7AFTH+Pi4xsfHc9tf6MB8WtLaup+PTZbtx8xOUC23fIu7P5ksfquk3e7+02Sd\nv5b0G5KWDczLJFS+ZDfmYZahzmUoY6uqWCcUj34FIAaLb/aOjCx+CEjGQubJSDpI0g9Vu+t9iKT7\nJB2/aJ21kh6TtHHR8pNUexLLoZJM0g5JH26yn47yhYp0YB7lQ75q1Tt948ZPdPT4sk6eETwwMOz9\n/dk+Pi3UdhfvI/bc0zKUsVV51CmP/oO4VPFcAVB+KnOOubTvcYlf0MLjEj9rZhckFbvazP5c0pmq\nDfQ0SS+6+0nJd4dUexLLi5LulfQBd3+xwT48dD1Cms/1nZh4Uj/4wTN67rmrVLtLNKu+viGNjW1v\nK3+y1Rziyckpbd58lSYmRjLZf+jtNttX7LmnZShjq0LWKc/+g7hU8VwBUG6hc8yDB+Z5KHtgPq/o\nQXRFDkQFmqH/AABiETowJ1kvIkUPoityICrQDP0HANAtCMxbNDk5pa1bR7Rp05C2bh3R5ORUZtte\nGOxUL9vBTkuV/8D9T0ka1IMPTnZU1zzqheqKrf+EvAYACIdzF6UQMoE9r5cKnGAoy8FIRW9//88f\nTyZKYgIaFCum/hNTWQCkx7mLrKjsgz/zkFeOeR65riEHO7Uy6dCtt96vvXu/uuS6rWAQFzoRS/8h\n3x0oJ85dZKVrJhgqgzxyXUNOnNHKpEObNg1p797s6sqEIOhELP2HfHegnDh3URYE5i1od8KL+bt9\n09NzWr26uLt9rZSfyT2AA3FeAOXEuYvSCJknk9dLEeeYx5TX1kpZYio3EAvOC6CcOHeRFZFjvrw8\nn2Peaq5rbHltrZQ/lrxeICacF0A5ce4iC0wwlELMEwxt2jSk8fGRhstvu+3A5QAAAIgTEwyVXGzP\nYAYAAECcuGOegaUGd05OTmnz5qs0MTGiWjrLrPr6hjQ2tp0/oSGIWAYbAwBQNaSypFBkYJ4m8Cav\nDXnhP4IAAIRDYJ5CkYF5bIM70d3ojwAAhEOOeeSYtAAxoT8CAFBeTDDUoVgmLeg0r7gqeclVqUe7\nYumPAACgdaSydCiGnN5OyxBDHbJQlXp0gjYAACAccsxTiOWpLEUN7uw0r7gqeclVqUeniu6PAABU\nVejAnFSWDGzYsK7QwK/TvOKq5CVXpR6dKro/AgCA9pB4WgGdTmJUlUmQqlIPAADQnUhlqYBWnqVe\n5UmQqlIPAAAQJ3LMU+j2wFxaOq+4myZBqko9AABAfAjMUyAwXxqDIgEAADrHBEPoGIMiAQAA4sdT\nWboAk84AQHrdPlEZgOKQytIFGBQJAOlwvQSwFHLMUyAwXx6DIgFgeYzJAbAUJhhCJph0BgCWx5gc\nAEUiMA+IPMXyKPuxKnv5gVgwJgdAkUhlCYQ8xfIo+7Eqe/mBmHA+AVgKOeYpxBiYk6dYHmU/VmUv\nPxAbxuQAaIYc85IiT7E8yn6syl5+IDaMyQFQFJLmAlnIU6xHnmKMyn6syl5+AABQw2/uQEZHt6mv\nb0gLAVMtT3F0dFthZUJjZT9WZS8/AACoIcc8IPIUy6Psx6rs5QcAoAwY/JlCrIE5AAAAqiN0YB48\nlcXMtpjZw2b2qJld0uDz95nZ/cnrDjM7oe6zI83sm2a2y8weNLM3hS4vAAAAUISgd8zNbIWkRyWd\nKmlG0j2SznL3h+vW2Shpl7s/bWZbJA27+8bksx2Sdrr7l82sR9Jh7v5Mg/1wxxwAAABBlf2O+UmS\nHnP3KXd/UdLXJZ1ev4K73+XuTyc/3iVptSSZ2RGSftPdv5ys91KjoBwAAACogtCB+WpJe+p+/lGy\nrJkPSPpW8n6DpJ+Y2ZfN7HtmdrWZ/XygcgIAAACFimaCITPbJOk8Sacki3oknSjpw+7+XTP7I0mX\nSmo468Pw8PC+9/39/erv7w9ZXAAAAFTc+Pi4xsfHc9tf6BzzjarljG9Jfr5Ukrv7Hyxa7wRJfyVp\ni7tPJMuOkfQddz8u+fkUSZe4+zsb7IcccwBtm3/c5PT0nFav5nGTAIDGQueYh75jfo+k15jZOkn/\nIuksSe+tX8HM1qoWlJ89H5RLkrvvNbM9ZvaL7j4/gPShwOUF0GUmJ6e0efNVmpgYkbRS0qzuumtI\nY2PbCc4BALkKmmPu7i9L+oikWyQ9KOnr7r7LzC4wsw8mqw1KeoWkPzWze83s7rpNXCjpWjO7T9Kv\nSPpMyPIC6D6DgzvqgnJJWqmJiRENDu4osFQAgG4UPMfc3b8t6bWLlv1Z3fvzJZ3f5Lv3S/q1oAUE\n0NWmp+e0EJTPW6mZmbkiigMA6GLBJxgCgJitXr1C0uyipbPq7eXyCADIF795AHS10dFt6usb0kJw\nPqu+viGNjm4rrEwAgO4U9KkseeGpLAA6Mf9UlpmZOfX28lQWAEBjoZ/KQmAOAAAApBA6MCeVBQAA\nAIgAgTkAAAAQAQJzAAAAIAIE5gAAAEAECMwBAACACBCYAwAAABHoKboAAID05p+5Pj09p9WreeY6\nAFQJzzEHgJKYnJzS5s1XaWJiRNJKzc9SOja2neAcAHLAc8wBAJKkwcEddUG5JK3UxMSIBgd3FFgq\nAEBWCMxRSePj40UXAREqe7+Ynp7TQlA+b6VmZuaKKE5llL1fIAz6BYpAYI5K4oKKRsreL1avXiFp\ndtHSWfX2cinvRNn7BcKgX6AIXM0BoCRGR7epr29IC8F5Lcd8dHRbYWUCAGSHp7IAQEls2LBOY2Pb\nNTh4hWZm5tTbu0Kjowz8BICqqMxTWYouAwAAAKov5FNZKhGYAwAAAGVHjjkAAAAQAQJzAAAAIAIE\n5gAAAEAEogvMzWyLmT1sZo+a2SVN1vljM3vMzO4zszcu910zO9rMbjGzR8zs78zsyDzqguwE6hdD\nZvYjM/te8tqSR12QnTb6xa/WLf+Sme01s+8vWp/rRckF6hdcL0qu3d8jZnasmd1mZg+a2QNmdmHd\n+lwvSixQn+jsWuHu0bxU+4/CDyWtk3SwpPskvW7ROv9Z0v9M3r9J0l3LfVfSH0j6RPL+EkmfLbqu\nvKLoF0OSLiq6frzy7xfJz6dIeqOk7y/6DteLEr8C9guuFyV+dfh75NWS3pi8XyXpEeKL8r8C9omO\nrhWx3TE/SdJj7j7l7i9K+rqk0xetc7qkr0iSu/+TpCPN7Jhlvnu6pGuS99dIOiNsNZCxUP1CkoI9\n8gjBddIv5O53SHqywXa5XpRbqH4hcb0os7b7hbv/2N3vS5Y/J2mXpNV13+F6UU6h+oTUwbUitsB8\ntaQ9dT//SPtXdKl1lvruMe6+V5Lc/ceSXpVhmRFeqH4hSR9J/jz1F/wJsnTa6RfTDdZZ7FVcL0ot\nVL+QuF6UWSb9wszWq/YXlbuSRVwvyivrPvFPdYvbvlbEFpi3o53/lfDw9upL0y/+VNJx7v5GST+W\ndGXYIqGkuF5A4nrR9cxslaQbJH3U3WebrMb1ooss6hPPJYs7ulbEFphPS1pb9/OxybLF66xpsM5S\n3/3x/J8pzezVkv41wzIjvCD9wt3/P08SwiT9uaRfy7DMCK+TfrGUvVwvSi1Iv+B6UXod9Qsz61Et\nAPuqu99Utw7Xi/IK0ic6vVbEFpjfI+k1ZrbOzA6RdJakmxetc7OkcyTJzDZKeir5M9JS371Z0rbk\n/bmSbhLKJEi/SC6i886U9IOw1UDGOukX80wH/nWF60W5BekXXC9Kr9N+8ZeSHnL3LzT4zrbkPdeL\ncgnSJzq+VhQ9KrbBKNktqo1ufUzSpcmyCyR9sG6dL6o2kvZ+SScu9d1k+Ssk3Zp8douko4quJ68o\n+sVXJH1ftZHYN6o2FqHwuvLKrV/8D0kzkv5d0hOSzkuWc70o+StQv+B6UfJXG/3iV5NlJ0t6OTn2\n90r6nqQtyWdcL0r8CtQnOrpWWLIRAAAAAAWKLZUFAAAA6EoE5gAAAEAECMwBAACACBCYAwAAABEg\nMAcAAAAiQGAOAAAARIDAHAAAAIgAgTkAdAkzWzzLKQAgIgTmAFBRyVTTD5vZNWb2gKSzzewfzey7\nZvYNMzssWe/tZrbLzO4xsy+Y2d8WXHQA6EoE5gBQba9RbUrpfkm/I+lUd//fJf2zpIvM7Ock/XdJ\nb3P3X5P0SklMCQ0ABSAwB4Bqm3L3eyRtlPR6SXea2b2SzpG0TtLrJE24+xPJ+tcVU0wAQE/RBQAA\nBDWb/GuSbnH3gfoPzexXks8AAAXjjjkAVNt80H2XpJPNrE+SzOwwM/uPkh6RtMHM1ibr/Z8FlBEA\nIAJzAKg6lyR3/4mkbZKuM7P7Jf2jpNe6+/+S9CFJf2dm90h6RtLTBZUVALqauTPGBwC6mZmtdPfZ\n5P2fSHrU3b9QcLEAoOtwxxwAcL6Z3WtmD0o6QtKfFV0gAOhG3DEHAAAAIsAdcwAAACACBOYAAABA\nBAjMAQAAgAgQmAMAAAARIDAHAAAAIkBgDgAAAESAwBwAAACIAIE5AAAAEAECcwAAACACBOYAAABA\nBAjMAQAAgAgQmAMAAAARIDAHAAAAItBTdAGyYGZedBkAAABQfe5uobZdicBcktyJzbFgeHhYw8PD\nRRcDkaFfoBH6BRqhX6ARs2AxuSRSWQAAAIAoEJgDAAAAESAwRyX19/cXXQREiH6BRugXaIR+gSJY\nFXKzzcyrUA8AAADEy8yCDv7kjjkAAAAQAQJzAAAAIAIE5gAAAEAECMwBAACACBCYAwAAABEgMAcA\nAAAiQGAOAAAARIDAHAAAAIgAgTkAAAAQAQJzAAAAIAIE5gAAAEAEggfmZrbFzB42s0fN7JIGn59m\nZveb2b1mdreZnbzo8xVm9j0zuzl0WQEAAICimLuH27jZCkmPSjpV0oykeySd5e4P161zmLs/n7x/\ng6Tr3f34us8/Juk/STrC3U9rsh8PWQ8AAABUw+TklAYHd2h6ek6rV6/Q6Og2bdiwLtV3zUzubqHK\n1hNqw4mTJD3m7lOSZGZfl3S6pH2B+XxQnlglaW7+BzM7VtLbJX1a0kWBywoAAIAKm5yc0ubNV2li\nYkTSSkmzuuuuIY2NbU8dnIcUOpVltaQ9dT//KFm2HzM7w8x2SfpbSe+v++jzkn5PErfDAQAA0JHB\nwR11QbkkrdTExIgGB3cUWKoFoe+Yp+LuN0q60cxOkfQpSZvN7B2S9rr7fWbWL2nJPxsMDw/ve9/f\n36/+/v5g5QUAAED5TE/PaSEon7dSMzNzjVbX+Pi4xsfHQxdrn9CB+bSktXU/H5ssa8jd7zCz48zs\nFZJOlnSamb1d0s9LOtzMvuLu5zT6bn1gDgAAACy2evUKSbPaPzifVW9v4ySSxTd7R0ZGQhYv+ODP\ngyQ9otrgz3+RdLek97r7rrp1+tx9Inl/oqSb3H3Nou28RdLHGfwJAACAdjXKMe/rS59jXurBn+7+\nspl9RNItquWzf8ndd5nZBbWP/WpJ7zazcyT9TNILkt4TskwAAADoThs2rNPY2HYNDl6hmZk59fau\n0OhoHAM/pcB3zPPCHXMAAACEFvqOOTN/AgAAABEgMAcAAAAiQGAOAAAARIDAHAAAAIgAgTkAAAAQ\nAQJzAAAAIAIE5gAAAEAECMwBAACACBCYAwAAABEgMAcAAAAi0FN0AQAAQPeYnJzS4OAOTU/PafXq\nFRod3aYNG9YVXSwgCubuRZehY2bmVagHAABVNjk5pc2br9LExIiklZJm1dc3pLGx7QTnKAUzk7tb\nqO2TygIAAHIxOLijLiiXpJWamBjR4OCOAksFxIPAHAAA5GJ6ek4LQfm8lZqZmSuiOEB0CMwBAEAu\nVq9eIWl20dJZ9fYSjgASgTkAAMjJ6Og29fUNaSE4r+WYj45uK6xMQEwY/AkAAHIz/1SWmZk59fby\nVBaUS+jBnwTmAAAAQAo8lQUAAADoAkwwhGgxCQUAAOgmpLIgSkxCAQAAYkMqC7oSk1AAAIBuQ2CO\nKDEJBQAA6DbkmCNKC5NQ1AfnTEKB6mJMBYpE/wPiQI45okSOOboJ/R1Fov8B6fEc8xQIzKuJSSjQ\nLbZuHdG1116sxX8hGhi4Ql/72lBRxUKXoP8B6YUOzEllQbQ2bFjHLwV0BcZUoEj0PyAeBOYVQX4g\nUF6MqUCR6H9APEhlqQDyA4Fy4xxGkeh/QHrkmKfQ7YE5+YFA+TGmAkWi/wHpkGOOZZEfCJQfYypQ\nJPofEAcSyCpgIT+wHvmBAAAAZUIqS0B5DcgkP7BzDJ4tD44VAKAo5JinEGNgnnewTH5g+/iPTXlw\nrAAARSIwTyHGwJwBmeXBsSoPjhUAoEihA/PgSchmtsXMHjazR83skgafn2Zm95vZvWZ2t5mdnCw/\n1sxuM7MHzewBM7swdFmzxIDM8uBYlQfHCgBQZUGfymJmKyR9UdKpkmYk3WNmN7n7w3Wr3eruNyfr\nv0HS9ZKOl/SSpIvc/T4zWyXpn83slkXfjRYTNpRH3seKHOn2cazKhfYDgBa5e7CXpI2SvlX386WS\nLlli/V+X9GCTz26UdGqTzzw2u3c/7n19H3fpOZfcpee8r+/jvnv340UXDYvkeazoF53hWJUH7Qeg\nipKYM1jsHDTH3MzeLelt7v7B5Oetkk5y9wsXrXeGpN+X9EpJ73D3f1r0+XpJ45J+2d2fa7AfD1mP\ndjEgszzyOlbkSHeOY1UOtB+AKuqKCYbc/UZJN5rZKZI+JWnz/GdJGssNkj7aKCifNzw8vO99f3+/\n+vv7QxU3NSZsKI+8jhU50p3jWJUD7QegCsbHxzU+Pp7b/kIH5tOS1tb9fGyyrCF3v8PMjjOzV7j7\nT82sR7Wg/KvuftNSO6oPzIFW5ZULy9iD8qjqsaKvA0B6i2/2joyMhN1hyDwZSQdJ+qGkdZIOkXSf\npOMXrdNX9/5ESXvqfv6KpCtT7KfNTCGAvGU0VsVjRV8HgM6ozDnmUu1xiZK+oNqjGb/k7p81swuS\nil1tZp+QdI6kn0l6QdLF7v6d5LGJt0t6QJInr8vc/dsN9uGh64HqyjsXlrEH5VG1Y0VfB4DOlD7H\nPAmkX7to2Z/Vvf+cpM81+N6dqt1xB4LKOxeWsQflUbVjRV8HgLiR7Ieut5ALW49cWFQPfR0A4hY8\nlSUPVUpliX1CjjzLl9e+JientHnzVZqYGFHtbuKs+vqGNDa2Paq2R/tiP6/yQl9Ht+HcX1DFtiii\nTqFTWYIO/szrpYoM/ox9sFSVB47t3v24DwwM+6ZNn/SBgeFo2hydi/28yht9Hd2Cc39BFduiqDop\n8ODPwoPqTCpRkcB8YGC4roP5vo42MDBcdNHcPd/yxd4WKA/6EtCdOPcXVLEtiqpT6MCcxMKIxD4h\nR57li70tUB70JaA7ce4vqGJbVLFOUiQzf6Im9gk5auXbJel6SXOqjR1+T5Dyxd4WKA/6UvlUMRcW\n+SvDuc+EX+2rYp0kkcoSk9hzwHbuvMN7es7dr3w9Pef6zp13ZL6v2NsC5UFfKheOF7ISe1+q8rit\nPFQ1x5ynskQm5gk5mJwEZUVfKo+8rzOotpjPfX6ndq6IOpV+giG0JuYJOZicBGVFXyqPquaNohgx\nn/v8Tu1cFetU8kQc5InJSQCExnUG3YK+jkZIZUFqnUxOwmAuAGkwCVK5cG1vH329nEKnshCYoyXt\n5HNx8QHQiirmwlYR1/bO0dfLh8A8BQLzuDGYCwCqh2s7ulHowJxEJgTHYC4AqB6u7UD2KhOYb906\nosnJqaKLgQYY4FJ9k5NT2rp1RJs2DXEuVhTHGItxbS8XzuGSCPmQ9LxekirxsPyqynNiIuSvihNX\nYH8cYzRCvygPjlV2FHiCoQr9t3alJiZGNDi4o+iCYJGrr75VL710iaQrJA1JukIvvXSJrr761oJL\nhiwMDu6oG/wlcS5WD8cYjWzYsE5jY9s1MHCFNm0a0sDAFQz8jBTncHlUbIIhcttiVMtDPF61oHwB\nx6oayDOtPo4xmqniBC9VxDlcHhULzMuf2xb7M2HbKd9CHuL+I/fLfqzylGe/aHVfZTi+sZ9XsWv3\nGNPuyAp9aQG/hysuZJ5MXi9VJMc89hywdssXe71il2f7tbOv2I9v7OUrgyr2C5QHfWkBv4eLp8A5\n5oUH1ZlUQvKBgeHSd7CBgeG6k8b3nTwDA8NFF83dOyvf7t2P+8DAsG/a9MlKHKs85dkv2t1XzMc3\n9vPq/2/v/sPtquo7j7+/kHGoFyWx1NKEhMSrVfQZVFRkGhtvwExTx/ojWB4ksWAr0kcFR4cZGEua\nZDJ1xMdSNU6fGdRHsMXwy3Z0xmqJP24CUSwCwV+gGK4Bc1usDyHj3I4VzXf+2Ptyzr05995z9jlr\n7bXX+bye5zw559y9z/6uddbae2Wf796rKXr9jlXvMihqSy06Dtcv9MA8m1SWHHLcUs8B6yc+5SFW\nF7NdVN1Wyt9v6v2qKXr9jlXvMihqSy06DudPyUUJSf2esKnHl6uY9Z7jd5xjmZpA9S6DorbUoroY\nAgudUgdeB5zQ9nox8NqQp/F7fRTFaL7Uc8Bixzf9s9vYWJo/u8WKr5+cwl7jS70NVpFjmZpA9S6D\n0k9bSv040qum9Kvc6r0dgVNZrNjG3Mxsn7u/YNZ797j7C0P8R6EKM/OFytEU01dbT04eYenS9K48\njxXfxMQB1q3b0Xbf1SlGR7ckc4/c2PH1Wu/9xJd6G6wixzI1gepdBqVKW0r9OFJV6v0q13qfZma4\nuwXbwEIjd+AbHd77Zsj/LfT6IJMz5tKS+sU+ik9EJG3aD9Yj93ongZk/v25mV5vZaPm4Grgr2P8U\nREj/Yh/FJyKSNu0H66F67083A/NLgJ8BNwI3AD8F3hYyqCo2bdrGxMSBusOozcTEATZt2sbatVuy\nqIuqF7hUrYde10v9ApzU48tZbn2xH6oLqVPs44gUdPzpU8jT8bEeZDLBUFVNuRikFzEnNMlx8pTU\n48uV6r1FdSF1y3Hf3gS51yF1TzAE7AIWt71eAvxtyKB6LgRkl8PUi1zzuWJNaJLjpDru6ceXo1z7\nYhWqC0mBJsaqR87Hn9AD824mGDrR3R9rO8N+yMyePuAT9wMynDlMueZzxZrQJMdJdSD9+HKUa1+s\nQnUhKdDEWPXQ8ae6bgbmR8xshbs/BGBmpwCJ3ptwOHOYWvlc7TuT4auLqvUQu/6mb3V18OARli0L\ne8vJGNuRFvXFlqIu7gNuAo5QXNJ0bhZ1ob6VL/Xh4ZB0H17olDqwHngI+AvgL4EDwG+FPI3f6wPl\nmGedz9WtmDnmsWNMdTsyk+q9Zffu233Rogtm1MWiRRf47t231x1aX/Qd503fb/76/Y6pO8e8iIET\ngVeVjxNDBlSpEJBdDlOvcs7n6kXVeohVf7HyF5UnWR/1xUKubTDXckmL+nDe+u3DoQfm3aSyAPwC\n+BFwHPDcctajPd2saGbrgQ9Q/I75MXe/atbfXw1sp/it83Hgne6+t5t12w17LpPyuQpV6yFW/cXK\nX1SeZH3UFwu5tsFcyyUt6sN5S70PLzgwN7M3A+8ATgb2AWcCXwXO6mLdY4APA2cDk8CdZvZpd7+/\nbbEvuPtnyuX/FUVC4qldrivSKLHyF5UnKXXLtQ3mWi6RYZF6H+4mincALwEOuPta4IXAY/Ov8oQz\ngAfc/YC7P04xQdFr2hdw939qe3k8xZnzrtZt1+0kAFUmDtizZy+rVp3D4sW/x6pV57Bnz94F15GZ\nVIeF7dsvZHR0C63JF6YYHd3C9u0Xzrter+226naqqvr9aiKPfPXTBm+44VMcf/zZLFr0uxx//Nnc\ncMOnwgXao1h9OLbU4xPppEq7jX187NlCuS7AneW/+4B/WT7/djd5MsA5wDVtrzcBH+qw3GspLt//\nMfDSXtYt/9ZV8n6VhP9cL2CKSXU4U6/5i/1c1BojT7Lq96uLrPJXpQ3u3HmLw6YZ7QI2+c6dt0SI\nuDux+nAsqccn0kk/7baf4yN1X/wJ/DWwGNgK7AE+DfxNVx/ew+C6/PvLgF29rkuXEwxVSfhfuXJD\nx3VWrtwwz9cm7VSH/Un9YrOq32/q5ZJ6jIyc1bFdjIycVXdolaXe1lOPT6STutpt6IH5gjnm7v66\n8ulWM/sycALw+S5PyB8EVrS9Prl8b65t3W5mzzCzp/W6bvH/Bti798uMj7+csbGxo4OpkPB/6NBI\nx3Uee2z2ezIX1WF/Ur9Qper3m3q5pB4//enT6NQuivebKfW2nnp8Ip3Earfj4+OMj48P9DPn0+1d\nWQBw9909fv6dwDPLSYn+HjgPeEP7AmY26u77y+enA09y90fNbMF1Z9oKTLF6tXUclEO1hP8lS6Y4\nfPjodRYvnpprlb5Uven9nj17ueCCqzl0aIQlS6a47rp3sWbN6iAx9qqow6MnGglVh1WkPNlA6hMg\nVe0jqV+AI/U47rhHmZo6ul0cd9yjdYV0lF77SOqTLfXTFzVhWnPkVoexjiFjYwG7fikAABtPSURB\nVGMzxpXbtm0b6OcfJeTp+OKMP+uB7wIPAFeU710MvKV8/h+BbwF3A3uBfz3funNsI4sc86r5Uqnn\ncKeeM5p6fmXqEyApx1wGKcf9Rer76NQnZ9O+on851mFdZaLuHPMmPOhhgqEqCf+7d9/uK1du8MWL\n3+grV24ItjOtmi+Veg536vmLqcfnnv4ESFX7iCbykE527rzFR0bO8mOPfb2PjJyVzKDcvVofyXUf\nownTmiPXOqzjGBJ6YN5TKkvKup0MoMrEAWvWrGZiInxaSNV8qdRzuFPPX0w9Pkh/AqSqfUQTeUgn\n5513Duedd07dYXRUpY/kuo/RhGnNkWsd5ngMmXNgbmY/AbzTnyj+t/DUYFElJlZeVtV8qao5vqmX\nK/X4cpRrXeSWWzkt5rUludZhr6r0kSb0qyrfbxMmTFO7LaR+HJY2IU/Hx3oUxQgj9fxe92o5mamX\nK/X4cpVjXeRYJvdmXP+So9T3Z1XkmmOeer3HlGO7rQup5JgDT6e4feEKYEXIoHouRMCBeey8rOp5\nfrscNji8sfx3V1I5j72WK/X4cpZbXeSaWxnz2pJc67CqKn0k5X7Vz/cbq1wp58A3RerH4aYIPTBf\nMMfczF4N/CmwFPgRcArFfZ+eF+AEfnJi52VVz/N7RflomZy8bYF10i1X6vHlLLe6yDW3Mua1JbnW\nYVVV+kjK/aqf7zdWuVLOgW+K1I/DUugmEWw7cCbwPXdfBZwN3BE0qoS08rLapZUbWCXG1MuVenzS\nHLm2pSVLpuhUrhDzA+Rah1LI9fvNtVyxqP5qstApdeDr5b/3AsdMPw95Gr/XB4nmmE//bDQ21vut\nGU84ofvbzj344A98+fKLHK50+GOHK3358osanTvWT05hr3Xez3rSnxj1HrsPV9Vr31eOeX1i7dtj\nid1HYvWr2O02ZrlyrL+moO4cc+ALwPHADmAn8EHgKyGD6rkQAQfm7tXzCWNO1LJixaUz1lux4tKu\nB7Ep5jy69x6fLhBqltgX+Mbow1VV7fux5lhwT39/EUvMfXtMsfpIXYPlGDnwKV8I28/21O9nSmFg\nPkKR8rIIuAC4FPjlkEH1XIjAA/Mqqlw0UfViLl2gUahaD6q/eqRe7zHjS32SMGmJuW9PXa6TLVWh\nyZaGR+iBeTcTDF0M3OjuB4Hr+s6dGRJVLpqoejGXLtAoVK0H1V89Uq/3mPGlPkmYtMTct6cu18mW\nqtBkSzIo3QzMnwLcamaPAjcCN7v7I2HDar4qN/MvJgq6D7gJOELxQ8W5C17M1YTJK2KoWg9NqL8c\nJ3lIvd5jxld1krCYcmyDVRTt4uj99ML79rS/3ypynWypiiZMtiQN0e2pdeA04E+A+4EvhDyN3+uD\nBFNZquSBVZkoqOq2cpRrjnnq8VWVermakGMeS+rfVUxVvqvUv9+qqpQr17aUa465HI3AqSxWbGNh\nZnYS8LvAecBT3P20EP9RqMLMvNtyxDR9hmly8ghLly58hmnTpm1cf/1lzP6f8MaN71/w3qO9bitX\nVesh5frrp12kLuV6h7jx7dmzlwsuuJrHHhth8eIprrvuXaxZszrItnqVcxvsVdW6SPn7raqoi3OZ\n/evBxo03zVsXqff7qmKVK9f6awozw90t1Od3M8HQW4FzgV8BbgYucvfvhAooJzFv5p/y5BUxVa2H\nlOsv55zClOsd4sa3Zs1qJibSHKjl3AZ7VbUuUv5+qyrq4lRgZh9ZqC5S7/dVpTzZkjRHNznmy4F/\n5+77Qgcz7JQ7Jp2oXUjd1AZbVBctqguRwes6lSVlZuYbN24N/rNR6IueJiYOsG7dDvbv30axo5ti\ndHQLu3Zdop+pEpR6u9DFev1THRa0b2qZmDjA2NjVPPTQe5iuixUr3s34+LuSqYuY+6ZYdRGzL6be\n71UX9QqdylL7hZuDeADZXGihm/k3Q+rtQhcI9U91OJP2TYUqMy3HFHvirhh1EbtMKfd71UX9qHuC\noSY8ioF50Wh0M3+JIfV2kXp8TaA6lE5Sbxcx48txUh19v/Vsq0lCD8wzSwTTzfwljtTbRerxNYHq\nUDpJvV3EjC/HSXX0/dazLWnp5uLPBtHN/HOSch5d6u0i9fiaoMpEMjJT6vmpVeKr2i5i1UXMvl91\nW9O3jjx0aIQlSxa+dWQTyhRLP/HFPM7l2PejCXk6PtaDjHLMpZB6Hl3q7SLXCU1iUh32J/U+UjW+\n1CfVSX1bqr/+xJxIT5P2dYZyzLsbmIe8GEkXPcXXhDy6lNtFUabvOGz14qKs4vWw5wb2QvmV/Um9\n/qrGV2W92HURc9/U67ZWrtzQsS5Wrtww0O30I+V9u3u1+GIe53Lt+9NCD8yzSWUJebN93cw/vibk\n0aXcLqpO/CEtyq/sT+r1VzW+KuvFrouY+6Zet3Xo0Aid6uKxx2a/1992+pHyvh2qxRfzOJdr348l\nm4F5SEnnImWqam5b9ZzRONuqSrmB/ek1pxXSzzWtKsdc5yqqxldlvdTroh+9tqclS6Y4fPjouli8\neCp0qF1LfR8Y8zhXRertPfX4ak9DGcSjKEYYqedK5Sr1fLjUcx5zzQ2somqueI51kXq7jSnXHPOY\nYuWYx5T6d9WEfXuudTgN5ZjXOzBPPVcqZ73mtvXzXcXcVq+UG9ifqjmt7unnmvYq51znKqr3kd6v\n30i9Lqqo2p52777dV67c4IsXv9FXrtyQzKDcPf19YMzjXD9Sb+/9xBd6YK5UlgWknouUs15z2/r5\nrmJuq1fKDexP1ZxWSD/XtFc55zpXUb2P9H79Rup1UUXV9rRmzWomJuZPJatL6vvAmMe5fqTe3lOO\nL5GEmnS1cpHaJZSLJE+I+V1pW82xZMkUncqUUk5rLDl+v7GpDltyrIvUy5R6fDIAIU/Hx3rQQyrL\n9M8XY2Pd/XyReq6UtOSaQ9dPTmEvbX16nRUrLp2xrRUrLm10e4+d01ql3mPR/qx/Mftj6nJsT/2U\nqeo+t9cxScx9dMx225Q+gnLMBzcw73eHmmqulLTkmkPX67b6aevLl1/kcGWZP3ulL19+UePbfKyc\n1iYMVLQ/61+s/tgEObanKmWKdZF+zH10E05A1UED8wEOzFO/qENkUGJOniItqj/pRO0if7EmnmrC\njQdS31a/Qg/MhyopKfWLOkQGJebkKdKi+pNO1C7yF2viqSbceCD1baVuqO7KkvxN5Rsg9YkXqsqt\nXDEnT+lHlYl/qmjCpDq5tUFp0bGnPin3/diTVcWcqK5XxbbuA24CjlDcm+Tc4ewjIU/Hx3oQOMdc\nCrnWX47lasIkFLEuymxCnmSObVBa9P3WI/W+H2ud2NuqIvWJp9qhHPPBDczd87xQJZYm5YD1Itdy\nVW3rsfpIPxP/9KIJk+rk2galRcee+JrQ92OtE3OiuiqatA8MPTAPnspiZuuBD1D8LvExd79q1t/P\nBy4vX/4EeKu7f6P82zuBP6D4XeObwJvc/Wf9xJPyTeVTl2sOWK7lqtrWY/WRfib+6UUTJtXJtQ1K\ni4498TWh78daJ+ZEdVVoH9gSdGBuZscAHwbOBiaBO83s0+5+f9tiDwJr3P1wOYi/BjjTzJYClwDP\ncfefmdmNwHnAJ0LGLHPLNX9WuW0zxfquliyZ4vDho+t90BP/NCG/twkxpi7lfQykH1+O1K9aUq+L\n1OOLKuTpeOBM4HNtr68ALp9n+cXAw+XzpcABYAnFfyD+F/CKOdbr84cJ6Uau+bNNym0LLeZ3tXPn\nLQ6bZmwLNvnOnbcMdDuptz/3ZsSYstTrL/X4cqV6b0m9LlKPrx1NzjEHzgGuaXu9CfjQPMtfNmv5\nSynSWx4B/mKe9QZQ1dKNHPNnU48vplzvW9uE/N4mxJiq1Ptw6vHlTP2qJfW6SD2+aaEH5sncLtHM\n1gJvAl5Wvl4MvAY4BTgM3GJm57v7Jzutv3Xr1ieej42NMTY2Fjji4ZRj/mzq8cWU631rm5Df24QY\nU5V6H049vpypX7WkXhepxjc+Ps74+Hi07YUemB8EVrS9Prl8bwYzO40it3y9ux8q334F8KC7P1ou\n81fAbwALDswlLannjqUeX0zx71urepf+pd6WUo9PROY2+2Tvtm3bwm4w5Ol44Fjg+xRnvZ8E7ANO\nnbXMCuAB4MxZ759BcSeW4wADrgXeNsd2BvLzhISReu5Y7Pimf64bG0vv57p+riPotUyptwv3tL8r\naUm9LaUeX2zqV9JkBE5lsWIb4ZR3WvkgrdslvtfMLi4Ldo2ZfQTYQHGhpwGPu/sZ5bpbKO7E8jhw\nD/Bmd3+8wzY8dDmkP9N3JJicPMLSpendkSBWfBMTB1i3bgf792+jOHs2xejoFnbtuiSZ+ui1Lvop\nU8rtognflbSk3JYg/fhiUb+SpjMz3N2CfX4OA1oNzKUpNm3axvXXX8bsn7Q3bnx/krl13cixTJBv\nuUTqpH4lTRd6YK4EN5GIcrwILMcyQb7lEqmT+pXI/JK5K0ssmuShHqr3Qo4XgeVYJsi3XFWl3odT\njy+mlOtC/UpkASET2GM96PLiT12AUw/Ve0uOdZFjmdzzLVcVqddF6vHFlHpdpB6fyEJo+sWfMXSb\nY67ctnqo3mfK8SKwHMsE+ZarV6n34dTji6kJdaF+JU0WOsd8qFJZlNtWD9X7TKlOotCPHMsE+Zar\nV6n34dTji6kJdaF+JTK3oRqYK7etHqp3kWZLvQ+nHl9MqguZS8rXHkibkHkysR4oxzxpqneRZku9\nD6ceX0yqC+lE7WJwUI75wnq5j7ly2+qhehdpttT7cOrxxaS6kNmacO1BU2iCoS5ogiERERGRztau\n3cL4+LaO73/pS0e/L3PTBEMiIiIiUlnr2oN2uvYgRTpjLiIiIpXogsKWlOtiYuIA69btYP/+bRTp\nLFOMjm5h165LkomxKZTK0gUNzEVEROLSYK+lCXWhaw8GQwPzLmhgLiIiEpcuKGxRXQwP5ZiLiIhI\ncpowmVEsqgsZlKGaYKgJUs5RExERmabJjFpUF/3T+KegVJaENCFHTUREBGDPnr2cffZH+PnP/xvT\nx6xFi97GF794EWvWrK47vKh0/O5Pk+pPOeZdyGVgrhw1ERFpiuKYdS5wE3CEIjv2XDZuvGkoj1m6\nuLK6Jo1/Qg/MlcqSEOWoiYhIUxTHrFOBmQOnYT1mrVp1SnKDyKbQ+KdFyU8J0QQAIiLSFDpmyaCo\nLbUolSUhTcqxEhGR4daEY5YuKGyGJrSlacox70IuA3NQjpqIiDRHysesJg32JO221E4D8y7kNDAX\nERGR/jXpgkJpDk0wJCIiItIjXVAoTaS7soiIiEh2cp30R3nzeVMqi4iIiGQnxxzzHMvUNMox74IG\n5iIiIjJbUy4o7Jby5uunCYZEREREKsht0h/lzedPA3MREUlK6jm0qccn+eonb17tthmUyiIiIslI\nPYc29fgkb1Xbn9rt4CjHvAsamIuI5CH1HNrU45P8VcmbV7sdHOWYi4jI0Eg9hzb1+CR/VfLm1W6b\no9k38xQRkay0cmjbpXPv6dTjE+lE7bY59I2IiEgytm+/kNHRLbQGEUUu7PbtF9YWU7vU4xPpRO22\nOZRjLiIiSUn93tOpxyfSidrtYOjizy5oYC4iIiIioYUemAdPZTGz9WZ2v5l9z8wu7/D3883s3vJx\nu5md1va3E8zsZjO7z8y+bWYvDR2viIiIiEgdgp4xN7NjgO8BZwOTwJ3Aee5+f9syZwL3ufthM1sP\nbHX3M8u/XQvsdvePm9ki4Mnu/n86bEdnzEVEREQkqKafMT8DeMDdD7j748ANwGvaF3D3O9z9cPny\nDmAZgJk9FfhNd/94udzPOw3KRURERERyEHpgvgx4uO31D8v35vJm4HPl81XAj83s42Z2t5ldY2a/\nFChOEREREZFaJTPBkJmtBd4EvKx8axFwOvA2d/+6mX0AuALoeFf9rVu3PvF8bGyMsbGxkOGKiIiI\nSObGx8cZHx+Ptr3QOeZnUuSMry9fXwG4u181a7nTgE8B6919f/nerwJfdfdnlK9fBlzu7r/TYTvK\nMa9g+tZJBw8eYdky3TpJREREZD6hc8xDnzG/E3immZ0C/D1wHvCG9gXMbAXFoPyN04NyAHd/xMwe\nNrNfd/fpC0i/EzjeoTExcYB163awf/82iml6p7jjji3s2nWJBuciIiIiNQiaY+7uvwDeDtwKfBu4\nwd3vM7OLzewt5WKbgacBf25m95jZ37V9xKXA9Wa2D3g+8J6Q8Q6TzZuvbRuUA4ywf/82Nm++tsao\nRERERIZX8Bxzd/888OxZ7/2PtucXARfNse69wEuCBjikDh48QmtQPm2EyckjdYQjIiIiMvSCTzAk\naVq27Bhgata7UyxdqiYhIiIiUgeNwobU9u0XMjq6hdbgfIrR0S1s335hbTGJiIiIDLOgd2WJRXdl\nqWb6riyTk0dYulR3ZRERERGZT+i7smhgLiIiIiLShdADc6WyiIiIiIgkQANzEREREZEEaGAuIiIi\nIpIADcxFRERERBKggbmIiIiISAI0MBcRERERScCiugMQERERkfRMz3dy8OARli3TfCcx6D7mIiIi\nIjLDxMQB1q3bwf7924ARpmcI37XrkqEenOs+5iIiIiIS1ebN17YNygFG2L9/G5s3X1tjVPnTwFyy\nND4+XncIkiC1C+lE7UI6GfZ2cfDgEVqD8mkjTE4eqSOcoaGBuWRp2Heo0pnahXSidiGdDHu7WLbs\nGGBq1rtTLF2qoWNIql0RERERmWH79gsZHd1Ca3Be5Jhv335hbTENA92VRURERERmWLXqFHbtuoTN\nm9/P5OQRli49hu3bh/vCzxiyuStL3TGIiIiISP5C3pUli4G5iIiIiEjTKcdcRERERCQBGpiLiIiI\niCRAA3MRERERkQQkNzA3s/Vmdr+Zfc/MLp9jmQ+Z2QNmts/MXjDrb8eY2d1m9pm295aY2a1m9l0z\n+1szOyF0OWSwArWL15vZt8zsF2Z2eugyyOAFahfvM7P7yuU/ZWZPDV0OGaxA7eI/m9m9ZnaPmX3e\nzE4KXQ4ZrBDtou1v/97MjpjZ00LFL4MXaF+xxcx+WL5/t5mt7yWmpAbmZnYM8GHgt4DnAW8ws+fM\nWua3gVF3fxZwMfDfZ33MO4DvzHrvCuAL7v5s4EvAfwoQvgQSsF18E3gdsDtE3BJWwHZxK/A8d38B\n8ADaXzRKwHbxPnd/vru/EPgssCVE/BJGwHaBmZ0MrAMOBAhdAgnZJoCr3f308vH5XuJKamAOnAE8\n4O4H3P1x4AbgNbOWeQ3wCQB3/xpwgpn9KjzROV4JfLTDOteVz68DXhsmfAkkSLtw9++6+wNAsNse\nSVCh2sUX3H16zuk7gJPDFUECCNUu/m/byxFA85I3S6jxBcCfAf8hVOASTMg2UXlckdrAfBnwcNvr\nH5bvzbfMwbZlpjvH7HtAPt3dHwFw938Anj6ogCWKUO1Cmi1Gu/h94HP9hSmRBWsXZvZfzOwh4Hzg\njwcVsEQRpF2Y2auBh939mwONVmIIeQx5e5n68tFe06dTG5hXZmb/FnjE3fdR/E9lvv+taIA2JHps\nFzIkumkXZvZHwOPu/snY8Uk9FmoX7n6lu68ArgcuqSFEqcFc7cLMfgl4NzPTmnSMGQIL7Cv+HHhG\nmQ75D8DVvXx2agPzg8CKttcnl+/NXmZ5h2VWA682sweBncBaM/tEucwjbT89nAT8KEDsEk6odiHN\nFqxdmNmFFD9Rnj/4sCWwGPuLTwLnDCxiiSFEuxgFVgL3mtlEufxdZqZf5ZshyL7C3f/RW7N3fgR4\nSU9RuXsyD+BY4PvAKcCTgH3AqbOWeSXw2fL5mcAdHT7n5cBn2l5fBVxePr8ceG/dZdWj/nbR9v6X\ngRfVXU490mgXwHrg28Av111GPZJqF89se34JcFPdZdWj/nYx628TwJK6y6pHvW0COKnt+TuBT/YS\n1yIS4u6/MLO3U9wV4RjgY+5+n5ldXPzZr3H3vzGzV5rZ94Ep4E1dfPRVwE1m9vsUV02fG6oMMnih\n2oWZvRbYAZwI/G8z2+fuvx2wKDJAAfcXOyh20rvMDIod8VsDFUMGLGC7eK+Z/TrFRZ8HgD8MVQYZ\nvIDtYsZmUCpLYwRsE+8rb6t4BPgBxd1cumbliF5ERERERGqUWo65iIiIiMhQ0sBcRERERCQBGpiL\niIiIiCRAA3MRERERkQRoYC4iIiIikgANzEVEREREEqCBuYiIiIhIAjQwFxFpEDO7xsyes8AyHzez\nDR3eP8XM3hAuuie2c4GZ7Qi9HRGR3GhgLiLSIO7+Fne/v+Lqq4DzBxnPPDR7nYhIjzQwFxGpgZld\nVk4HjZn9mZl9sXy+1sz+0szWmdlXzOzrZnajmT25/PuXzez08vkfmNl3zeyO8kz6h9o28XIz22tm\n3287e/5fgZeZ2d1m9o454nqumX2tXGafmY2W7/+emd1rZveY2XXle68qt32Xmd1qZr/S4fNONLNb\nys/8mpn9xoCqUEQkOxqYi4jU4zbgN8vnLwJGzOzY8r1vAFcCZ7v7i4G7gHe1r2xmv1YucwawGpid\n3nKSu68Gfge4qnzvCuA2dz/d3T84R1x/CHzA3U8HXgz80MyeC7wbGHP3FwLTg/rb3P1Md38RcCNw\neYfP+yBwtbu/FHg98NH5KkVEZJgtqjsAEZEhdRfwIjN7CvDP5euXUAzMPwM8F9hrZgb8C+Ars9Y/\nAxh398MAZnYz8Ky2v/9PAHe/z8ye3kNcXwX+yMyWA3/l7t83s7OAm939UPmZj5XLLjezm4BfK2Oc\n6PB5rwBOLcsBcLyZPdnd/6mHmEREhoIG5iIiNXD3n5vZD4ALgb0UZ8nXAqPAg8Ct7r5xgY+xef72\nz10uNzuunWZ2B/Aq4LNmdvE8n7EDeL+7f9bMXg5smSPGl7r7493GICIyrJTKIiJSn9uAy4A9wO0U\naST3AF8DVrfldz/ZzJ41a907gTVmdoKZLQLOmWc704PqnwBPmS8gM1vl7hPuvoPizP1pwJeA15vZ\n08pllpSLPxWYLJ9fMMdH3kor9QUze/582xcRGWYamIuI1Oc24CTgq+7+I+D/AXvc/ccUZ9J3mtm9\nFGkszy7XcQB3nwTeA/xd+TkTwOH2ZdpMv/4GcKS8gLPjxZ/AuWb2LTO7B3ge8Al3/w7wJ8Du8v0/\nLZfdBtxiZncC/zjH570DeHF54ei3gIvnWE5EZOiZu+5oJSLSRGY24u5T5UWjfw18zN0/XXdcIiJS\njc6Yi4g019byDPY3gQc1KBcRaTadMRcRGUJm9m8obqM4fRAwisH9fLnqIiISkAbmIiIiIiIJUCqL\niIiIiEgCNDAXEREREUmABuYiIiIiIgnQwFxEREREJAH/H1j+36GuLXUAAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x5b009f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "################################################################################\n",
    "# TODO: Train the best FullyConnectedNet that you can on CIFAR-10. You might   #\n",
    "# batch normalization and dropout useful. Store your best model in the         #\n",
    "# best_model variable.                                                         #\n",
    "################################################################################\n",
    "best_model = None\n",
    "\n",
    "results = random_search(max_search_times=200,\n",
    "                        report_val_acc=0.5,\n",
    "                        num_epochs=30,\n",
    "                        lr_decays=(0.8, 0.9),   # \n",
    "                        weight_scales=(0.04, 0.045),  # \n",
    "                        regs=(0.001, 0.02),  # \n",
    "                        learning_rates=(0.001, 0.002),  # \n",
    "                        )\n",
    "\n",
    "\n",
    "print(len(results))\n",
    "print_results(results)\n",
    "\n",
    "\n",
    "\n",
    "# random_search phase 1\n",
    "(max_search_times=500,\n",
    "report_val_acc=0.20,\n",
    "num_epochs=3,\n",
    "lr_decays=(0.8, 0.99),   # all value good enough \n",
    "weight_scales=(0.001, 0.1),  # 0.04 - 0.08\n",
    "regs=(0.1, 1, 'linear'), # all value good enough \n",
    "learning_rates=(0.00001, 0.002),  # 0.0005 - 0.002\n",
    ")\n",
    "\n",
    "# random_search phase 2\n",
    "(max_search_times=200,\n",
    "report_val_acc=0.4,\n",
    "num_epochs=10,\n",
    "lr_decays=(0.5, 0.99),   # 0.8 - 0.9\n",
    "weight_scales=(0.04, 0.08),  # 0.04 - 0.045\n",
    "regs=(0.01, 1),  # 0.01 - 0.02 \n",
    "learning_rates=(0.0005, 0.002),  # 0.001 - 0.002\n",
    ")\n",
    "\n",
    "# random_search phase 3\n",
    "(max_search_times=200,\n",
    "report_val_acc=0.5,\n",
    "num_epochs=30,\n",
    "lr_decays=(0.8, 0.9),   # 0.89\n",
    "weight_scales=(0.04, 0.045),  # all seems good\n",
    "regs=(0.001, 0.02),  # 0.003\n",
    "learning_rates=(0.001, 0.002),  # 0.0013\n",
    ")\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(results))\n",
    "print_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# solver\n",
    "results_back = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train (17000, 3, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "# best params\n",
    "# (max_search_times=200,\n",
    "# report_val_acc=0.5,\n",
    "# num_epochs=30,\n",
    "# lr_decays=(0.8, 0.9),   # 0.89\n",
    "# weight_scales=(0.04, 0.045),  # all seems good\n",
    "# regs=(0.001, 0.02),  # 0.003\n",
    "# learning_rates=(0.001, 0.002),  # 0.0013\n",
    "# )\n",
    "print('X_train', data['X_train'].shape)\n",
    "\n",
    "model = FullyConnectedNet([100, 100, 100, 100, 100], \n",
    "                          weight_scale=0.04,\n",
    "                          reg=0.1)\n",
    "solver = Solver(model, \n",
    "                data,\n",
    "                num_epochs=20, \n",
    "                batch_size=100,\n",
    "                update_rule='adam',\n",
    "                lr_decay=0.89,\n",
    "                optim_config={\n",
    "                  'learning_rate': 0.0013,\n",
    "                },\n",
    "                verbose=False)\n",
    "solver.train()\n",
    "print('-----')\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Training accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(solver.train_acc_history, '-o')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(solver.val_acc_history, '-o')\n",
    "\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test you model\n",
    "Run your best model on the validation and test sets. You should achieve above 50% accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_test_pred = np.argmax(best_model.loss(X_test), axis=1)\n",
    "y_val_pred = np.argmax(best_model.loss(X_val), axis=1)\n",
    "print 'Validation set accuracy: ', (y_val_pred == y_val).mean()\n",
    "print 'Test set accuracy: ', (y_test_pred == y_test).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
