{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 快速开始Sequential模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-27T14:06:51.653495",
     "start_time": "2017-02-27T14:06:46.932967"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# 快速开始Sequential模型\n",
    "\n",
    "# Sequential是多个网络层的线性堆叠\n",
    "\n",
    "# 可以通过向Sequential模型传递一个layer的list来构造该模型\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model = Sequential([\n",
    "Dense(32, input_dim=784),\n",
    "Activation('relu'),\n",
    "Dense(10),\n",
    "Activation('softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-23T17:10:48.174154",
     "start_time": "2017-02-23T17:10:48.140511"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 也可以通过.add()方法一个个的将layer加入模型中\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=784))\n",
    "model.add(Activation('relu'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 输入数据的shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型需要知道输入数据的shape，因此，Sequential的第一层需要接受一个关于输入数据shape的参数，后面的各个层则可以自动的推导出中间数据的shape，因此不需要为每个层都指定这个参数。有几种方法来为第一层指定输入数据的shape\n",
    "\n",
    "- 传递一个input_shape的关键字参数给第一层，input_shape是一个tuple类型的数据，其中也可以填入None，如果填入None则表示此位置可能是任何正整数。数据的batch大小不应包含在其中。\n",
    "- 传递一个batch_input_shape的关键字参数给第一层，该参数包含数据的batch大小。该参数在指定固定大小batch时比较有用，例如在stateful RNNs中。事实上，Keras在内部会通过添加一个None将input_shape转化为batch_input_shape\n",
    "- 有些2D层，如Dense，支持通过指定其输入维度input_dim来隐含的指定输入数据shape。一些3D的时域层支持通过参数input_dim和input_length来指定输入shape。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-23T17:16:51.779514",
     "start_time": "2017-02-23T17:16:51.563688"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LSTM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-40917de8e043>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_input_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LSTM' is not defined"
     ]
    }
   ],
   "source": [
    "# 下面的三个指定输入数据shape的方法是严格等价的：\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_shape=(784,)))\n",
    "model = Sequential()\n",
    "model.add(Dense(32, batch_input_shape=(None, 784)))\n",
    "# note that batch dimension is \"None\" here,\n",
    "# so the model will be able to process batches of any size.</pre>\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=784))\n",
    "\n",
    "\n",
    "# 下面三种方法也是严格等价的：\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(10, 64)))\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, batch_input_shape=(None, 10, 64)))\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, input_length=10, input_dim=64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编译"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在训练模型之前通过compile来对学习过程进行配置。compile接收三个参数：\n",
    "\n",
    "- 优化器optimizer：该参数可指定为已预定义的优化器名，如rmsprop、adagrad，或一个Optimizer类的对象，详情见optimizers\n",
    "- 损失函数loss：该参数为模型试图最小化的目标函数，它可为预定义的损失函数名，如categorical_crossentropy、mse，也可以为一个损失函数。详情见objectives\n",
    "- 指标列表metrics：对分类问题，我们一般将该列表设置为metrics=['accuracy']。指标可以是一个预定义指标的名字,也可以是一个用户定制的函数.指标函数应该返回单个张量,或一个完成metric_name - > metric_value映射的字典.请参考性能评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for a multi-class classification problem\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# for a binary classification problem\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# for a mean squared error regression problem\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='mse')\n",
    "\n",
    "\n",
    "\n",
    "# for custom metrics\n",
    "import keras.backend as K\n",
    "\n",
    "def mean_pred(y_true, y_pred):\n",
    "    return K.mean(y_pred)\n",
    "\n",
    "def false_rates(y_true, y_pred):\n",
    "    false_neg = ...\n",
    "    false_pos = ...\n",
    "    return {\n",
    "        'false_neg': false_neg,\n",
    "        'false_pos': false_pos,\n",
    "        }\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy', mean_pred, false_rates]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练\n",
    "\n",
    "Keras以Numpy数组作为输入数据和标签的数据类型。训练模型一般使用fit函数，该函数的详情见这里。下面是一些例子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-27T14:07:19.467784",
     "start_time": "2017-02-27T14:07:17.388984"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "0s - loss: 0.7387 - acc: 0.4700\n",
      "Epoch 2/30\n",
      "0s - loss: 0.7204 - acc: 0.5040\n",
      "Epoch 3/30\n",
      "0s - loss: 0.7165 - acc: 0.5150\n",
      "Epoch 4/30\n",
      "0s - loss: 0.7041 - acc: 0.5300\n",
      "Epoch 5/30\n",
      "0s - loss: 0.6961 - acc: 0.5410\n",
      "Epoch 6/30\n",
      "0s - loss: 0.6909 - acc: 0.5470\n",
      "Epoch 7/30\n",
      "0s - loss: 0.6907 - acc: 0.5350\n",
      "Epoch 8/30\n",
      "0s - loss: 0.6732 - acc: 0.6020\n",
      "Epoch 9/30\n",
      "0s - loss: 0.6645 - acc: 0.6040\n",
      "Epoch 10/30\n",
      "0s - loss: 0.6611 - acc: 0.6000\n",
      "Epoch 11/30\n",
      "0s - loss: 0.6557 - acc: 0.6080\n",
      "Epoch 12/30\n",
      "0s - loss: 0.6497 - acc: 0.6280\n",
      "Epoch 13/30\n",
      "0s - loss: 0.6385 - acc: 0.6320\n",
      "Epoch 14/30\n",
      "0s - loss: 0.6367 - acc: 0.6440\n",
      "Epoch 15/30\n",
      "0s - loss: 0.6337 - acc: 0.6400\n",
      "Epoch 16/30\n",
      "0s - loss: 0.6291 - acc: 0.6450\n",
      "Epoch 17/30\n",
      "0s - loss: 0.6140 - acc: 0.6810\n",
      "Epoch 18/30\n",
      "0s - loss: 0.6216 - acc: 0.6750\n",
      "Epoch 19/30\n",
      "0s - loss: 0.6101 - acc: 0.6910\n",
      "Epoch 20/30\n",
      "0s - loss: 0.6054 - acc: 0.6980\n",
      "Epoch 21/30\n",
      "0s - loss: 0.6006 - acc: 0.7010\n",
      "Epoch 22/30\n",
      "0s - loss: 0.5907 - acc: 0.6890\n",
      "Epoch 23/30\n",
      "0s - loss: 0.5906 - acc: 0.7230\n",
      "Epoch 24/30\n",
      "0s - loss: 0.5840 - acc: 0.7380\n",
      "Epoch 25/30\n",
      "0s - loss: 0.5788 - acc: 0.7280\n",
      "Epoch 26/30\n",
      "0s - loss: 0.5764 - acc: 0.7310\n",
      "Epoch 27/30\n",
      "0s - loss: 0.5704 - acc: 0.7420\n",
      "Epoch 28/30\n",
      "0s - loss: 0.5681 - acc: 0.7440\n",
      "Epoch 29/30\n",
      "0s - loss: 0.5628 - acc: 0.7510\n",
      "Epoch 30/30\n",
      "0s - loss: 0.5633 - acc: 0.7610\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16122126a58>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for a single-input model with 2 classes (binary):\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_dim=784, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# generate dummy data\n",
    "import numpy as np\n",
    "data = np.random.random((1000, 784))\n",
    "labels = np.random.randint(2, size=(1000, 1))\n",
    "\n",
    "# train the model, iterating on the data in batches\n",
    "# of 32 samples\n",
    "model.fit(data, labels, nb_epoch=30, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-23T17:24:31.508344",
     "start_time": "2017-02-23T17:24:29.676799"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 0s - loss: 2.9599 - acc: 0.1080      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s - loss: 2.6320 - acc: 0.1150     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s - loss: 2.5086 - acc: 0.1420     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s - loss: 2.4194 - acc: 0.1580         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s - loss: 2.2822 - acc: 0.2080     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s - loss: 2.1613 - acc: 0.2400     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s - loss: 2.0939 - acc: 0.2590     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s - loss: 1.9655 - acc: 0.2990     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s - loss: 1.9028 - acc: 0.3290     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s - loss: 1.7418 - acc: 0.4110     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23ce2d115f8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for a multi-input model with 10 classes:\n",
    "\n",
    "left_branch = Sequential()\n",
    "left_branch.add(Dense(32, input_dim=784))\n",
    "\n",
    "right_branch = Sequential()\n",
    "right_branch.add(Dense(32, input_dim=784))\n",
    "\n",
    "from keras.layers import Merge\n",
    "merged = Merge([left_branch, right_branch], mode='concat')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(merged)\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# generate dummy data\n",
    "import numpy as np\n",
    "from keras.utils.np_utils import to_categorical\n",
    "data_1 = np.random.random((1000, 784))\n",
    "data_2 = np.random.random((1000, 784))\n",
    "\n",
    "# these are integers between 0 and 9\n",
    "labels = np.random.randint(10, size=(1000, 1))\n",
    "# we convert the labels to a binary matrix of size (1000, 10)\n",
    "# for use with categorical_crossentropy\n",
    "labels = to_categorical(labels, 10)\n",
    "\n",
    "# train the model\n",
    "# note that we are passing a list of Numpy arrays as training data\n",
    "# since the model has 2 inputs\n",
    "model.fit([data_1, data_2], labels, nb_epoch=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 如何在每个epoch后记录训练/测试的loss和正确率？\n",
    "\n",
    "\n",
    "# model.fit在运行结束后返回一个History对象，\n",
    "# 其中含有的history属性包含了训练过程中损失函数的值以及其他度量指标。\n",
    "\n",
    "hist = model.fit(X, y, validation_split=0.2)\n",
    "print(hist.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-23T17:46:31.633767",
     "start_time": "2017-02-23T17:46:31.608704"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hist = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-23T17:46:38.251370",
     "start_time": "2017-02-23T17:46:38.239833"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': [2.9599301223754884, 2.6319890480041503, 2.5086392288208006, 2.4193947353363039, 2.2821615600585936, 2.1612506513595582, 2.0938981533050538, 1.9655143051147461, 1.9027737426757811, 1.7417905464172363], 'acc': [0.108, 0.115, 0.14199999999999999, 0.158, 0.20799999999999999, 0.23999999999999999, 0.25900000000000001, 0.29899999999999999, 0.32900000000000001, 0.41099999999999998]}\n"
     ]
    }
   ],
   "source": [
    "print(hist.history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 例子\n",
    "\n",
    "这里是一些帮助你开始的例子\n",
    "\n",
    "在Keras代码包的examples文件夹中，你将找到使用真实数据的示例模型：\n",
    "\n",
    "- CIFAR10 小图片分类：使用CNN和实时数据提升\n",
    "- IMDB 电影评论观点分类：使用LSTM处理成序列的词语\n",
    "- Reuters（路透社）新闻主题分类：使用多层感知器（MLP）\n",
    "- MNIST手写数字识别：使用多层感知器和CNN\n",
    "- 字符级文本生成：使用LSTM ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-23T17:29:20.932745",
     "start_time": "2017-02-23T17:29:20.827930"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-11-a94f4c3c87b0>, line 61)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-a94f4c3c87b0>\"\u001b[0;36m, line \u001b[0;32m61\u001b[0m\n\u001b[0;31m    类似VGG的卷积神经网络：\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "# 基于多层感知器的softmax多分类：\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model = Sequential()\n",
    "# Dense(64) is a fully-connected layer with 64 hidden units.\n",
    "# in the first layer, you must specify the expected input data shape:\n",
    "# here, 20-dimensional vectors.\n",
    "model.add(Dense(64, input_dim=20, init='uniform'))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, init='uniform'))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, init='uniform'))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          nb_epoch=20,\n",
    "          batch_size=16)\n",
    "score = model.evaluate(X_test, y_test, batch_size=16)   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 相似MLP的另一种实现：\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=20, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 用于二分类的多层感知器：\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=20, init='uniform', activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 类似VGG的卷积神经网络：\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model = Sequential()\n",
    "# input: 100x100 images with 3 channels -> (3, 100, 100) tensors.\n",
    "# this applies 32 convolution filters of size 3x3 each.\n",
    "model.add(Convolution2D(32, 3, 3, border_mode='valid', input_shape=(3, 100, 100)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(32, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Convolution2D(64, 3, 3, border_mode='valid'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "# Note: Keras does automatic shape inference.\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
    "\n",
    "model.fit(X_train, Y_train, batch_size=32, nb_epoch=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 使用LSTM的序列分类\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 256, input_length=maxlen))\n",
    "model.add(LSTM(output_dim=128, activation='sigmoid', inner_activation='hard_sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train, batch_size=16, nb_epoch=10)\n",
    "score = model.evaluate(X_test, Y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 如何保存Keras模型？\n",
    "\n",
    "\n",
    "我们不推荐使用pickle或cPickle来保存Keras模型\n",
    "\n",
    "你可以使用model.save(filepath)将Keras模型和权重保存在一个HDF5文件中，该文件将包含：\n",
    "\n",
    "- 模型的结构，以便重构该模型\n",
    "- 模型的权重\n",
    "- 训练配置（损失函数，优化器等）\n",
    "- 优化器的状态，以便于从上次训练中断的地方开始\n",
    "\n",
    "使用keras.models.load_model(filepath)来重新实例化你的模型，如果文件中存储了训练配置的话，该函数还会同时完成模型的编译\n",
    "\n",
    "例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "del model  # deletes the existing model\n",
    "\n",
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "model = load_model('my_model.h5')\n",
    "\n",
    "\n",
    "# 如果你只是希望保存模型的结构，而不包含其权重或配置信息，可以使用：\n",
    "# save as JSON\n",
    "json_string = model.to_json()\n",
    "# save as YAML\n",
    "yaml_string = model.to_yaml()\n",
    "# 这项操作将把模型序列化为json或yaml文件，这些文件对人而言也是友好的，\n",
    "# 如果需要的话你甚至可以手动打开这些文件并进行编辑。\n",
    "\n",
    "\n",
    "# 当然，你也可以从保存好的json文件或yaml文件中载入模型：\n",
    "# model reconstruction from JSON:\n",
    "from keras.models import model_from_json\n",
    "model = model_from_json(json_string)\n",
    "# model reconstruction from YAML\n",
    "model = model_from_yaml(yaml_string)\n",
    "\n",
    "\n",
    "# 如果需要保存模型的权重，可通过下面的代码利用HDF5进行保存。\n",
    "# 注意，在使用前需要确保你已安装了HDF5和其Python库h5py\n",
    "model.save_weights('my_model_weights.h5')\n",
    "# 如果你需要在代码中初始化一个完全相同的模型，请使用：\n",
    "model.load_weights('my_model_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fit(self, x, y, \n",
    "    batch_size=32, \n",
    "    nb_epoch=10, \n",
    "    verbose=1, \n",
    "    callbacks=[], \n",
    "    validation_split=0.0, \n",
    "    validation_data=None, \n",
    "    shuffle=True, \n",
    "    class_weight=None, \n",
    "    sample_weight=None\n",
    "   )\n",
    "\n",
    "\n",
    "# 本函数将模型训练nb_epoch轮，其参数有：\n",
    "\n",
    "# x：输入数据。如果模型只有一个输入，那么x的类型是numpy array，如果模型有多个输入，那么x的类型应当为list，list的元素是对应于各个输入的numpy array\n",
    "# y：标签，numpy array\n",
    "# batch_size：整数，指定进行梯度下降时每个batch包含的样本数。训练时一个batch的样本会被计算一次梯度下降，使目标函数优化一步。\n",
    "# nb_epoch：整数，训练的轮数，训练数据将会被遍历nb_epoch次。Keras中nb开头的变量均为\"number of\"的意思\n",
    "# verbose：日志显示，0为不在标准输出流输出日志信息，1为输出进度条记录，2为每个epoch输出一行记录\n",
    "# callbacks：list，其中的元素是keras.callbacks.Callback的对象。这个list中的回调函数将会在训练过程中的适当时机被调用，参考回调函数\n",
    "# validation_split：0~1之间的浮点数，用来指定训练集的一定比例数据作为验证集。验证集将不参与训练，并在每个epoch结束后测试的模型的指标，如损失函数、精确度等。\n",
    "# validation_data：形式为（X，y）的tuple，是指定的验证集。此参数将覆盖validation_spilt。\n",
    "# shuffle：布尔值或字符串，一般为布尔值，表示是否在训练过程中随机打乱输入样本的顺序。若为字符串“batch”，则是用来处理HDF5数据的特殊情况，它将在batch内部将数据打乱。\n",
    "# class_weight：字典，将不同的类别映射为不同的权值，该参数用来在训练过程中调整损失函数（只能用于训练）\n",
    "# sample_weight：权值的numpy array，用于在训练时调整损失函数（仅用于训练）。可以传递一个1D的与样本等长的向量用于对样本进行1对1的加权，或者在面对时序数据时，传递一个的形式为（samples，sequence_length）的矩阵来为每个时间步上的样本赋不同的权。这种情况下请确定在编译模型时添加了sample_weight_mode='temporal'。\n",
    "# fit函数返回一个History的对象，其History.history属性记录了损失函数和其他指标的数值随epoch变化的情况，如果有验证集的话，也包含了验证集的这些指标变化情况"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 图片预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 图片生成器ImageDataGenerator\n",
    "\n",
    "keras.preprocessing.image.ImageDataGenerator(featurewise_center=False,\n",
    "    samplewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False,\n",
    "    rotation_range=0.,\n",
    "    width_shift_range=0.,\n",
    "    height_shift_range=0.,\n",
    "    shear_range=0.,\n",
    "    zoom_range=0.,\n",
    "    channel_shift_range=0.,\n",
    "    fill_mode='nearest',\n",
    "    cval=0.,\n",
    "    horizontal_flip=False,\n",
    "    vertical_flip=False,\n",
    "    rescale=None,\n",
    "    dim_ordering=K.image_dim_ordering())\n",
    "\n",
    "# 用以生成一个batch的图像数据，支持实时数据提升。训练时该函数会无限生成数据，直到达到规定的epoch次数为止。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参数\n",
    "\n",
    "- featurewise_center：布尔值，使输入数据集去中心化（均值为0）, 按feature执行\n",
    "- samplewise_center：布尔值，使输入数据的每个样本均值为0\n",
    "- featurewise_std_normalization：布尔值，将输入除以数据集的标准差以完成标准化, 按feature执行\n",
    "- samplewise_std_normalization：布尔值，将输入的每个样本除以其自身的标准差\n",
    "- zca_whitening：布尔值，对输入数据施加ZCA白化\n",
    "- rotation_range：整数，数据提升时图片随机转动的角度\n",
    "- width_shift_range：浮点数，图片宽度的某个比例，数据提升时图片水平偏移的幅度\n",
    "- height_shift_range：浮点数，图片高度的某个比例，数据提升时图片竖直偏移的幅度\n",
    "- shear_range：浮点数，剪切强度（逆时针方向的剪切变换角度）\n",
    "- zoom_range：浮点数或形如[lower,upper]的列表，随机缩放的幅度，若为浮点数，则相当于[lower,upper] = [1 - zoom_range, 1+zoom_range]\n",
    "- channel_shift_range：浮点数，随机通道偏移的幅度\n",
    "- fill_mode：；‘constant’，‘nearest’，‘reflect’或‘wrap’之一，当进行变换时超出边界的点将根据本参数给定的方法进行处理\n",
    "- cval：浮点数或整数，当fill_mode=constant时，指定要向超出边界的点填充的值\n",
    "- horizontal_flip：布尔值，进行随机水平翻转\n",
    "- vertical_flip：布尔值，进行随机竖直翻转\n",
    "- rescale: 重放缩因子,默认为None. 如果为None或0则不进行放缩,否则会将该数值乘到数据上(在应用其他变换之前)\n",
    "- dim_ordering：‘tf’和‘th’之一，规定数据的维度顺序。‘tf’模式下数据的形状为samples, height, width, channels，‘th’下形状为(samples, channels, height, width).该参数的默认值是Keras配置文件~/.keras/keras.json的image_dim_ordering值,如果你从未设置过的话,就是'tf'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 土办法\n",
    "# show image from local\n",
    "from keras.utils.visualize_util import plot\n",
    "plot(model, show_shapes=True, to_file='model.png')\n",
    "from IPython.display import display, Image\n",
    "display(Image('model.png', width=500))\n",
    "\n",
    "# 后来发现可以配置在Jupyter直接显示SVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-27T14:55:43.095587",
     "start_time": "2017-02-27T14:55:43.038681"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-d866d79115e7>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-d866d79115e7>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    命令行输入pip install pydot-ng & brew install graphviz\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# 我们也可以直接获取一个pydot.Graph对象，然后按照自己的需要配置它，例如，如果要在ipython中展示图片\n",
    "\n",
    "from IPython.display import SVG\n",
    "from keras.utils.visualize_util import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))\n",
    "\n",
    "# 【Tips】依赖 pydot-ng 和 graphviz，\n",
    "# 命令行输入pip install pydot-ng & brew install graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution2D层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keras.layers.convolutional.Convolution2D(nb_filter, \n",
    "                                         nb_row, \n",
    "                                         nb_col, \n",
    "                                         init='glorot_uniform', \n",
    "                                         activation='linear', \n",
    "                                         weights=None, \n",
    "                                         border_mode='valid', \n",
    "                                         subsample=(1, 1), \n",
    "                                         dim_ordering='th', \n",
    "                                         W_regularizer=None, \n",
    "                                         b_regularizer=None, \n",
    "                                         activity_regularizer=None, \n",
    "                                         W_constraint=None, \n",
    "                                         b_constraint=None, \n",
    "                                         bias=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "二维卷积层对二维输入进行滑动窗卷积，当使用该层作为第一层时，应提供input_shape参数。\n",
    "例如input_shape = (3,128,128)代表128*128的彩色RGB图像\n",
    "\n",
    "\n",
    "\n",
    "- nb_filter：卷积核的数目\n",
    "- nb_row：卷积核的行数\n",
    "- nb_col：卷积核的列数\n",
    "- init：初始化方法，为预定义初始化方法名的字符串，或用于初始化权重的Theano函数。该参数仅在不传递weights参数时有意义。\n",
    "- activation：激活函数，为预定义的激活函数名（参考激活函数），或逐元素（element-wise）的Theano函数。如果不指定该参数，将不会使用任何激活函数（即使用线性激活函数：a(x)=x）\n",
    "- weights：权值，为numpy array的list。该list应含有一个形如（input_dim,output_dim）的权重矩阵和一个形如(output_dim,)的偏置向量。\n",
    "- border_mode：边界模式，为“valid”，“same”或“full”，full需要以theano为后端\n",
    "- subsample：长为2的tuple，输出对输入的下采样因子，更普遍的称呼是“strides”\n",
    "- W_regularizer：施加在权重上的正则项，为WeightRegularizer对象\n",
    "- b_regularizer：施加在偏置向量上的正则项，为WeightRegularizer对象\n",
    "- activity_regularizer：施加在输出上的正则项，为ActivityRegularizer对象\n",
    "- W_constraints：施加在权重上的约束项，为Constraints对象\n",
    "- b_constraints：施加在偏置上的约束项，为Constraints对象\n",
    "- dim_ordering：‘th’或‘tf’。‘th’模式中通道维（如彩色图像的3通道）位于第1个位置（维度从0开始算），而在‘tf’模式中，通道维位于第3个位置。例如128*128的三通道彩色图片，在‘th’模式中input_shape应写为（3，128，128），而在‘tf’模式中应写为（128，128，3），注意这里3出现在第0个位置，因为input_shape不包含样本数的维度，在其内部实现中，实际上是（None，3，128，128）和（None，128，128，3）。默认是image_dim_ordering指定的模式，可在~/.keras/keras.json中查看，若没有设置过则为'tf'。\n",
    "- bias：布尔值，是否包含偏置向量（即层对输入做线性变换还是仿射变换）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正则项"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正则项在优化过程中层的参数或层的激活值添加惩罚项，这些惩罚项将与损失函数一起作为网络的最终优化目标\n",
    "\n",
    "惩罚项基于层进行惩罚，目前惩罚项的接口与层有关，但Dense, TimeDistributedDense, MaxoutDense, Covolution1D, Covolution2D, Convolution3D具有共同的接口。\n",
    "\n",
    "这些层有三个关键字参数以施加正则项：\n",
    "\n",
    "- W_regularizer：施加在权重上的正则项，为WeightRegularizer对象\n",
    "- b_regularizer：施加在偏置向量上的正则项，为WeightRegularizer对象\n",
    "- activity_regularizer：施加在输出上的正则项，为ActivityRegularizer对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.regularizers import l2, activity_l2\n",
    "model.add(Dense(64, \n",
    "                input_dim=64, \n",
    "                W_regularizer=l2(0.01), \n",
    "                activity_regularizer=activity_l2(0.01)\n",
    "               ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "keras.regularizers支持以下缩写\n",
    "\n",
    "- l1(l=0.01)：L1正则项，又称LASSO\n",
    "- l2(l=0.01)：L2正则项，又称权重衰减或Ridge\n",
    "- l1l2(l1=0.01, l2=0.01)： L1-L2混合正则项, 又称ElasticNet\n",
    "- activity_l1(l=0.01)： L1激活值正则项\n",
    "- activity_l2(l=0.01)： L2激活值正则项\n",
    "- activity_l1l2(l1=0.01, l2=0.01)： L1+L2激活值正则项\n",
    "\n",
    "【Tips】\n",
    "正则项通常用于对模型的训练施加某种约束，L1正则项即L1范数约束，该约束会使被约束矩阵/向量更稀疏。\n",
    "L2正则项即L2范数约束，该约束会使被约束的矩阵/向量更平滑，因为它对脉冲型的值有很大的惩罚。【@Bigmoyan】"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keras.layers.core.Dense(output_dim, \n",
    "                        init='glorot_uniform', \n",
    "                        activation='linear', \n",
    "                        weights=None, \n",
    "                        W_regularizer=None, \n",
    "                        b_regularizer=None, \n",
    "                        activity_regularizer=None, \n",
    "                        W_constraint=None, \n",
    "                        b_constraint=None, \n",
    "                        bias=True, \n",
    "                        input_dim=None)\n",
    "\n",
    "\n",
    "# Dense就是常用的全连接层，这里是一个使用示例：\n",
    "\n",
    "# as first layer in a sequential model:\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=16))\n",
    "# now the model will take as input arrays of shape (*, 16)\n",
    "# and output arrays of shape (*, 32)\n",
    "\n",
    "# this is equivalent to the above:\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_shape=(16,)))\n",
    "\n",
    "# after the first layer, you don't need to specify\n",
    "# the size of the input anymore:\n",
    "model.add(Dense(32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参数：\n",
    "\n",
    "- output_dim：大于0的整数，代表该层的输出维度。模型中非首层的全连接层其输入维度可以自动推断，因此非首层的全连接定义时不需要指定输入维度。\n",
    "- init：初始化方法，为预定义初始化方法名的字符串，或用于初始化权重的Theano函数。该参数仅在不传递weights参数时才有意义。\n",
    "- activation：激活函数，为预定义的激活函数名（参考激活函数），或逐元素（element-wise）的Theano函数。如果不指定该参数，将不会使用任何激活函数（即使用线性激活函数：a(x)=x）\n",
    "- weights：权值，为numpy array的list。该list应含有一个形如（input_dim,output_dim）的权重矩阵和一个形如(output_dim,)的偏置向量。\n",
    "- W_regularizer：施加在权重上的正则项，为WeightRegularizer对象\n",
    "- b_regularizer：施加在偏置向量上的正则项，为WeightRegularizer对象\n",
    "- activity_regularizer：施加在输出上的正则项，为ActivityRegularizer对象\n",
    "- W_constraints：施加在权重上的约束项，为Constraints对象\n",
    "- b_constraints：施加在偏置上的约束项，为Constraints对象\n",
    "- bias：布尔值，是否包含偏置向量（即层对输入做线性变换还是仿射变换）\n",
    "- input_dim：整数，输入数据的维度。当Dense层作为网络的第一层时，必须指定该参数或input_shape参数。"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
